{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2da6d105-2df5-40a6-9baa-02497fcef0d8",
   "metadata": {},
   "source": [
    "# Measuring performance in HIP applications\n",
    "\n",
    "An understanding of how well HIP applications perform is a vital part of the development process. Two main techniques, **profiling** and **tracing** collect information about how well an application is performing. **Profiling** is the statistical collection of the cumulative time that threads spend in each program component. **Tracing** is a collection of both **when** and **for how long** threads spend in each application component. Since HIP applications use either an AMD or a CUDA backend, the profiling tools from each platform are available for use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4bd9a5-220f-4ebc-99b5-fe753e6eca84",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Event based timing\n",
    "\n",
    "Events in HIP are used with streams to check the progress of work that has been submitted and establish dependencies between workflows. They can also be used to time the execution of work, such as kernels and memory copies. Here is how they fit into the picture of a HIP application.\n",
    "\n",
    "<figure style=\"margin-left:auto; margin-right:auto; width:70%;\">\n",
    "    <img style=\"vertical-\n",
    "                align:middle\" src=\"../images/hip_components.svg\">\n",
    "    <figcaption style= \"text-align:lower; margin:1em; float:bottom; vertical-align:bottom;\">Components of a HIP application. Events are associated with streams, and provide a way to time the duration of work in a stream. </figcaption>\n",
    "</figure>\n",
    "\n",
    "## Example application\n",
    "\n",
    "The code [mat_mult_profiling.cpp](mat_mult_profiling.cpp) contains a complete example where events are used to time the execution of the host to device memory copy as well as the timing of the matrix multiplication kernel. The data type **hipEvent_t** stores event data. \n",
    "\n",
    "### Source code changes\n",
    "\n",
    "In [mat_mult_profiling.cpp](mat_mult_profiling.cpp) we use the function **hipEventCreate** to create two events **t1** and **t2**, as seen in line 111.\n",
    "\n",
    "```C++\n",
    "    // mat_mult_profiling.cpp:111\n",
    "\n",
    "    // Create events for the memory copies and kernel runs\n",
    "    hipEvent_t t1=0, t2=0;\n",
    "    // Create the events\n",
    "    H_ERRCHK(hipEventCreate(&t1));\n",
    "    H_ERRCHK(hipEventCreate(&t2));\n",
    "```\n",
    "\n",
    "Now we wish to use these events to time the upload of host matrices **A_h** and **B_h** to the compute device. The HIP function **hipEventRecord** inserts the event into the \"flow\" of a stream. We haven't talked in depth about HIP streams yet and at this stage we can think of a stream as a queue to which work is submitted. Since we are not using a particular stream we are using the default stream (denoted by 0). We insert event `t1` into the default stream, perform the memory copies, then insert `t2` after the copy is launched.\n",
    "\n",
    "```C++\n",
    "    // Record the start event into the default stream\n",
    "    H_ERRCHK(hipEventRecord(t1,0));\n",
    "    \n",
    "    // Peform the memory copies\n",
    "    H_ERRCHK(hipMemcpy(A_d, A_h, nbytes_A, hipMemcpyHostToDevice));\n",
    "    H_ERRCHK(hipMemcpy(B_d, B_h, nbytes_B, hipMemcpyHostToDevice));\n",
    "    \n",
    "    // Record the stop event into the default stream\n",
    "    H_ERRCHK(hipEventRecord(t2,0));\n",
    "```\n",
    "\n",
    "The function **hipEventSynchronize** waits until events reach a complete status. Then we can use the function **hipEventElapsedTime** to get the time elapsed between the two events. The helper function **h_get_event_time_ms** takes care of calling these functions, prints performance measurement information, and returns the number of milliseconds between the two events.\n",
    "\n",
    "```C++\n",
    "    // Total number of Bytes copied\n",
    "    size_t total_bytes = nbytes_A + nbytes_B;\n",
    "\n",
    "    // Get the elapsed time in milliseconds\n",
    "    float elapsed_ms = h_get_event_time_ms(t1, t2, \"memcpy\", &total_bytes);\n",
    "```\n",
    "\n",
    "The source code of **h_get_event_time_ms** is in <a href=\"../include/hip_helper.hpp\">hip_helper.hpp</a> and reproduced below:\n",
    "\n",
    "```C++\n",
    "// Get how much time elapsed between two events that were recorded\n",
    "float h_get_event_time_ms(\n",
    "        // Assumes start and stop events have been recorded\n",
    "        // with the hipEventRecord() function\n",
    "        hipEvent_t t1,\n",
    "        hipEvent_t t2,\n",
    "        const char* message, \n",
    "        size_t* nbytes) {\n",
    "    \n",
    "    // Make sure the stop and start events have finished\n",
    "    H_ERRCHK(hipEventSynchronize(t2));\n",
    "    H_ERRCHK(hipEventSynchronize(t1));\n",
    "\n",
    "    // Elapsed time in milliseconds\n",
    "    float elapsed_ms=0;\n",
    "\n",
    "    // Convert the time into milliseconds\n",
    "    H_ERRCHK(hipEventElapsedTime(&elapsed_ms, t1, t2));\n",
    "        \n",
    "    // Print the timing message if necessary\n",
    "    if ((message != NULL) && (strlen(message)>0)) {\n",
    "        std::printf(\"Time for event \\\"%s\\\": %.3f ms\", message, elapsed_ms);\n",
    "        \n",
    "        // Print transfer rate if nbytes is not NULL\n",
    "        if (nbytes != NULL) {\n",
    "            double io_rate_MBs = h_get_io_rate_MBs(\n",
    "                elapsed_ms, \n",
    "                *nbytes\n",
    "            );\n",
    "            std::printf(\" (%.2f MB/s)\", io_rate_MBs);\n",
    "        }\n",
    "        std::printf(\"\\n\");\n",
    "    }\n",
    "    \n",
    "    return elapsed_ms;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db80de96-12f4-474c-b8e7-fcb4e0d50a24",
   "metadata": {},
   "source": [
    "We can reuse the events to time the execution of the kernel. \n",
    "\n",
    "```C++\n",
    "    // Record the start event into the default stream\n",
    "    H_ERRCHK(hipEventRecord(t1,0));\n",
    "\n",
    "    // Launch the kernel using hipLaunchKernelGGL method\n",
    "    hipLaunchKernelGGL(mat_mult, \n",
    "            grid_nblocks, \n",
    "            block_size, sharedMemBytes, 0, \n",
    "            A_d, B_d, C_d,\n",
    "            N1_A,\n",
    "            N0_C,\n",
    "            N1_C\n",
    "    );\n",
    "\n",
    "    // Record the stop event into the default stream \n",
    "    H_ERRCHK(hipEventRecord(t2,0));\n",
    "\n",
    "    // Get the elapsed time in milliseconds\n",
    "    elapsed_ms = h_get_event_time_ms(t1, t2, \"mat_mult kernel\", NULL);\n",
    "```\n",
    "\n",
    "When we are finished with an event we can destroy them with the **hipEventDestroy** function. \n",
    "\n",
    "```C++\n",
    "    // Destroy events\n",
    "    H_ERRCHK(hipEventDestroy(t1));\n",
    "    H_ERRCHK(hipEventDestroy(t2));\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bd3ff9-409d-4f1a-a692-24e512e58bcb",
   "metadata": {},
   "source": [
    "In this manner we instrument the uploads, downloads, and kernel execution in the source file [mat_mult_profiling.cpp](mat_mult_profiling.cpp). Now we run the instrumented code and view the timing results. Change directory to **course_material/L5_Profiling** and run the following code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259bedb4-78c2-40fd-bd37-fb830077dbb9",
   "metadata": {},
   "source": [
    "## Import the environment\n",
    "\n",
    "The command below brings the `run` and `build` commands within reach of the Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5d0b201-baf0-4ff5-9333-63fb1f95eed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PATH'] = f\"{os.environ['PATH']}:../install/bin\"\n",
    "\n",
    "# At a Bash terminal you need to do this instead\n",
    "# source ../env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63a7ca7f-acee-40b7-9ec5-30bd9d0ba996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 66%] Built target hip_helper\n",
      "[100%] Built target mat_mult_profiling.exe\n",
      "\u001b[36mInstall the project...\u001b[0m\n",
      "-- Install configuration: \"RELEASE\"\n",
      "Device id: 0\n",
      "\tname:                                    AMD Radeon VII\n",
      "\tglobal memory size:                      17163 MB\n",
      "\tavailable registers per block:           65536 \n",
      "\tmax threads per SM or CU:                2560 \n",
      "\tmaximum shared memory size per block:    65 KB\n",
      "\tmaximum shared memory size per SM or CU: 65 KB\n",
      "\tmaximum pitch size for memory copies:    2147 MB\n",
      "\tmax block size:                          (1024,1024,1024)\n",
      "\tmax threads in a block:                  1024\n",
      "\tmax Grid size:                           (2147483647,65536,65536)\n",
      "Time for event \"memcpy\": 0.914 ms (1738.94 MB/s)\n",
      "Time for event \"mat_mult kernel\": 0.809 ms\n",
      "Maximum error (infinity norm) is: 2.28882e-05\n"
     ]
    }
   ],
   "source": [
    "!build mat_mult_profiling.exe; run mat_mult_profiling.exe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ec2892-86fe-46df-9e43-8446ebb8f977",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Performance measurement with AMD tools\n",
    "\n",
    "AMD has a number of tools available to help with collection of performance data. The low-level AMD profiler tool **ROC-profiler** (rocprof) has the ability to collect traces and information from hardware performance counters. Tools like [Omnitrace](https://github.com/AMDResearch/omnitrace) expand on the information collected by `rocprof` to include CPU resources and system metrics like GPU temperature and power usage. Tools like [Omniperf](https://github.com/AMDResearch/omniperf) use information from rocprof to help understand **how well** an application is performing in relation to peak performance, using reports such as roofline analysis and making information collected by rocprof understandable through graphical interfaces.\n",
    "\n",
    "### HIP application traces with rocprof\n",
    "\n",
    "Collection of HIP application traces with **rocprof** is accomplished with both the **--hip-trace** and **--hsa-trace** flags. Tracing with **rocprof** only seems to work with the AMD HIP backend at present. Here is what a typical profling command looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a09e5d94-80c2-4f99-99c5-afc97c285458",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RPL: on '240507_144305' from '/opt/rocm-6.0.2' in '/nethome/tpotter/Pelagos/Projects/HIP_Course/course_material/L5_Profiling'\n",
      "RPL: profiling '\"mat_mult_profiling.exe\"'\n",
      "RPL: input file ''\n",
      "RPL: output dir '/tmp/rpl_data_240507_144305_1332270'\n",
      "RPL: result dir '/tmp/rpl_data_240507_144305_1332270/input_results_240507_144305'\n",
      "ROCtracer (1332292):\n",
      "ROCProfiler: input from \"/tmp/rpl_data_240507_144305_1332270/input.xml\"\n",
      "  0 metrics\n",
      "    HSA-trace(*)\n",
      "    HSA-activity-trace()\n",
      "    HIP-trace(*)\n",
      "Device id: 0\n",
      "\tname:                                    AMD Radeon VII\n",
      "\tglobal memory size:                      17163 MB\n",
      "\tavailable registers per block:           65536 \n",
      "\tmax threads per SM or CU:                2560 \n",
      "\tmaximum shared memory size per block:    65 KB\n",
      "\tmaximum shared memory size per SM or CU: 65 KB\n",
      "\tmaximum pitch size for memory copies:    2147 MB\n",
      "\tmax block size:                          (1024,1024,1024)\n",
      "\tmax threads in a block:                  1024\n",
      "\tmax Grid size:                           (2147483647,65536,65536)\n",
      "Time for event \"memcpy\": 1.322 ms (1202.37 MB/s)\n",
      "Time for event \"mat_mult kernel\": 0.824 ms\n",
      "Maximum error (infinity norm) is: 2.28882e-05\n",
      "\n",
      "ROCPRofiler: 1 contexts collected, output directory /tmp/rpl_data_240507_144305_1332270/input_results_240507_144305\n",
      "hsa_copy_deps: 1\n",
      "scan hsa API data 563:564                                                                                                    hsa_copy_deps: 0\n",
      "scan hip API data 45:46                                                                                                    File 'rocprof_trace/result.csv' is generating\n",
      "File 'rocprof_trace/result.stats.csv' is generating\n",
      "dump json 0:1                                                                                                    \n",
      "File 'rocprof_trace/result.json' is generating\n",
      "File 'rocprof_trace/result.hsa_stats.csv' is generating\n",
      "dump json 564:565                                                                                                    \n",
      "File 'rocprof_trace/result.json' is generating\n",
      "File 'rocprof_trace/result.copy_stats.csv' is generating\n",
      "dump json 2:3                                                                                                    \n",
      "File 'rocprof_trace/result.json' is generating\n",
      "File 'rocprof_trace/result.hip_stats.csv' is generating\n",
      "dump json 45:46                                                                                                    \n",
      "File 'rocprof_trace/result.json' is generating\n"
     ]
    }
   ],
   "source": [
    "!rocprof --hip-trace --hsa-trace -o rocprof_trace/result.csv mat_mult_profiling.exe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b616c5e-6096-427a-af49-74a933c7e64b",
   "metadata": {
    "tags": []
   },
   "source": [
    "Inside the **rocprof_trace** folder you will find the following files:\n",
    "\n",
    "| file | purpose |\n",
    "| --- | --- |\n",
    "| result.sysinfo.txt | System information on available devices |\n",
    "| result.copy_stats.csv | Statistics on all IO calls |\n",
    "| result.hip_stats.csv | Statistics on non-IO HIP function calls |\n",
    "| result.hsa_stats.csv | Statistics on HSA function calls |\n",
    "| result.stats.csv | Statistics on all kernel calls |\n",
    "| result.db | SQLITE3 database of profiling information |\n",
    "| result.json | Trace information in JSON format |\n",
    "| result.csv | Information on kernels such as **mat_mult** |\n",
    "\n",
    "We can load the trace file **rocprof_trace/result.json** using a web browser. In a web browser you can go to this site for a user interface on viewing trace information for offline use.\n",
    "\n",
    "[https://ui.perfetto.dev/](https://ui.perfetto.dev/)\n",
    "\n",
    "Download the trace file **result.json** to your computer and open it with the Perfetto UI in your web browser."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3383f3-4c27-4daf-b506-68486ce77bd1",
   "metadata": {},
   "source": [
    "<figure style=\"margin-left:0; margin-right:auto; width:100%;\">\n",
    "    <img style=\"vertical-align:middle\" src=\"../images/Perfetto_UI.png\">\n",
    "    <figcaption style= \"text-align:lower; margin:1em; float:bottom; vertical-align:bottom;\">Viewing rocprof application traces with Perfetto UI.</figcaption>\n",
    "</figure>\n",
    "\n",
    "If you zoom (using the `wasd` keys) in you can see calls in GPU threads, COPY threads and HOST threads on the CPU. Notice how the **hipEventRecord** function is executed before and after the **hipMemcpy** calls and the **mat_mult** kernel execution. If you click on the **mat_mult** function you can see how long the kernel took to execute.\n",
    "\n",
    "<figure style=\"margin-left:0; margin-right:auto; width:100%;\">\n",
    "    <img style=\"vertical-align:middle\" src=\"../images/Perfetto_UI_kernel.png\">\n",
    "    <figcaption style= \"text-align:lower; margin:1em; float:bottom; vertical-align:bottom;\">Determining the time for a kernel call</figcaption>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb51c18-aeb9-47f5-b5c2-57d8d7ee99cb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Hardware performance counters with rocprof\n",
    "\n",
    "Hardware performance counters are devices in a processor that measure events, such as the number of wavefronts executed, or the number of times a cache is missed. Rocprof can collect performance counters on kernels. The type of performance counter information that can be captured is obtained with this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bedf65ea-5b6d-4262-b447-d4839f33216f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RPL: on '240507_144318' from '/opt/rocm-6.0.2' in '/nethome/tpotter/Pelagos/Projects/HIP_Course/course_material/L5_Profiling'\n",
      "Derived metrics:\n",
      "\n",
      "  gpu-agent1 : TCC_EA1_RDREQ_32B_sum : Number of 32-byte TCC/EA read requests. Sum over TCC EA1s.\n",
      "      TCC_EA1_RDREQ_32B_sum = sum(TCC_EA1_RDREQ_32B,16)\n",
      "\n",
      "  gpu-agent1 : TCC_EA1_RDREQ_sum : Number of TCC/EA read requests (either 32-byte or 64-byte). Sum over TCC EA1s.\n",
      "      TCC_EA1_RDREQ_sum = sum(TCC_EA1_RDREQ,16)\n",
      "\n",
      "  gpu-agent1 : TCC_EA1_WRREQ_sum : Number of transactions (either 32-byte or 64-byte) going over the TC_EA_wrreq interface. Sum over TCC EA1s.\n",
      "      TCC_EA1_WRREQ_sum = sum(TCC_EA1_WRREQ,16)\n",
      "\n",
      "  gpu-agent1 : TCC_EA1_WRREQ_64B_sum : Number of 64-byte transactions going (64-byte write or CMPSWAP) over the TC_EA_wrreq interface. Sum over TCC EA1s.\n",
      "      TCC_EA1_WRREQ_64B_sum = sum(TCC_EA1_WRREQ_64B,16)\n",
      "\n",
      "  gpu-agent1 : TCC_WRREQ1_STALL_max : Number of cycles a write request was stalled. Max over TCC instances.\n",
      "      TCC_WRREQ1_STALL_max = max(TCC_EA1_WRREQ_STALL,16)\n",
      "\n",
      "  gpu-agent1 : RDATA1_SIZE : The total kilobytes fetched from the video memory. This is measured on EA1s.\n",
      "      RDATA1_SIZE = (TCC_EA1_RDREQ_32B_sum*32+(TCC_EA1_RDREQ_sum-TCC_EA1_RDREQ_32B_sum)*64)\n",
      "\n",
      "  gpu-agent1 : WDATA1_SIZE : The total kilobytes written to the video memory. This is measured on EA1s.\n",
      "      WDATA1_SIZE = ((TCC_EA1_WRREQ_sum-TCC_EA1_WRREQ_64B_sum)*32+TCC_EA1_WRREQ_64B_sum*64)\n",
      "\n",
      "  gpu-agent1 : FETCH_SIZE : The total kilobytes fetched from the video memory. This is measured with all extra fetches and any cache or memory effects taken into account.\n",
      "      FETCH_SIZE = (TCC_EA_RDREQ_32B_sum*32+(TCC_EA_RDREQ_sum-TCC_EA_RDREQ_32B_sum)*64+RDATA1_SIZE)/1024\n",
      "\n",
      "  gpu-agent1 : WRITE_SIZE : The total kilobytes written to the video memory. This is measured with all extra fetches and any cache or memory effects taken into account.\n",
      "      WRITE_SIZE = ((TCC_EA_WRREQ_sum-TCC_EA_WRREQ_64B_sum)*32+TCC_EA_WRREQ_64B_sum*64+WDATA1_SIZE)/1024\n",
      "\n",
      "  gpu-agent1 : WRITE_REQ_32B : The total number of 32-byte effective memory writes.\n",
      "      WRITE_REQ_32B = (TCC_EA_WRREQ_sum-TCC_EA_WRREQ_64B_sum)+(TCC_EA1_WRREQ_sum-TCC_EA1_WRREQ_64B_sum)+(TCC_EA_WRREQ_64B_sum+TCC_EA1_WRREQ_64B_sum)*2\n",
      "\n",
      "  gpu-agent1 : TA_BUSY_avr : TA block is busy. Average over TA instances.\n",
      "      TA_BUSY_avr = avr(TA_TA_BUSY,16)\n",
      "\n",
      "  gpu-agent1 : TA_BUSY_max : TA block is busy. Max over TA instances.\n",
      "      TA_BUSY_max = max(TA_TA_BUSY,16)\n",
      "\n",
      "  gpu-agent1 : TA_BUSY_min : TA block is busy. Min over TA instances.\n",
      "      TA_BUSY_min = min(TA_TA_BUSY,16)\n",
      "\n",
      "  gpu-agent1 : TA_FLAT_READ_WAVEFRONTS_sum : Number of flat opcode reads processed by the TA. Sum over TA instances.\n",
      "      TA_FLAT_READ_WAVEFRONTS_sum = sum(TA_FLAT_READ_WAVEFRONTS,16)\n",
      "\n",
      "  gpu-agent1 : TA_FLAT_WRITE_WAVEFRONTS_sum : Number of flat opcode writes processed by the TA. Sum over TA instances.\n",
      "      TA_FLAT_WRITE_WAVEFRONTS_sum = sum(TA_FLAT_WRITE_WAVEFRONTS,16)\n",
      "\n",
      "  gpu-agent1 : TCC_HIT_sum : Number of cache hits. Sum over TCC instances.\n",
      "      TCC_HIT_sum = sum(TCC_HIT,16)\n",
      "\n",
      "  gpu-agent1 : TCC_MISS_sum : Number of cache misses. Sum over TCC instances.\n",
      "      TCC_MISS_sum = sum(TCC_MISS,16)\n",
      "\n",
      "  gpu-agent1 : TCC_EA_RDREQ_32B_sum : Number of 32-byte TCC/EA read requests. Sum over TCC instances.\n",
      "      TCC_EA_RDREQ_32B_sum = sum(TCC_EA_RDREQ_32B,16)\n",
      "\n",
      "  gpu-agent1 : TCC_EA_RDREQ_sum : Number of TCC/EA read requests (either 32-byte or 64-byte). Sum over TCC instances.\n",
      "      TCC_EA_RDREQ_sum = sum(TCC_EA_RDREQ,16)\n",
      "\n",
      "  gpu-agent1 : TCC_EA_WRREQ_sum : Number of transactions (either 32-byte or 64-byte) going over the TC_EA_wrreq interface. Sum over TCC instances.\n",
      "      TCC_EA_WRREQ_sum = sum(TCC_EA_WRREQ,16)\n",
      "\n",
      "  gpu-agent1 : TCC_EA_WRREQ_64B_sum : Number of 64-byte transactions going (64-byte write or CMPSWAP) over the TC_EA_wrreq interface. Sum over TCC instances.\n",
      "      TCC_EA_WRREQ_64B_sum = sum(TCC_EA_WRREQ_64B,16)\n",
      "\n",
      "  gpu-agent1 : TCC_WRREQ_STALL_max : Number of cycles a write request was stalled. Max over TCC instances.\n",
      "      TCC_WRREQ_STALL_max = max(TCC_EA_WRREQ_STALL,16)\n",
      "\n",
      "  gpu-agent1 : TCP_TCP_TA_DATA_STALL_CYCLES_sum : Total number of TCP stalls TA data interface.\n",
      "      TCP_TCP_TA_DATA_STALL_CYCLES_sum = sum(TCP_TCP_TA_DATA_STALL_CYCLES,16)\n",
      "\n",
      "  gpu-agent1 : TCP_TCP_TA_DATA_STALL_CYCLES_max : Maximum number of TCP stalls TA data interface.\n",
      "      TCP_TCP_TA_DATA_STALL_CYCLES_max = max(TCP_TCP_TA_DATA_STALL_CYCLES,16)\n",
      "\n",
      "  gpu-agent1 : VFetchInsts : The average number of vector fetch instructions from the video memory executed per work-item (affected by flow control). Excludes FLAT instructions that fetch from video memory.\n",
      "      VFetchInsts = (SQ_INSTS_VMEM_RD-TA_FLAT_READ_WAVEFRONTS_sum)/SQ_WAVES\n",
      "\n",
      "  gpu-agent1 : VWriteInsts : The average number of vector write instructions to the video memory executed per work-item (affected by flow control). Excludes FLAT instructions that write to video memory.\n",
      "      VWriteInsts = (SQ_INSTS_VMEM_WR-TA_FLAT_WRITE_WAVEFRONTS_sum)/SQ_WAVES\n",
      "\n",
      "  gpu-agent1 : FlatVMemInsts : The average number of FLAT instructions that read from or write to the video memory executed per work item (affected by flow control). Includes FLAT instructions that read from or write to scratch.\n",
      "      FlatVMemInsts = (SQ_INSTS_FLAT-SQ_INSTS_FLAT_LDS_ONLY)/SQ_WAVES\n",
      "\n",
      "  gpu-agent1 : LDSInsts : The average number of LDS read or LDS write instructions executed per work item (affected by flow control).  Excludes FLAT instructions that read from or write to LDS.\n",
      "      LDSInsts = (SQ_INSTS_LDS-SQ_INSTS_FLAT_LDS_ONLY)/SQ_WAVES\n",
      "\n",
      "  gpu-agent1 : FlatLDSInsts : The average number of FLAT instructions that read or write to LDS executed per work item (affected by flow control).\n",
      "      FlatLDSInsts = SQ_INSTS_FLAT_LDS_ONLY/SQ_WAVES\n",
      "\n",
      "  gpu-agent1 : VALUUtilization : The percentage of active vector ALU threads in a wave. A lower number can mean either more thread divergence in a wave or that the work-group size is not a multiple of 64. Value range: 0% (bad), 100% (ideal - no thread divergence).\n",
      "      VALUUtilization = 100*SQ_THREAD_CYCLES_VALU/(SQ_ACTIVE_INST_VALU*MAX_WAVE_SIZE)\n",
      "\n",
      "  gpu-agent1 : VALUBusy : The percentage of GPUTime vector ALU instructions are processed. Value range: 0% (bad) to 100% (optimal).\n",
      "      VALUBusy = 100*SQ_ACTIVE_INST_VALU*4/SIMD_NUM/GRBM_GUI_ACTIVE\n",
      "\n",
      "  gpu-agent1 : SALUBusy : The percentage of GPUTime scalar ALU instructions are processed. Value range: 0% (bad) to 100% (optimal).\n",
      "      SALUBusy = 100*SQ_INST_CYCLES_SALU*4/SIMD_NUM/GRBM_GUI_ACTIVE\n",
      "\n",
      "  gpu-agent1 : FetchSize : The total kilobytes fetched from the video memory. This is measured with all extra fetches and any cache or memory effects taken into account.\n",
      "      FetchSize = FETCH_SIZE\n",
      "\n",
      "  gpu-agent1 : WriteSize : The total kilobytes written to the video memory. This is measured with all extra fetches and any cache or memory effects taken into account.\n",
      "      WriteSize = WRITE_SIZE\n",
      "\n",
      "  gpu-agent1 : MemWrites32B : The total number of effective 32B write transactions to the memory\n",
      "      MemWrites32B = WRITE_REQ_32B\n",
      "\n",
      "  gpu-agent1 : L2CacheHit : The percentage of fetch, write, atomic, and other instructions that hit the data in L2 cache. Value range: 0% (no hit) to 100% (optimal).\n",
      "      L2CacheHit = 100*sum(TCC_HIT,16)/(sum(TCC_HIT,16)+sum(TCC_MISS,16))\n",
      "\n",
      "  gpu-agent1 : MemUnitStalled : The percentage of GPUTime the memory unit is stalled. Try reducing the number or size of fetches and writes if possible. Value range: 0% (optimal) to 100% (bad).\n",
      "      MemUnitStalled = 100*max(TCP_TCP_TA_DATA_STALL_CYCLES,16)/GRBM_GUI_ACTIVE/SE_NUM\n",
      "\n",
      "  gpu-agent1 : WriteUnitStalled : The percentage of GPUTime the Write unit is stalled. Value range: 0% to 100% (bad).\n",
      "      WriteUnitStalled = 100*TCC_WRREQ_STALL_max/GRBM_GUI_ACTIVE\n",
      "\n",
      "  gpu-agent1 : LDSBankConflict : The percentage of GPUTime LDS is stalled by bank conflicts. Value range: 0% (optimal) to 100% (bad).\n",
      "      LDSBankConflict = 100*SQ_LDS_BANK_CONFLICT/GRBM_GUI_ACTIVE/CU_NUM\n",
      "\n",
      "  gpu-agent1 : GPUBusy : The percentage of time GPU was busy.\n",
      "      GPUBusy = 100*GRBM_GUI_ACTIVE/GRBM_COUNT\n",
      "\n",
      "  gpu-agent1 : Wavefronts : Total wavefronts.\n",
      "      Wavefronts = SQ_WAVES\n",
      "\n",
      "  gpu-agent1 : VALUInsts : The average number of vector ALU instructions executed per work-item (affected by flow control).\n",
      "      VALUInsts = SQ_INSTS_VALU/SQ_WAVES\n",
      "\n",
      "  gpu-agent1 : SALUInsts : The average number of scalar ALU instructions executed per work-item (affected by flow control).\n",
      "      SALUInsts = SQ_INSTS_SALU/SQ_WAVES\n",
      "\n",
      "  gpu-agent1 : SFetchInsts : The average number of scalar fetch instructions from the video memory executed per work-item (affected by flow control).\n",
      "      SFetchInsts = SQ_INSTS_SMEM/SQ_WAVES\n",
      "\n",
      "  gpu-agent1 : GDSInsts : The average number of GDS read or GDS write instructions executed per work item (affected by flow control).\n",
      "      GDSInsts = SQ_INSTS_GDS/SQ_WAVES\n",
      "\n",
      "  gpu-agent1 : MemUnitBusy : The percentage of GPUTime the memory unit is active. The result includes the stall time (MemUnitStalled). This is measured with all extra fetches and writes and any cache or memory effects taken into account. Value range: 0% to 100% (fetch-bound).\n",
      "      MemUnitBusy = 100*max(TA_TA_BUSY,16)/GRBM_GUI_ACTIVE/SE_NUM\n",
      "\n",
      "  gpu-agent1 : ALUStalledByLDS : The percentage of GPUTime ALU units are stalled by the LDS input queue being full or the output queue being not ready. If there are LDS bank conflicts, reduce them. Otherwise, try reducing the number of LDS accesses if possible. Value range: 0% (optimal) to 100% (bad).\n",
      "      ALUStalledByLDS = 100*SQ_WAIT_INST_LDS*4/SQ_WAVES/GRBM_GUI_ACTIVE\n",
      "\n",
      "ROCPRofiler: 0 contexts collected\n"
     ]
    }
   ],
   "source": [
    "!rocprof --list-derived"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce2b350-4a1c-4408-b807-9b79e5f911c6",
   "metadata": {},
   "source": [
    "We can specify the counters to collect in a file such as [rocprof_counters.txt](rocprof_counters.txt). Here we specify some commonly used metrics for collection. Each **pmc** line is a unique experiment involving an individual run of the code. In this example we collect stats for the **mat_mult** kernel for the first 64 work-items on GPU 0.\n",
    "\n",
    "```txt\n",
    "# Cache hits and Cache misses\n",
    "pmc: TCC_HIT_sum, TCC_MISS_sum\n",
    "\n",
    "# Total video memory fetched and written\n",
    "pmc: FETCH_SIZE, WRITE_SIZE\n",
    "\n",
    "# Percentage of time the GPU was busy, total wavefronts executed\n",
    "pmc: GPUBusy, Wavefronts\n",
    "\n",
    "# Average number of vector and scalar instructions executed per work-item\n",
    "pmc: VALUInsts, SALUInsts\n",
    "\n",
    "# Average number of vector and scalar fetch instructions per work-item\n",
    "pmc: VFetchInsts, SFetchInsts\n",
    "\n",
    "# Average number of vector write instructions per work-item\n",
    "pmc: VWriteInsts\n",
    "\n",
    "# Average number of shared and global memory read or write instructions per work item\n",
    "pmc: LDSInsts, GDSInsts\n",
    "\n",
    "# Percentage of active vector ALU threads in a wave, percentage of GPU time vector and scalar instructions are processed\n",
    "pmc: VALUUtilization, VALUBusy, SALUBusy, \n",
    "\n",
    "# Percentage of fetch, write, atomic, and other instructions that hit the L2 cache\n",
    "pmc: L2CacheHit\n",
    "\n",
    "# Percentage of time the memory unit is active (including stalled), and just stalled, percentage of time the write unit is stalled\n",
    "pmc: MemUnitBusy, MemUnitStalled, WriteUnitStalled\n",
    "\n",
    "# Percentage of time ALU's are stalled by shared memory access, percentage of GPU time local memory is stalled by bank conflicts\n",
    "pmc: ALUStalledByLDS, LDSBankConflict\n",
    "\n",
    "# Dispatches range, which work-items to profile\n",
    "range: 0 : 64\n",
    "# Which GPU's to profile\n",
    "gpu: 0\n",
    "# Names of kernels to profile\n",
    "kernel: mat_mult\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee575685-ab1f-40bf-aa3f-50224668d1aa",
   "metadata": {
    "tags": []
   },
   "source": [
    "Then we can use rocprof to collect the data for these counters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e6071ab-48a0-4529-bc46-d7ea4d554965",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RPL: on '240507_144333' from '/opt/rocm-6.0.2' in '/nethome/tpotter/Pelagos/Projects/HIP_Course/course_material/L5_Profiling'\n",
      "RPL: profiling '\"mat_mult_profiling.exe\"'\n",
      "RPL: input file 'rocprof_counters.txt'\n",
      "RPL: output dir '/tmp/rpl_data_240507_144333_1334300'\n",
      "RPL: result dir '/tmp/rpl_data_240507_144333_1334300/input0_results_240507_144333'\n",
      "ROCProfiler: input from \"/tmp/rpl_data_240507_144333_1334300/input0.xml\"\n",
      "  gpu_index = 0\n",
      "  kernel = mat_mult\n",
      "  range = 0:64\n",
      "  2 metrics\n",
      "    TCC_HIT_sum, TCC_MISS_sum\n",
      "Device id: 0\n",
      "\tname:                                    AMD Radeon VII\n",
      "\tglobal memory size:                      17163 MB\n",
      "\tavailable registers per block:           65536 \n",
      "\tmax threads per SM or CU:                2560 \n",
      "\tmaximum shared memory size per block:    65 KB\n",
      "\tmaximum shared memory size per SM or CU: 65 KB\n",
      "\tmaximum pitch size for memory copies:    2147 MB\n",
      "\tmax block size:                          (1024,1024,1024)\n",
      "\tmax threads in a block:                  1024\n",
      "\tmax Grid size:                           (2147483647,65536,65536)\n",
      "Time for event \"memcpy\": 0.917 ms (1733.78 MB/s)\n",
      "Time for event \"mat_mult kernel\": 10.208 ms\n",
      "Maximum error (infinity norm) is: 2.28882e-05\n",
      "\n",
      "ROCPRofiler: 1 contexts collected, output directory /tmp/rpl_data_240507_144333_1334300/input0_results_240507_144333\n",
      "RPL: result dir '/tmp/rpl_data_240507_144333_1334300/input10_results_240507_144333'\n",
      "ROCProfiler: input from \"/tmp/rpl_data_240507_144333_1334300/input10.xml\"\n",
      "  gpu_index = 0\n",
      "  kernel = mat_mult\n",
      "  range = 0:64\n",
      "  3 metrics\n",
      "    MemUnitBusy, MemUnitStalled, WriteUnitStalled\n",
      "Device id: 0\n",
      "\tname:                                    AMD Radeon VII\n",
      "\tglobal memory size:                      17163 MB\n",
      "\tavailable registers per block:           65536 \n",
      "\tmax threads per SM or CU:                2560 \n",
      "\tmaximum shared memory size per block:    65 KB\n",
      "\tmaximum shared memory size per SM or CU: 65 KB\n",
      "\tmaximum pitch size for memory copies:    2147 MB\n",
      "\tmax block size:                          (1024,1024,1024)\n",
      "\tmax threads in a block:                  1024\n",
      "\tmax Grid size:                           (2147483647,65536,65536)\n",
      "Time for event \"memcpy\": 0.946 ms (1680.68 MB/s)\n",
      "Time for event \"mat_mult kernel\": 10.587 ms\n",
      "Maximum error (infinity norm) is: 2.28882e-05\n",
      "\n",
      "ROCPRofiler: 1 contexts collected, output directory /tmp/rpl_data_240507_144333_1334300/input10_results_240507_144333\n",
      "RPL: result dir '/tmp/rpl_data_240507_144333_1334300/input11_results_240507_144333'\n",
      "ROCProfiler: input from \"/tmp/rpl_data_240507_144333_1334300/input11.xml\"\n",
      "  gpu_index = 0\n",
      "  kernel = mat_mult\n",
      "  range = 0:64\n",
      "  2 metrics\n",
      "    ALUStalledByLDS, LDSBankConflict\n",
      "Device id: 0\n",
      "\tname:                                    AMD Radeon VII\n",
      "\tglobal memory size:                      17163 MB\n",
      "\tavailable registers per block:           65536 \n",
      "\tmax threads per SM or CU:                2560 \n",
      "\tmaximum shared memory size per block:    65 KB\n",
      "\tmaximum shared memory size per SM or CU: 65 KB\n",
      "\tmaximum pitch size for memory copies:    2147 MB\n",
      "\tmax block size:                          (1024,1024,1024)\n",
      "\tmax threads in a block:                  1024\n",
      "\tmax Grid size:                           (2147483647,65536,65536)\n",
      "Time for event \"memcpy\": 1.050 ms (1513.92 MB/s)\n",
      "Time for event \"mat_mult kernel\": 10.202 ms\n",
      "Maximum error (infinity norm) is: 2.28882e-05\n",
      "\n",
      "ROCPRofiler: 1 contexts collected, output directory /tmp/rpl_data_240507_144333_1334300/input11_results_240507_144333\n",
      "RPL: result dir '/tmp/rpl_data_240507_144333_1334300/input1_results_240507_144333'\n",
      "ROCProfiler: input from \"/tmp/rpl_data_240507_144333_1334300/input1.xml\"\n",
      "  gpu_index = 0\n",
      "  kernel = mat_mult\n",
      "  range = 0:64\n",
      "  1 metrics\n",
      "    FETCH_SIZE\n",
      "Device id: 0\n",
      "\tname:                                    AMD Radeon VII\n",
      "\tglobal memory size:                      17163 MB\n",
      "\tavailable registers per block:           65536 \n",
      "\tmax threads per SM or CU:                2560 \n",
      "\tmaximum shared memory size per block:    65 KB\n",
      "\tmaximum shared memory size per SM or CU: 65 KB\n",
      "\tmaximum pitch size for memory copies:    2147 MB\n",
      "\tmax block size:                          (1024,1024,1024)\n",
      "\tmax threads in a block:                  1024\n",
      "\tmax Grid size:                           (2147483647,65536,65536)\n",
      "Time for event \"memcpy\": 0.940 ms (1691.27 MB/s)\n",
      "Time for event \"mat_mult kernel\": 10.646 ms\n",
      "Maximum error (infinity norm) is: 2.28882e-05\n",
      "\n",
      "ROCPRofiler: 1 contexts collected, output directory /tmp/rpl_data_240507_144333_1334300/input1_results_240507_144333\n",
      "RPL: result dir '/tmp/rpl_data_240507_144333_1334300/input2_results_240507_144333'\n",
      "ROCProfiler: input from \"/tmp/rpl_data_240507_144333_1334300/input2.xml\"\n",
      "  gpu_index = 0\n",
      "  kernel = mat_mult\n",
      "  range = 0:64\n",
      "  1 metrics\n",
      "    WRITE_SIZE\n",
      "Device id: 0\n",
      "\tname:                                    AMD Radeon VII\n",
      "\tglobal memory size:                      17163 MB\n",
      "\tavailable registers per block:           65536 \n",
      "\tmax threads per SM or CU:                2560 \n",
      "\tmaximum shared memory size per block:    65 KB\n",
      "\tmaximum shared memory size per SM or CU: 65 KB\n",
      "\tmaximum pitch size for memory copies:    2147 MB\n",
      "\tmax block size:                          (1024,1024,1024)\n",
      "\tmax threads in a block:                  1024\n",
      "\tmax Grid size:                           (2147483647,65536,65536)\n",
      "Time for event \"memcpy\": 0.911 ms (1745.36 MB/s)\n",
      "Time for event \"mat_mult kernel\": 10.208 ms\n",
      "Maximum error (infinity norm) is: 2.28882e-05\n",
      "\n",
      "ROCPRofiler: 1 contexts collected, output directory /tmp/rpl_data_240507_144333_1334300/input2_results_240507_144333\n",
      "RPL: result dir '/tmp/rpl_data_240507_144333_1334300/input3_results_240507_144333'\n",
      "ROCProfiler: input from \"/tmp/rpl_data_240507_144333_1334300/input3.xml\"\n",
      "  gpu_index = 0\n",
      "  kernel = mat_mult\n",
      "  range = 0:64\n",
      "  2 metrics\n",
      "    GPUBusy, Wavefronts\n",
      "Device id: 0\n",
      "\tname:                                    AMD Radeon VII\n",
      "\tglobal memory size:                      17163 MB\n",
      "\tavailable registers per block:           65536 \n",
      "\tmax threads per SM or CU:                2560 \n",
      "\tmaximum shared memory size per block:    65 KB\n",
      "\tmaximum shared memory size per SM or CU: 65 KB\n",
      "\tmaximum pitch size for memory copies:    2147 MB\n",
      "\tmax block size:                          (1024,1024,1024)\n",
      "\tmax threads in a block:                  1024\n",
      "\tmax Grid size:                           (2147483647,65536,65536)\n",
      "Time for event \"memcpy\": 0.935 ms (1700.53 MB/s)\n",
      "Time for event \"mat_mult kernel\": 10.698 ms\n",
      "Maximum error (infinity norm) is: 2.28882e-05\n",
      "\n",
      "ROCPRofiler: 1 contexts collected, output directory /tmp/rpl_data_240507_144333_1334300/input3_results_240507_144333\n",
      "RPL: result dir '/tmp/rpl_data_240507_144333_1334300/input4_results_240507_144333'\n",
      "ROCProfiler: input from \"/tmp/rpl_data_240507_144333_1334300/input4.xml\"\n",
      "  gpu_index = 0\n",
      "  kernel = mat_mult\n",
      "  range = 0:64\n",
      "  2 metrics\n",
      "    VALUInsts, SALUInsts\n",
      "Device id: 0\n",
      "\tname:                                    AMD Radeon VII\n",
      "\tglobal memory size:                      17163 MB\n",
      "\tavailable registers per block:           65536 \n",
      "\tmax threads per SM or CU:                2560 \n",
      "\tmaximum shared memory size per block:    65 KB\n",
      "\tmaximum shared memory size per SM or CU: 65 KB\n",
      "\tmaximum pitch size for memory copies:    2147 MB\n",
      "\tmax block size:                          (1024,1024,1024)\n",
      "\tmax threads in a block:                  1024\n",
      "\tmax Grid size:                           (2147483647,65536,65536)\n",
      "Time for event \"memcpy\": 0.923 ms (1721.46 MB/s)\n",
      "Time for event \"mat_mult kernel\": 10.168 ms\n",
      "Maximum error (infinity norm) is: 2.28882e-05\n",
      "\n",
      "ROCPRofiler: 1 contexts collected, output directory /tmp/rpl_data_240507_144333_1334300/input4_results_240507_144333\n",
      "RPL: result dir '/tmp/rpl_data_240507_144333_1334300/input5_results_240507_144333'\n",
      "ROCProfiler: input from \"/tmp/rpl_data_240507_144333_1334300/input5.xml\"\n",
      "  gpu_index = 0\n",
      "  kernel = mat_mult\n",
      "  range = 0:64\n",
      "  2 metrics\n",
      "    VFetchInsts, SFetchInsts\n",
      "Device id: 0\n",
      "\tname:                                    AMD Radeon VII\n",
      "\tglobal memory size:                      17163 MB\n",
      "\tavailable registers per block:           65536 \n",
      "\tmax threads per SM or CU:                2560 \n",
      "\tmaximum shared memory size per block:    65 KB\n",
      "\tmaximum shared memory size per SM or CU: 65 KB\n",
      "\tmaximum pitch size for memory copies:    2147 MB\n",
      "\tmax block size:                          (1024,1024,1024)\n",
      "\tmax threads in a block:                  1024\n",
      "\tmax Grid size:                           (2147483647,65536,65536)\n",
      "Time for event \"memcpy\": 0.921 ms (1724.75 MB/s)\n",
      "Time for event \"mat_mult kernel\": 10.214 ms\n",
      "Maximum error (infinity norm) is: 2.28882e-05\n",
      "\n",
      "ROCPRofiler: 1 contexts collected, output directory /tmp/rpl_data_240507_144333_1334300/input5_results_240507_144333\n",
      "RPL: result dir '/tmp/rpl_data_240507_144333_1334300/input6_results_240507_144333'\n",
      "ROCProfiler: input from \"/tmp/rpl_data_240507_144333_1334300/input6.xml\"\n",
      "  gpu_index = 0\n",
      "  kernel = mat_mult\n",
      "  range = 0:64\n",
      "  1 metrics\n",
      "    VWriteInsts\n",
      "Device id: 0\n",
      "\tname:                                    AMD Radeon VII\n",
      "\tglobal memory size:                      17163 MB\n",
      "\tavailable registers per block:           65536 \n",
      "\tmax threads per SM or CU:                2560 \n",
      "\tmaximum shared memory size per block:    65 KB\n",
      "\tmaximum shared memory size per SM or CU: 65 KB\n",
      "\tmaximum pitch size for memory copies:    2147 MB\n",
      "\tmax block size:                          (1024,1024,1024)\n",
      "\tmax threads in a block:                  1024\n",
      "\tmax Grid size:                           (2147483647,65536,65536)\n",
      "Time for event \"memcpy\": 0.972 ms (1634.76 MB/s)\n",
      "Time for event \"mat_mult kernel\": 10.618 ms\n",
      "Maximum error (infinity norm) is: 2.28882e-05\n",
      "\n",
      "ROCPRofiler: 1 contexts collected, output directory /tmp/rpl_data_240507_144333_1334300/input6_results_240507_144333\n",
      "RPL: result dir '/tmp/rpl_data_240507_144333_1334300/input7_results_240507_144333'\n",
      "ROCProfiler: input from \"/tmp/rpl_data_240507_144333_1334300/input7.xml\"\n",
      "  gpu_index = 0\n",
      "  kernel = mat_mult\n",
      "  range = 0:64\n",
      "  2 metrics\n",
      "    LDSInsts, GDSInsts\n",
      "Device id: 0\n",
      "\tname:                                    AMD Radeon VII\n",
      "\tglobal memory size:                      17163 MB\n",
      "\tavailable registers per block:           65536 \n",
      "\tmax threads per SM or CU:                2560 \n",
      "\tmaximum shared memory size per block:    65 KB\n",
      "\tmaximum shared memory size per SM or CU: 65 KB\n",
      "\tmaximum pitch size for memory copies:    2147 MB\n",
      "\tmax block size:                          (1024,1024,1024)\n",
      "\tmax threads in a block:                  1024\n",
      "\tmax Grid size:                           (2147483647,65536,65536)\n",
      "Time for event \"memcpy\": 0.927 ms (1715.22 MB/s)\n",
      "Time for event \"mat_mult kernel\": 10.477 ms\n",
      "Maximum error (infinity norm) is: 2.28882e-05\n",
      "\n",
      "ROCPRofiler: 1 contexts collected, output directory /tmp/rpl_data_240507_144333_1334300/input7_results_240507_144333\n",
      "RPL: result dir '/tmp/rpl_data_240507_144333_1334300/input8_results_240507_144333'\n",
      "ROCProfiler: input from \"/tmp/rpl_data_240507_144333_1334300/input8.xml\"\n",
      "  gpu_index = 0\n",
      "  kernel = mat_mult\n",
      "  range = 0:64\n",
      "  3 metrics\n",
      "    VALUUtilization, VALUBusy, SALUBusy\n",
      "Device id: 0\n",
      "\tname:                                    AMD Radeon VII\n",
      "\tglobal memory size:                      17163 MB\n",
      "\tavailable registers per block:           65536 \n",
      "\tmax threads per SM or CU:                2560 \n",
      "\tmaximum shared memory size per block:    65 KB\n",
      "\tmaximum shared memory size per SM or CU: 65 KB\n",
      "\tmaximum pitch size for memory copies:    2147 MB\n",
      "\tmax block size:                          (1024,1024,1024)\n",
      "\tmax threads in a block:                  1024\n",
      "\tmax Grid size:                           (2147483647,65536,65536)\n",
      "Time for event \"memcpy\": 0.947 ms (1677.56 MB/s)\n",
      "Time for event \"mat_mult kernel\": 10.165 ms\n",
      "Maximum error (infinity norm) is: 2.28882e-05\n",
      "\n",
      "ROCPRofiler: 1 contexts collected, output directory /tmp/rpl_data_240507_144333_1334300/input8_results_240507_144333\n",
      "RPL: result dir '/tmp/rpl_data_240507_144333_1334300/input9_results_240507_144333'\n",
      "ROCProfiler: input from \"/tmp/rpl_data_240507_144333_1334300/input9.xml\"\n",
      "  gpu_index = 0\n",
      "  kernel = mat_mult\n",
      "  range = 0:64\n",
      "  1 metrics\n",
      "    L2CacheHit\n",
      "Device id: 0\n",
      "\tname:                                    AMD Radeon VII\n",
      "\tglobal memory size:                      17163 MB\n",
      "\tavailable registers per block:           65536 \n",
      "\tmax threads per SM or CU:                2560 \n",
      "\tmaximum shared memory size per block:    65 KB\n",
      "\tmaximum shared memory size per SM or CU: 65 KB\n",
      "\tmaximum pitch size for memory copies:    2147 MB\n",
      "\tmax block size:                          (1024,1024,1024)\n",
      "\tmax threads in a block:                  1024\n",
      "\tmax Grid size:                           (2147483647,65536,65536)\n",
      "Time for event \"memcpy\": 0.930 ms (1708.73 MB/s)\n",
      "Time for event \"mat_mult kernel\": 10.588 ms\n",
      "Maximum error (infinity norm) is: 2.28882e-05\n",
      "\n",
      "ROCPRofiler: 1 contexts collected, output directory /tmp/rpl_data_240507_144333_1334300/input9_results_240507_144333\n",
      "File 'rocprof_counters/result.csv' is generating\n"
     ]
    }
   ],
   "source": [
    "!rocprof -i rocprof_counters.txt -o rocprof_counters/result.csv mat_mult_profiling.exe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7440b698-587e-42ec-a944-382dc9d2c481",
   "metadata": {
    "tags": []
   },
   "source": [
    "If your chosen performance counters are supported, then the file [rocprof_counters/result.csv](rocprof_counters/result.csv) should contain a count for every time the counter was triggered. The file [rocprof_counters/example.csv](rocprof_counters/example.csv) is an example file collected with rocprof on **mat_mult_profiling.exe**. This [page](https://docs.amd.com/bundle/ROCProfiler-User-Guide-v5.1/page/rocprof_Command_Line_Tool.html) has information on what the keys in the CSV file mean."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3090705-8e92-4834-b46b-af432243382c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Rocprof under a job manager\n",
    "\n",
    "Rocprof runs fine under a job manager like SLURM, you just need to make an output file for each process launched. For example on SLURM the `$SLURM_JOBID` and `$SLURM_PROCID` environment variables are helpful in separating the output. Put the rocprof commands in a script called **profile.sh**.\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "rocprof -i rocprof_counters.txt -o rocprof_counters/result-$SLURM_JOBID-$SLURM_PROCID.csv mat_mult_profiling_mpi.exe\n",
    "```\n",
    "\n",
    "Then you can run the script from **srun** like this so it picks up the environment variable **$SLURM_PROCID** from within the script.\n",
    "\n",
    "```bash\n",
    "srun -N $SLURM_JOB_NUM_NODES -n $SLURM_NTASKS -c $OMP_NUM_THREADS ./profile.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadc4531-2f2a-4437-a5d5-7ef16e670537",
   "metadata": {},
   "source": [
    "A complete example for using rocprof with an MPI-enabled application is in **course_material/L2_Using_HIP_On_Setonix/rocprof_mpi**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c822fc0-0f3b-4f05-b259-8bc690a7d250",
   "metadata": {},
   "source": [
    "### Rocprofiler API\n",
    "\n",
    "If you'd like to instrument code with profiling calls the **[rocprofiler API](https://github.com/ROCm-Developer-Tools/rocprofiler/blob/amd-master/doc/rocprofiler_spec.md)** is available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c16d9e3-0dc9-478f-8d84-3f71f67579c9",
   "metadata": {},
   "source": [
    "### Tracing with Omnitrace\n",
    "\n",
    "[Omnitrace](https://github.com/AMDResearch/omnitrace) is an AMD research project to collect performance information on a program at runtime. It supports programs written in C, C++, Fortran and Python, as well as compute frameworks like OpenCL and HIP. Load the modules for Omnitrace, (you will find these commands in either the welcome letter or in Lesson 2). Now compile the software with `make`.\n",
    "\n",
    "```bash\n",
    "cd course_material/L5_Profiling\n",
    "make\n",
    "```\n",
    "\n",
    "Then we can use Omnitrace to make a trace of **mat_mult_profiling.exe**.\n",
    "\n",
    "```bash\n",
    "omnitrace-instrument -- mat_mult_profiling.exe\n",
    "```\n",
    "\n",
    "Or we can have omnitrace **instrument** the application for profiling. This is useful if we want to run an application with MPI support.\n",
    "\n",
    "```bash\n",
    "omnitrace-instrument -v -1 -o mat_mult_profiling.inst.exe -- mat_mult_profiling.exe\n",
    "omnitrace-run -- mat_mult_profiling.inst.exe\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bee8406-d101-4c4c-884e-a6a86cfc872f",
   "metadata": {},
   "source": [
    "If you look in the subfolders \n",
    "\n",
    "* **omnitrace-mat_mult_profiling-output**\n",
    "* **omnitrace-mat_mult_profiling.inst-output**, \n",
    "\n",
    "either in **course_material/L5_Profiling** or in the example folder **course_material/L5_Profiling/omnitrace_example** there are subfolders with dates on them. In those subfolders are `*.proto` files for use with perfetto. Download the **.proto** file to your computer and open it with [ui.perfetto.dev](https://ui.perfetto.dev) in a similar way to the json trace files from rocprof. You should see when and for how long functions are executed on the host and for how long kernels are executed on the device, along with a more detailed set of metrics such as CPU frequency and power consumption.\n",
    "\n",
    "<figure style=\"margin-left:0; margin-right:auto; width:100%;\">\n",
    "    <img style=\"vertical-align:middle\" src=\"../images/omnitrace.png\">\n",
    "    <figcaption style= \"text-align:lower; margin:1em; float:bottom; vertical-align:bottom;\">Examining the output from Omnitrace using <a href=\"https://ui.perfetto.dev\">ui.perfetto.dev</a></figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a0a9d0-0406-47a1-bf01-b1eb087a7c75",
   "metadata": {},
   "source": [
    "### Performance measurement with Omniperf\n",
    "\n",
    "The AMD research tool Omniperf [Omniperf](https://github.com/AMDResearch/omniperf) is a powerful tool for measuring the performance of applications on AMD Instinct GPU's like the MI250X on Setonix. It can perform feats like [Roofline Analysis](https://en.wikipedia.org/wiki/Roofline_model). Load the Omniperf modules, using the module load commands from either the welcome letter or from Lesson 2. Then use Omniperf like this to make an analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4b44b0-9b5a-4fbc-8702-c3ea664a4b2c",
   "metadata": {},
   "source": [
    "```bash\n",
    "omniperf profile -n mat_mult -- mat_mult_profiling.exe -o mat_mult.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732da7f1-ddba-434a-ae93-d945401af617",
   "metadata": {},
   "source": [
    "The resulting hardware collection information is in a directory called **workloads/mat_mult**. You can view the output in text format using the command\n",
    "\n",
    "```bash\n",
    "omniperf analyze -p workloads/mat_mult/mi200 &> analysis.txt\n",
    "```\n",
    "\n",
    "or, if you have Omniperf installed to your laptop you can see the results from your web browser. Download the **workloads** directory to your computer and run this command. \n",
    "\n",
    "```bash\n",
    "omniperf analyze -p workloads/mat_mult/mi200 --gui\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcdbb79-6c0e-4133-98be-d54c7631dfba",
   "metadata": {},
   "source": [
    "Then you should be able to go to the location [http://127.0.0.1:8050](http://127.0.0.1:8050) and view the profiling information collected. An example data collection is in **course_material/L5_Profiling/omniperf_example**.\n",
    "\n",
    "#### Roofline models with Omniperf\n",
    "\n",
    "The **[Arithmetic intensity](https://en.wikipedia.org/wiki/Roofline_model#Arithmetic_intensity)** of an algorithm is the ratio of floating point operations (FLOPS) computed per byte transferred. It helps us gauge if an algorithm is likely to be constrained by either the bandwidth or floating point performance of a compute resource. In matrix multiplication the input matrix **A** is of size ($N_{0,C}, N_{1,A}$) and **B** is of size ($N_{1,A}, N_{1,C}$). Every element of matrix **C** requires $N_{1,A}$ loads from A, $N_{1,A}$ loads from B, and 1 store to **C**. It also requires $N_{1,A}$ multiplications and $N_{1,A}$ additions. The arithmetic intensity of matrix multiplication is then\n",
    "\n",
    "$$ a = \\frac{2N1_A}{(2N1_A+1)b} $$\n",
    "\n",
    "where **b** is the number of bytes stored per element. When $N1_A$ is large the theoretical arithmetic intensity for matrix multiplication is\n",
    "\n",
    "$$ a \\approx \\frac{1}{b}. $$\n",
    "\n",
    "If a processor has a peak floating point performance of $\\textbf{F}_{P}$ FLOP/second, and a particular cache can feed that processor at a peak bandwidth of $\\textbf{B}_{P}$ bytes/second, then we can calculate a floating point limit that is dependent on memory bandwidth.\n",
    "\n",
    "$$F_{B} = a  \\frac{\\mbox{FLOP}}{\\mbox{byte}} B_{P}\\frac{\\mbox{byte}}{\\mbox{second}} = a B_{P} \\frac{\\mbox{FLOP}}{\\mbox{second}}$$ \n",
    "\n",
    "The actual attainable floating point performance will be either $F_{B}$ or $F_{P}$, whatever is lower. If we set $F_{B} = F_{P}$ then we can solve for the crossover point in arithmetic intensity.\n",
    "\n",
    "$$a_{0}=\\frac{F_{P}}{B_{P}}$$\n",
    "\n",
    "Therefore the limits (or roofline) on performance is as follows:  \n",
    "\n",
    "$$\n",
    "F = \\left \\{\n",
    "\\begin{array}{rl}\n",
    "aB_{P} & \\mbox{if} \\space a<\\frac{F_{P}}{B_{P}},\\\\\n",
    "F_{P}& \\mbox{otherwise}\n",
    "\\end{array}\n",
    "\\right .\n",
    "$$\n",
    "\n",
    "For example, a single compute device in a AMD Mi250x GPU processor has a peak 32-bit floating point processing rate of $F_{P} = 23.95$ TFLOPS and a peak memory bandwidth of $F_{B}=1.6$ TB/s from global memory. Problems will be constrained by memory bandwidth up to an arithmetic intensity of \n",
    "\n",
    "$$a_{0}=\\frac{23.95}{1.6} \\approx 15$$\n",
    "\n",
    "Shown below is a roofline plot of **mat_mult_profiling.cpp**, showing the various rooflines for the L1, L2, and global memory (HBM) caches. The crossover point for 32-bit floating point arithmetic and global memory is correctly situated around **a=15**. Performance in loading memory cache appears to be close to optimal at the theoretical arithmetic intensity of a=0.25, however significant gains in performance look possible if we can improve loads from main memory.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3038dfaa-3008-4326-bd52-cbf8e5fd9baa",
   "metadata": {},
   "source": [
    "<figure style=\"margin-left:0; margin-right:auto; width:70%;\">\n",
    "    <img style=\"vertical-align:middle\" src=\"../images/roofline_plot.png\">\n",
    "    <figcaption style= \"text-align:lower; margin:1em; float:bottom; vertical-align:bottom;\">Roofline model created from mat_mult_profiling.exe</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302f458b-1bd1-4fb9-8856-b2d94677ded9",
   "metadata": {},
   "source": [
    "If you just want **pdf** versions of the roofline models then run this command\n",
    "\n",
    "```bash\n",
    "omniperf profile -n mat_mult --roof-only -- mat_mult_profiling.exe\n",
    "```\n",
    "\n",
    "The pdf files will be available in **workloads/mat_mult/mi200**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b2aaf2-ef12-44af-b96a-1e2a35a75082",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Performance measurement with NVIDIA tools\n",
    "\n",
    "HIP applications that use the CUDA backend (when CUDA is available and the environment variable `HIP_PLATFORM` is set to `nvidia`) have access to the NVIDIA performance measurement tools such as [NVIDIA Nsight Systems](https://developer.nvidia.com/nsight-systems) and [NVIDIA Nsight Compute](https://developer.nvidia.com/nsight-compute). Here we briefly cover how to use these tools.\n",
    "\n",
    "### Tracing with Nsight Systems\n",
    "\n",
    "The command line application **nsys** can collect traces on **mat_mult_profiling.exe**. Make sure you have re-compiled **mat_mult_profiling.exe** with the `HIP_PLATFORM` environment variable set to `nvidia`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8e3e699-6e16-431b-94f8-39c07100ac87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: nsys: command not found\n"
     ]
    }
   ],
   "source": [
    "!nsys profile -o nsys_trace/results mat_mult_profiling.exe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451fd761-d14f-407b-afd4-319424719b78",
   "metadata": {},
   "source": [
    "Then you can use this command under Linux to view the application trace \n",
    "\n",
    "```bash\n",
    "nsys-ui nsys_trace/results.nsys-rep\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51171e76-80dc-4c03-a1b8-0fe557832350",
   "metadata": {},
   "source": [
    "It's important to note that when using the NVIDIA backend it is important to note that HIP is a **thin layer** over CUDA. NVIDIA performance tools will report usage for the underlying CUDA functions instead of the HIP labelled functions. For example, when using Nsight Systems it will report a call to **cudaDeviceSynchronize** instead of **hipDeviceSynchronize**. One has to make the mental mapping between HIP and CUDA API calls."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5b9bfd-94ac-4158-bb20-300b732adf93",
   "metadata": {},
   "source": [
    "### Hardware collection with Nsight compute\n",
    "\n",
    "Nsight compute has the ability to collect hardware performance counters, however this ability needs either administrator access or access granted to performance counters at the OS level. If this access is possible then the following command will collect hardware performance counters on **mat_mult_profiling.exe**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "220a1c36-8bb4-44ff-b508-1437fb28234f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: ncu: command not found\n"
     ]
    }
   ],
   "source": [
    "!ncu -f -o ncu_counters/results mat_mult_profiling.exe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cd0f55-b0c0-48d1-98bd-b7be9ef65991",
   "metadata": {},
   "source": [
    "Then you can run the command:\n",
    "\n",
    "```bash\n",
    "ncu-ui\n",
    "```\n",
    "\n",
    "To view the hardware performance counter information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ded4596-2705-4315-800b-624f8e062d9c",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This chapter covers how to measure performance in HIP applications. HIP events are tools within the HIP framework to measure the execution time of kernels or memory copies. External tools such as `rocprof` can trace applications to collect information on **when** and for **how long** compute resources are used, as well as collecting low level information from hardware performance counters. Higher level tools like **Omnitrace** and **Omniperf** collect additional information and make the information obtained through rocprof more accessible through GUI-based reporting tools. Since HIP is a cross-platform environment, we conclude the chapter by walking through some performance monitoring tools that NVIDIA backends can make use of."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c38c04c-7c8d-4646-a382-0b02fb91a42e",
   "metadata": {},
   "source": [
    "<address>\n",
    "Written by Dr. Toby Potter of <a href=\"https://www.pelagos-consulting.com\">Pelagos Consulting and Education</a> for the <a href=\"https://pawsey.org.au\">Pawsey Supercomputing Centre</a>.<br>\n",
    "</address>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
