{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49390830-5c18-41f4-9f1d-18bddc857edd",
   "metadata": {},
   "source": [
    "# Optimising compute with concurrent IO in HIP\n",
    "\n",
    "With many iterative processes there is a need to get information **off** the device at regular intervals. Up to this point we have been transferring data off the compute device **after** the kernel is finished. Furthermore, the routines to transport memory between device and host have thus far been used in a blocking manner, meaning the code running on the host *pauses* while the transfer occurs. Most compute devices have the ability to transfer data **while** kernels are running. This means IO transfers can take place during compute, and in some instances may **take place entirely** while the kernel runs. For the cost of additional programming complexity, significant compute savings can be obtained, as the following diagram illustrates:\n",
    "\n",
    "<figure style=\"margin-bottom 3em; margin-top: 2em; margin-left:auto; margin-right:auto; width:100%\">\n",
    "    <img style=\"vertical-align:middle\" src=\"../images/optimising_io.svg\"> <figcaption style= \"text-align:lower; margin:1em; float:bottom; vertical-align:bottom;\">Figure: The difference between sequential and concurrent IO.</figcaption>\n",
    "</figure>\n",
    "\n",
    "## How to enable concurrent IO\n",
    "\n",
    "### Use multiple streams\n",
    "\n",
    "A stream is a place where one can perform work, such as the execution of a kernel or an IO operation. When no stream or the 0 stream is specified there is a default stream called the **null stream** to which work is submitted. Thus far we have been using just the null stream for compute and IO. GPU's have the ability to run streams that do IO **at the same time** as the stream/s devoted to compute. If a kernel launch does not keep all hardware threads busy on the device, then streams allow for multiple kernels to be run at the same time. In either case **there is a performance avantage to be gained** by using multiple streams. \n",
    "\n",
    "Streams are initialised using the commands **hipStreamCreate** and **hipStreamCreateWithFlags**. The latter command allows one to create streams where the null stream can optionally wait for work in the stream to complete (implicitly synchronise) before running its own work. The command **h_create_streams** from <a href=\"../include/hip_helper.hpp\">hip_helper.hpp</a> is used to create additional streams with the option of passing a flag to implicitly synchronise with the null stream. Below is the code for **h_create_streams**:\n",
    "\n",
    "```C++\n",
    "/// Create a number of streams\n",
    "hipStream_t* h_create_streams(size_t nstreams, int synchronise) {\n",
    "    // Blocking is a boolean, 0==no, \n",
    "    assert(nstreams>0);\n",
    "\n",
    "    unsigned int flag = hipStreamDefault;\n",
    "\n",
    "    // If blocking is 0 then set NonBlocking flag\n",
    "    // meaning we don't synchronise with the null stream\n",
    "    if (synchronise == 0) {\n",
    "        flag = hipStreamNonBlocking;\n",
    "    }\n",
    "\n",
    "    // Make the streams\n",
    "    hipStream_t* streams = (hipStream_t*)calloc(nstreams, sizeof(hipStream_t));\n",
    "\n",
    "    for (int i=0; i<nstreams; i++) {\n",
    "        H_ERRCHK(hipStreamCreateWithFlags(&streams[i], flag));\n",
    "    }\n",
    "\n",
    "    return streams;\n",
    "}\n",
    "```\n",
    "\n",
    "If streams operate concurrently, then there must be synchronisation controls. HIP events may be inserted into streams, and the event will reach a complete status after all prior work in the stream is done. Both event and stream synchronisation commands can help establish dependencies between streams. \n",
    "\n",
    "### Use pinned memory and asynchronous IO calls \n",
    "\n",
    "IO functions such as **hipMemcpy** and **hipMemcpy3D** are blocking, this means they wait until the copy is complete before returning. In HIP these IO functions have a sibling asychronous copy command such as **hipMemcpyAsync** and **hipMemcpy3DAsync** that take a stream as an extra argument and return immediately **only** if the host memory involved in the copy is **pinned**. In managed memory allocations the synchronisation occurs in the background and there is no need for an explicit synchronisation call."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec04ce78-df29-42dd-a38a-822a23407cae",
   "metadata": {},
   "source": [
    "## Example with the 2D wave equation\n",
    "\n",
    "The [scalar wave equation](https://en.wikipedia.org/wiki/Wave_equation) adequately describes a number of wavelike phenomena. If **U** is a 2D grid storing the amplitude of the wave at every location (the wavefield), **V** is a 2D grid storing velocity, and **t** is time, then 2D waves propagate according to the formula,\n",
    "\n",
    "$$\\frac{\\partial^2 \\textbf{U}}{{\\partial t}^2}=\\textbf{V}^2 \\left (\\frac{\\partial^2 \\textbf{U}}{{\\partial x_{0}}^2}+\\frac{\\partial^2 \\textbf{U}}{{\\partial x_{1}}^2} \\right)+f(t)$$\n",
    "\n",
    "where $x_0$ and $x_1$ are spatial directions and $f(t)$ is a forcing term. If $\\Delta t$ is the time step a second-order finite-difference approximation to the time derivative is given in terms of the amplitude at timesteps $\\textbf{U}_{0}, \\textbf{U}_{1}$ and $\\textbf{U}_{2}.$ \n",
    "\n",
    "$$\\frac{\\partial^2 \\textbf{U}}{{\\partial t}^2} \\approx \\frac{1}{\\Delta t^2} \\left ( \\textbf{U}_{0} -2 \\textbf{U}_{1}+\\textbf{U}_{2} \\right ) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78810535-577b-4789-a36c-d5a43d759bd7",
   "metadata": {},
   "source": [
    "Replace $\\frac{\\partial^2 \\textbf{U}}{{\\partial t}^2}$ with $\\frac{1}{\\Delta t^2} \\left( \\textbf{U}_{0} -2 \\textbf{U}_{1}+\\textbf{U}_{2} \\right )$ and solve for $\\textbf{U}_{2}$.\n",
    "\n",
    "$$\\textbf{U}_{2} \\approx 2 \\textbf{U}_{1} - \\textbf{U}_{0} + \\Delta t^2\\textbf{V}^2 \\left (\\frac{\\partial^2 \\textbf{U}_{1}}{{\\partial x_{0}}^2}+\\frac{\\partial^2 \\textbf{U}_{1}}{{\\partial x_{1}}^2} \\right)+f_{1}$$\n",
    "\n",
    "The equation above is now an iterative formula to generate the amplitude at the next timestep $\\textbf{U}_2$ if we know the present ampltiude $\\textbf{U}_{1}$ and past amplitude $\\textbf{U}_{0}.$ We also use finite difference approximations for the spatial derivatives, and express the spatial derivatives as a matrix multiplied by $\\textbf{U}_{1}$, but this complexity is unnecessary to show here. All we need to know is that the next timestep is a function ${\\textbf{F}}$ of the present and past timesteps, the velocity, and the forcing term.\n",
    "\n",
    "$$\\textbf{U}_{2}(t)=\\textbf{F}(\\textbf{U}_0(t), \\textbf{U}_1(t), \\textbf{V}, f_{1}(t))$$\n",
    "\n",
    "> In geophysics we usually use a [Ricker Wavelet](https://wiki.seg.org/wiki/Dictionary:Ricker_wavelet) for the forcing term $f$, and usually inject that wavelet into one cell within the grid as time progresses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d9870e-642e-4357-a49a-bf220fd42d7c",
   "metadata": {},
   "source": [
    "### Kernel implementation\n",
    "\n",
    "In [wave2d_sync.cpp](wave2d_sync.cpp), [wave2d_async_streams.cpp](wave2d_async_streams.cpp), and [wave2d_async_events.cpp](wave2d_async_events.cpp) is a kernel called **wave2d_4o** that implements the function **F** above. HIP device allocations store $\\textbf{U}_{0}, \\textbf{U}_{1}, \\textbf{U}_{2}$, and $\\textbf{V}$ on the compute device. Here is the kernel code:\n",
    "\n",
    "```C++\n",
    "// Kernel to solve the wave equation with fourth-order accuracy in space\n",
    "__global__ void wave2d_4o (\n",
    "        // Arguments\n",
    "        float_type* U0,\n",
    "        float_type* U1,\n",
    "        float_type* U2,\n",
    "        float_type* V,\n",
    "        size_t N0,\n",
    "        size_t N1,\n",
    "        float dt2,\n",
    "        float inv_dx02,\n",
    "        float inv_dx12,\n",
    "        // Position, frequency, and time for the\n",
    "        // wavelet injection\n",
    "        size_t P0,\n",
    "        size_t P1,\n",
    "        float pi2fm2t2) {    \n",
    "\n",
    "    // U2, U1, U0, V is of size (N0, N1)\n",
    "    size_t i0 = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    size_t i1 = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    // Required padding and coefficients for spatial finite difference\n",
    "    const int pad_l=2, pad_r=2, ncoeffs=5;\n",
    "    float coeffs[ncoeffs] = {-0.083333336f, 1.3333334f, -2.5f, 1.3333334f, -0.083333336f};\n",
    "    \n",
    "    // Limit i0 and i1 to the region of U2 within the padding\n",
    "    i0=min(i0, (size_t)(N0-1-pad_r));\n",
    "    i1=min(i1, (size_t)(N1-1-pad_r));\n",
    "    i0=max((size_t)pad_l, i0);\n",
    "    i1=max((size_t)pad_l, i1);\n",
    "    \n",
    "    // Position within the grid as a 1D offset\n",
    "    long offset=i0*N1+i1;\n",
    "    \n",
    "    // Temporary storage\n",
    "    float temp0=0.0f, temp1=0.0f;\n",
    "    float tempV=V[offset];\n",
    "    \n",
    "    // Calculate the Laplacian\n",
    "    for (long n=0; n<ncoeffs; n++) {\n",
    "        // Stride in dim0 is N1        \n",
    "        temp0+=coeffs[n]*U1[offset+(n*(long)N1)-(pad_l*(long)N1)];\n",
    "        // Stride in dim1 is 1\n",
    "        temp1+=coeffs[n]*U1[offset+n-pad_l];\n",
    "    }\n",
    "    \n",
    "    // Calculate the wavefield U2 at the next timestep\n",
    "    U2[offset]=(2.0f*U1[offset])-U0[offset]+((dt2*tempV*tempV)*(temp0*inv_dx02+temp1*inv_dx12));\n",
    "    \n",
    "    // Inject the forcing term at coordinates (P0, P1)\n",
    "    if ((i0==P0) && (i1==P1)) {\n",
    "        U2[offset]+=(1.0f-2.0f*pi2fm2t2)*exp(-pi2fm2t2);\n",
    "    }\n",
    "    \n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d266b76-ce52-455e-a9b5-c37b5238307c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Problem setup\n",
    "\n",
    "For this problem we create the 2D grid as a square box of size $(N0,N1)=(256,256)$. The velocity is uniform at 343m/s, which is approximately the speed of sound in air. Then we use a Ricker wavelet as a forcing term $f(t)$ to 'let off a firework' in the middle of the box and run a number of timesteps to see how a sound wave propagates in the box. \n",
    "\n",
    "<figure style=\"margin-bottom 3em; margin-top: 2em; margin-left:auto; margin-right:auto; width:80%\">\n",
    "    <img style=\"vertical-align:middle\" src=\"../images/wave2d_problem.svg\"> <figcaption style= \"text-align:lower; margin:1em; float:bottom; vertical-align:bottom;\">Figure: Problem setup for the 2D wave equation.</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcda1835-1595-438a-9d57-18f6839bc03c",
   "metadata": {},
   "source": [
    "The programs setup a velocity and wavefield of size (N0, N1). At each timestep the kernel **wave2d_4o** is used to update the solution and a Ricker wavelet is injected into the middle of the box. Enough timesteps are alloted so that the wave propagates through the medium and reflects off the walls. Wavefield arrays that are no longer needed are recycled for efficiency.\n",
    "\n",
    "The Python code below readies the framework for plotting the answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcced67c-c103-4ec3-b21e-e83a00f313de",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import subprocess\n",
    "from ipywidgets import widgets\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import animation, rc\n",
    "from IPython.display import HTML\n",
    "\n",
    "sys.path.insert(0, os.path.abspath(\"../common\"))\n",
    "\n",
    "import py_helper\n",
    "\n",
    "float_type = np.float32\n",
    "\n",
    "defines=py_helper.load_defines(\"mat_size.hpp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1637b9-c045-4b17-8492-a8a7c1f2f606",
   "metadata": {},
   "source": [
    "### Sequential (synchronous) IO solution\n",
    "\n",
    "In [wave2d_sync.cpp](wave2d_sync.cpp) we use an array of three HIP device allocations to represent the wavefield at timesteps (0,1,2). The null stream (stream 0) is used for both kernel execution and IO.\n",
    "\n",
    "<figure style=\"margin-bottom 3em; margin-top: 2em; margin-left:auto; margin-right:auto; width:100%\">\n",
    "    <img style=\"vertical-align:middle\" src=\"../images/sequential_io.svg\"> <figcaption style= \"text-align:lower; margin:1em; float:bottom; vertical-align:bottom;\">Figure: Sequential IO solution.</figcaption>\n",
    "</figure>\n",
    "\n",
    "After setting up the wavefields, the synchronous solution loops through timesteps. At each timestep:\n",
    "\n",
    "1. The kernel is launched to compute the next wavefield $\\textbf{U}_{2}$ from $\\textbf{U}_{0}$, $\\textbf{U}_{1}$ and $\\textbf{V}$.\n",
    "1. The past wavefield $\\textbf{U}_{0}$ is copied back to the output grid **out_h** as a plane in the 3D stack.\n",
    "\n",
    "The code for the central time loop in [wave2d_sync.cpp](wave2d_sync.cpp) is below:\n",
    "\n",
    "```C++\n",
    "    for (int n=0; n<NT; n++) {\n",
    "        // Get the wavefields\n",
    "        U0_d = U_ds[n%nscratch];\n",
    "        U1_d = U_ds[(n+1)%nscratch];\n",
    "        U2_d = U_ds[(n+2)%nscratch];\n",
    "        \n",
    "        // Shifted time\n",
    "        t = n*dt-2.0*td;\n",
    "        pi2fm2t2 = pi*pi*fm*fm*t*t;\n",
    "        \n",
    "        // Launch the kernel using hipLaunchKernelGGL method\n",
    "        // Use 0 when choosing the default (null) stream\n",
    "        hipLaunchKernelGGL(wave2d_4o, \n",
    "            grid_nblocks, block_size, sharedMemBytes, 0,\n",
    "            U0_d, U1_d, U2_d, V_d,\n",
    "            N0, N1, dt2,\n",
    "            inv_dx02, inv_dx12,\n",
    "            P0, P1, pi2fm2t2\n",
    "        );\n",
    "                           \n",
    "        // Check the status of the kernel launch\n",
    "        H_ERRCHK(hipGetLastError());\n",
    "          \n",
    "        // Copy the wavefield back to the host\n",
    "        // hipMemcpy is a barrier\n",
    "        if (n>1 && n<NT-1) { // For consistency with the async solution\n",
    "            H_ERRCHK(\n",
    "                hipMemcpy(\n",
    "                    (void*)&out_h[n*N0*N1],\n",
    "                    U0_d,\n",
    "                    nbytes_U,\n",
    "                    hipMemcpyDeviceToHost\n",
    "                )\n",
    "            );\n",
    "        }\n",
    "    }\n",
    "```\n",
    "\n",
    "Notice that with the **hipMemcpy** call above, we used an address located within the host memory allocation **out_h** to perform the copy. This technique is **not valid** for use in asynchronous copies.\n",
    "\n",
    "#### Make and run the application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6900c8bb-b3d7-4c56-958d-7dc9b28a3f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PATH'] = f\"{os.environ['PATH']}:../install/bin\"\n",
    "\n",
    "# At a Bash terminal you need to do this instead\n",
    "# source ../env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6e3ecc4-10ea-4f0c-8b9b-b62b61018e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target hip_helper\u001b[0m\n",
      "[  2%] Built target hip_helper\n",
      "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target hello_devices.exe\u001b[0m\n",
      "[  4%] Built target hello_devices.exe\n",
      "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target hello_devices_mpi_onefile.exe\u001b[0m\n",
      "[  5%] Built target hello_devices_mpi_onefile.exe\n",
      "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target hello_devices_mpi.exe\u001b[0m\n",
      "[  8%] Built target hello_devices_mpi.exe\n",
      "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target hello_jobstep.exe\u001b[0m\n",
      "[ 10%] Built target hello_jobstep.exe\n",
      "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target mat_mult_profiling_mpi.exe\u001b[0m\n",
      "[ 13%] Built target mat_mult_profiling_mpi.exe\n",
      "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target mat_mult.exe\u001b[0m\n",
      "[ 15%] Built target mat_mult.exe\n",
      "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target mat_mult_bug.exe\u001b[0m\n",
      "[ 17%] Built target mat_mult_bug.exe\n",
      "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target mat_mult_memcheck.exe\u001b[0m\n",
      "[ 19%] Built target mat_mult_memcheck.exe\n",
      "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target mat_mult_printf.exe\u001b[0m\n",
      "[ 21%] Built target mat_mult_printf.exe\n",
      "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target mat_mult_profiling.exe\u001b[0m\n",
      "[ 22%] Built target mat_mult_profiling.exe\n",
      "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target mat_mult_managed_mem.exe\u001b[0m\n",
      "[ 24%] Built target mat_mult_managed_mem.exe\n",
      "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target mat_mult_pinned_mem.exe\u001b[0m\n",
      "[ 26%] Built target mat_mult_pinned_mem.exe\n",
      "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target mat_mult_pitched_mem.exe\u001b[0m\n",
      "[ 28%] Built target mat_mult_pitched_mem.exe\n",
      "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target mat_mult_shared_faulty.exe\u001b[0m\n",
      "[ 30%] Built target mat_mult_shared_faulty.exe\n",
      "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target mat_mult_shared_vector.exe\u001b[0m\n",
      "[ 32%] Built target mat_mult_shared_vector.exe\n",
      "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target mat_mult_shared.exe\u001b[0m\n",
      "[ 34%] Built target mat_mult_shared.exe\n",
      "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target vector_types.exe\u001b[0m\n",
      "[ 36%] Built target vector_types.exe\n",
      "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target atomics.exe\u001b[0m\n",
      "[ 37%] Built target atomics.exe\n",
      "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target mat_mult_AT.exe\u001b[0m\n",
      "[ 39%] Built target mat_mult_AT.exe\n",
      "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target mat_mult_BT.exe\u001b[0m\n",
      "[ 41%] Built target mat_mult_BT.exe\n",
      "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target mat_mult_double.exe\u001b[0m\n",
      "[ 43%] Built target mat_mult_double.exe\n",
      "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target mat_mult_float.exe\u001b[0m\n",
      "[ 45%] Built target mat_mult_float.exe\n",
      "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target mat_mult_shared_A.exe\u001b[0m\n",
      "[ 47%] Built target mat_mult_shared_A.exe\n",
      "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target mat_mult_shared_B.exe\u001b[0m\n",
      "[ 49%] Built target mat_mult_shared_B.exe\n",
      "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target mat_mult_float_restrict.exe\u001b[0m\n",
      "[ 51%] Built target mat_mult_float_restrict.exe\n",
      "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target mat_mult_tile_shared_A.exe\u001b[0m\n",
      "[ 53%] Built target mat_mult_tile_shared_A.exe\n",
      "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target mat_mult_tile_shared_B.exe\u001b[0m\n",
      "[ 55%] Built target mat_mult_tile_shared_B.exe\n",
      "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target mat_mult_tile_shared_AB.exe\u001b[0m\n",
      "[ 57%] Built target mat_mult_tile_shared_AB.exe\n",
      "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target mat_mult_tile_shared_A_vector.exe\u001b[0m\n",
      "[ 59%] Built target mat_mult_tile_shared_A_vector.exe\n",
      "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target mat_mult_tile_shared_B_vector.exe\u001b[0m\n",
      "[ 61%] Built target mat_mult_tile_shared_B_vector.exe\n",
      "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target mat_mult_tile_shared_AB_vector.exe\u001b[0m\n",
      "[ 62%] Built target mat_mult_tile_shared_AB_vector.exe\n",
      "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target mat_mult_float_hipblas.exe\u001b[0m\n",
      "[ 63%] Built target mat_mult_float_hipblas.exe\n",
      "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target mat_mult_float_md_hipblas.exe\u001b[0m\n",
      "[ 65%] Built target mat_mult_float_md_hipblas.exe\n",
      "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target wave2d_sync.exe\u001b[0m\n",
      "[ 67%] Built target wave2d_sync.exe\n",
      "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target wave2d_async.exe\u001b[0m\n",
      "[ 69%] Built target wave2d_async.exe\n",
      "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target wave2d_async_events.exe\u001b[0m\n",
      "[ 71%] Built target wave2d_async_events.exe\n",
      "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target wave2d_async_streams.exe\u001b[0m\n",
      "[ 73%] Built target wave2d_async_streams.exe\n",
      "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target elementwise_challenge.exe\u001b[0m\n",
      "[ 75%] Built target elementwise_challenge.exe\n",
      "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target elementwise_challenge_answer.exe\u001b[0m\n",
      "[ 77%] Built target elementwise_challenge_answer.exe\n",
      "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target exercise_debug.exe\u001b[0m\n",
      "[ 79%] Built target exercise_debug.exe\n",
      "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target exercise_debug_answer.exe\u001b[0m\n",
      "[ 81%] Built target exercise_debug_answer.exe\n",
      "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target exercise_profiling.exe\u001b[0m\n",
      "[ 83%] Built target exercise_profiling.exe\n",
      "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target exercise_profiling_answer.exe\u001b[0m\n",
      "[ 84%] Built target exercise_profiling_answer.exe\n",
      "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target exercise_rectcopy.exe\u001b[0m\n",
      "[ 86%] Built target exercise_rectcopy.exe\n",
      "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target exercise_rectcopy_answer.exe\u001b[0m\n",
      "[ 88%] Built target exercise_rectcopy_answer.exe\n",
      "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target exercise_layer_cake.exe\u001b[0m\n",
      "[ 90%] Built target exercise_layer_cake.exe\n",
      "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target exercise_layer_cake_answers.exe\u001b[0m\n",
      "[ 92%] Built target exercise_layer_cake_answers.exe\n",
      "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target exercise_xcorr.exe\u001b[0m\n",
      "[ 94%] Built target exercise_xcorr.exe\n",
      "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target exercise_xcorr_answers.exe\u001b[0m\n",
      "[ 96%] Built target exercise_xcorr_answers.exe\n",
      "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target exercise_xcorr_testbench.exe\u001b[0m\n",
      "[ 98%] Built target exercise_xcorr_testbench.exe\n",
      "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target exercise_xcorr_testbench_answers.exe\u001b[0m\n",
      "[100%] Built target exercise_xcorr_testbench_answers.exe\n",
      "\u001b[36mInstall the project...\u001b[0m\n",
      "-- Install configuration: \"RELEASE\"\n"
     ]
    }
   ],
   "source": [
    "!build all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bac6b827-4ee7-4f24-9cbe-bc856bc18691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device id: 0\n",
      "\tname:                                    NVIDIA GeForce RTX 3060 Laptop GPU\n",
      "\tglobal memory size:                      6219 MB\n",
      "\tavailable registers per block:           65536 \n",
      "\tmax threads per SM or CU:                1536 \n",
      "\tmaximum shared memory size per block:    49 KB\n",
      "\tmaximum shared memory size per SM or CU: 0 KB\n",
      "\tmaximum pitch size for memory copies:    2147 MB\n",
      "\tmax block size:                          (1024,1024,64)\n",
      "\tmax threads in a block:                  1024\n",
      "\tmax Grid size:                           (2147483647,65535,65535)\n",
      "dt=0.001166, Vmax=343.000000\n",
      "dt=0.00116618, fm=34.3, Vmax=343, dt2=1.35998e-06\n",
      "The synchronous calculation took 34.705000 milliseconds.\n"
     ]
    }
   ],
   "source": [
    "!run wave2d_sync.exe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa42486-4d6f-4432-9c22-173f9f6c290d",
   "metadata": {},
   "source": [
    "#### Plot the output wavefield\n",
    "\n",
    "At the end of iteration a binary file containing the wavefield at every timestep is written to the file **array_out.dat**. We read in this wavefield and plot it below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e385ad3-3850-4231-b005-35909c5d1374",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "196b415ba7fe4a76b750e5d842528810",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='n', max=639), Output()), _dom_classes=('widget-interact'â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Read the output file back in for display\n",
    "output_sync=np.fromfile(\"array_out.dat\", dtype=float_type)\n",
    "nimages_sync=int(output_sync.size//(defines[\"N0_U\"]*defines[\"N1_U\"]))\n",
    "images_sync=output_sync.reshape(nimages_sync, defines[\"N0_U\"], defines[\"N1_U\"])\n",
    "\n",
    "py_helper.plot_slices(images_sync)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d690e0e-e7ab-4e33-a2aa-2988dfd62958",
   "metadata": {},
   "source": [
    "#### Application trace\n",
    "\n",
    "The script **make_traces.sh** produces traces in the **rocprof_trace** folder for each of the synchronous and concurrent IO solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c7d86a4-f911-4837-9917-2e4fcab10124",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RPL: on '231025_142656' from '/opt/rocm-5.6.1' in '/home/toby/Pelagos/Projects/HIP_Course/course_material/L8_IO_Optimisation'\n",
      "RPL: profiling '\"./wave2d_sync.exe\"'\n",
      "RPL: input file ''\n",
      "RPL: output dir '/tmp/rpl_data_231025_142656_20399'\n",
      "RPL: result dir '/tmp/rpl_data_231025_142656_20399/input_results_231025_142656'\n",
      "ROCProfiler: input from \"/tmp/rpl_data_231025_142656_20399/input.xml\"\n",
      "  0 metrics\n",
      "ROCtracer (20420):\n",
      "    HSA-trace(*)\n",
      "    HSA-activity-trace()\n",
      "    HIP-trace(*)\n",
      "Device id: 0\n",
      "\tname:                                    AMD Radeon Graphics\n",
      "\tglobal memory size:                      536 MB\n",
      "\tavailable registers per block:           65536 \n",
      "\tmax threads per SM or CU:                2048 \n",
      "\tmaximum shared memory size per block:    65 KB\n",
      "\tmaximum shared memory size per SM or CU: 65 KB\n",
      "\tmaximum pitch size for memory copies:    402 MB\n",
      "\tmax block size:                          (1024,1024,1024)\n",
      "\tmax threads in a block:                  1024\n",
      "\tmax Grid size:                           (2147483647,2147483647,2147483647)\n",
      "dt=0.001166, Vmax=343.000000\n",
      "dt=0.00116618, fm=34.3, Vmax=343, dt2=1.35998e-06\n",
      "The synchronous calculation took 68.777000 milliseconds.\n",
      "\n",
      "ROCPRofiler: 643 contexts collected, output directory /tmp/rpl_data_231025_142656_20399/input_results_231025_142656\n",
      "hsa_copy_deps: 1\n",
      "scan hsa API data 18975:18976                                                                                                    hsa_copy_deps: 0\n",
      "scan hip API data 3222:3223                                                                                                    File '/home/toby/Pelagos/Projects/HIP_Course/course_material/L8_IO_Optimisation/rocprof_trace/wave2d_sync.csv' is generating\n",
      "File '/home/toby/Pelagos/Projects/HIP_Course/course_material/L8_IO_Optimisation/rocprof_trace/wave2d_sync.stats.csv' is generating\n",
      "dump json 642:643                                                                                                    \n",
      "File '/home/toby/Pelagos/Projects/HIP_Course/course_material/L8_IO_Optimisation/rocprof_trace/wave2d_sync.json' is generating\n",
      "File '/home/toby/Pelagos/Projects/HIP_Course/course_material/L8_IO_Optimisation/rocprof_trace/wave2d_sync.hsa_stats.csv' is generating\n",
      "dump json 19618:19619                                                                                                    \n",
      "File '/home/toby/Pelagos/Projects/HIP_Course/course_material/L8_IO_Optimisation/rocprof_trace/wave2d_sync.json' is generating\n",
      "File '/home/toby/Pelagos/Projects/HIP_Course/course_material/L8_IO_Optimisation/rocprof_trace/wave2d_sync.copy_stats.csv' is generating\n",
      "dump json 637:638                                                                                                    \n",
      "File '/home/toby/Pelagos/Projects/HIP_Course/course_material/L8_IO_Optimisation/rocprof_trace/wave2d_sync.json' is generating\n",
      "File '/home/toby/Pelagos/Projects/HIP_Course/course_material/L8_IO_Optimisation/rocprof_trace/wave2d_sync.hip_stats.csv' is generating\n",
      "dump json 3222:3223                                                                                                    \n",
      "File '/home/toby/Pelagos/Projects/HIP_Course/course_material/L8_IO_Optimisation/rocprof_trace/wave2d_sync.json' is generating\n",
      "RPL: on '231025_142658' from '/opt/rocm-5.6.1' in '/home/toby/Pelagos/Projects/HIP_Course/course_material/L8_IO_Optimisation'\n",
      "RPL: profiling '\"./wave2d_async_streams.exe\"'\n",
      "RPL: input file ''\n",
      "RPL: output dir '/tmp/rpl_data_231025_142658_20505'\n",
      "RPL: result dir '/tmp/rpl_data_231025_142658_20505/input_results_231025_142658'\n",
      "ROCProfiler: input from \"/tmp/rpl_data_231025_142658_20505/input.xml\"\n",
      "  0 metrics\n",
      "ROCtracer (20525):\n",
      "    HSA-trace(*)\n",
      "    HSA-activity-trace()\n",
      "    HIP-trace(*)\n",
      "Device id: 0\n",
      "\tname:                                    AMD Radeon Graphics\n",
      "\tglobal memory size:                      536 MB\n",
      "\tavailable registers per block:           65536 \n",
      "\tmax threads per SM or CU:                2048 \n",
      "\tmaximum shared memory size per block:    65 KB\n",
      "\tmaximum shared memory size per SM or CU: 65 KB\n",
      "\tmaximum pitch size for memory copies:    402 MB\n",
      "\tmax block size:                          (1024,1024,1024)\n",
      "\tmax threads in a block:                  1024\n",
      "\tmax Grid size:                           (2147483647,2147483647,2147483647)\n",
      "dt=0.001166, Vmax=343.000000\n",
      "dt=0.00116618, fm=34.3, Vmax=343, dt2=1.35998e-06\n",
      "The asynchronous calculation took 35.852000 milliseconds.\n",
      "\n",
      "ROCPRofiler: 1282 contexts collected, output directory /tmp/rpl_data_231025_142658_20505/input_results_231025_142658\n",
      "hsa_copy_deps: 1\n",
      "scan hsa API data 26919:26920                                                                                                    hsa_copy_deps: 0\n",
      "scan hip API data 7085:7086                                                                                                    File '/home/toby/Pelagos/Projects/HIP_Course/course_material/L8_IO_Optimisation/rocprof_trace/wave2d_async_streams.csv' is generating\n",
      "File '/home/toby/Pelagos/Projects/HIP_Course/course_material/L8_IO_Optimisation/rocprof_trace/wave2d_async_streams.stats.csv' is generating\n",
      "dump json 1281:1282                                                                                                    \n",
      "File '/home/toby/Pelagos/Projects/HIP_Course/course_material/L8_IO_Optimisation/rocprof_trace/wave2d_async_streams.json' is generating\n",
      "File '/home/toby/Pelagos/Projects/HIP_Course/course_material/L8_IO_Optimisation/rocprof_trace/wave2d_async_streams.hsa_stats.csv' is generating\n",
      "dump json 28201:28202                                                                                                    \n",
      "File '/home/toby/Pelagos/Projects/HIP_Course/course_material/L8_IO_Optimisation/rocprof_trace/wave2d_async_streams.json' is generating\n",
      "File '/home/toby/Pelagos/Projects/HIP_Course/course_material/L8_IO_Optimisation/rocprof_trace/wave2d_async_streams.copy_stats.csv' is generating\n",
      "dump json 0:1                                                                                                    \n",
      "File '/home/toby/Pelagos/Projects/HIP_Course/course_material/L8_IO_Optimisation/rocprof_trace/wave2d_async_streams.json' is generating\n",
      "File '/home/toby/Pelagos/Projects/HIP_Course/course_material/L8_IO_Optimisation/rocprof_trace/wave2d_async_streams.hip_stats.csv' is generating\n",
      "dump json 7085:7086                                                                                                    \n",
      "File '/home/toby/Pelagos/Projects/HIP_Course/course_material/L8_IO_Optimisation/rocprof_trace/wave2d_async_streams.json' is generating\n",
      "RPL: on '231025_142702' from '/opt/rocm-5.6.1' in '/home/toby/Pelagos/Projects/HIP_Course/course_material/L8_IO_Optimisation'\n",
      "RPL: profiling '\"./wave2d_async_events.exe\"'\n",
      "RPL: input file ''\n",
      "RPL: output dir '/tmp/rpl_data_231025_142702_20584'\n",
      "RPL: result dir '/tmp/rpl_data_231025_142702_20584/input_results_231025_142702'\n",
      "ROCProfiler: input from \"/tmp/rpl_data_231025_142702_20584/input.xml\"\n",
      "  0 metrics\n",
      "ROCtracer (20604):\n",
      "    HSA-trace(*)\n",
      "    HSA-activity-trace()\n",
      "    HIP-trace(*)\n",
      "Device id: 0\n",
      "\tname:                                    AMD Radeon Graphics\n",
      "\tglobal memory size:                      536 MB\n",
      "\tavailable registers per block:           65536 \n",
      "\tmax threads per SM or CU:                2048 \n",
      "\tmaximum shared memory size per block:    65 KB\n",
      "\tmaximum shared memory size per SM or CU: 65 KB\n",
      "\tmaximum pitch size for memory copies:    402 MB\n",
      "\tmax block size:                          (1024,1024,1024)\n",
      "\tmax threads in a block:                  1024\n",
      "\tmax Grid size:                           (2147483647,2147483647,2147483647)\n",
      "dt=0.001166, Vmax=343.000000\n",
      "dt=0.00116618, fm=34.3, Vmax=343, dt2=1.35998e-06\n",
      "The asynchronous calculation took 30.842000 milliseconds.\n",
      "\n",
      "ROCPRofiler: 1282 contexts collected, output directory /tmp/rpl_data_231025_142702_20584/input_results_231025_142702\n",
      "hsa_copy_deps: 1\n",
      "scan hsa API data 28083:28084                                                                                                    hsa_copy_deps: 0\n",
      "scan hip API data 7082:7083                                                                                                    File '/home/toby/Pelagos/Projects/HIP_Course/course_material/L8_IO_Optimisation/rocprof_trace/wave2d_async_events.csv' is generating\n",
      "File '/home/toby/Pelagos/Projects/HIP_Course/course_material/L8_IO_Optimisation/rocprof_trace/wave2d_async_events.stats.csv' is generating\n",
      "dump json 1281:1282                                                                                                    \n",
      "File '/home/toby/Pelagos/Projects/HIP_Course/course_material/L8_IO_Optimisation/rocprof_trace/wave2d_async_events.json' is generating\n",
      "File '/home/toby/Pelagos/Projects/HIP_Course/course_material/L8_IO_Optimisation/rocprof_trace/wave2d_async_events.hsa_stats.csv' is generating\n",
      "dump json 29365:29366                                                                                                    \n",
      "File '/home/toby/Pelagos/Projects/HIP_Course/course_material/L8_IO_Optimisation/rocprof_trace/wave2d_async_events.json' is generating\n",
      "File '/home/toby/Pelagos/Projects/HIP_Course/course_material/L8_IO_Optimisation/rocprof_trace/wave2d_async_events.copy_stats.csv' is generating\n",
      "dump json 0:1                                                                                                    \n",
      "File '/home/toby/Pelagos/Projects/HIP_Course/course_material/L8_IO_Optimisation/rocprof_trace/wave2d_async_events.json' is generating\n",
      "File '/home/toby/Pelagos/Projects/HIP_Course/course_material/L8_IO_Optimisation/rocprof_trace/wave2d_async_events.hip_stats.csv' is generating\n",
      "dump json 7082:7083                                                                                                    \n",
      "File '/home/toby/Pelagos/Projects/HIP_Course/course_material/L8_IO_Optimisation/rocprof_trace/wave2d_async_events.json' is generating\n"
     ]
    }
   ],
   "source": [
    "!./make_traces.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3fa9b9-e925-4899-9ef5-9490a04d5e14",
   "metadata": {},
   "source": [
    "Then you can go to the address [https://ui.perfetto.dev](https://ui.perfetto.dev) to open the tracing utility. If you load the file **rocprof_trace/trace_sync.json** you should see something like this. The IO occurs after each kernel execution using the same command queue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589fdcb5-31dd-4a66-b1a4-bef7e039546d",
   "metadata": {},
   "source": [
    "<figure style=\"margin-bottom 3em; margin-top: 2em; margin-left:auto; margin-right:auto; width:100%\">\n",
    "    <img style=\"vertical-align:middle\" src=\"../images/synchronous_io.png\"> <figcaption style= \"text-align:lower; margin:1em; float:bottom; vertical-align:bottom;\">Figure: Sequential IO solution. IO occurs after compute.</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efc71db-6210-49c8-99ec-ebe09fee8a98",
   "metadata": {},
   "source": [
    "### Concurrent (asynchronous) IO solutions\n",
    "\n",
    "In [wave2d_async_streams.cpp](wave2d_async_streams.cpp) and [wave2d_async_events.cpp](wave2d_async_events.cpp) are two solutions for concurrent IO. The goal is to use **multiple streams** so that while one stream is executing a kernel, the others are working on IO.\n",
    "\n",
    "<figure style=\"margin-bottom 3em; margin-top: 2em; margin-left:auto; margin-right:auto; width:100%\">\n",
    "    <img style=\"vertical-align:middle\" src=\"../images/concurrent_io.svg\"> <figcaption style= \"text-align:lower; margin:1em; float:bottom; vertical-align:bottom;\">Figure: Concurrent IO solution.</figcaption>\n",
    "</figure>\n",
    "\n",
    "Both solutions use one stream for compute and a number of streams for IO, however the difference between the two solutions is that [wave2d_async_streams.cpp](wave2d_async_streams.cpp) mainly uses synchronisation on streams to establish the necessary dependencies between IO and compute, while [wave2d_async_events.cpp](wave2d_async_events.cpp) mainly uses HIP events to achieve the same synchronisation. Both accomplish the same task of moving data while compute is taking place.\n",
    "\n",
    "#### Concurrent access to buffers from the host and device\n",
    "\n",
    "It is a **race condition** to read from a device allocation (from another stream) at the same time as a kernel is writing to the allocation. Furthermore, while it is technically possible to asynchronously read from an array that is also being read by a kernel, it can lead to undefined behaviour, as memory can be moved around. Therefore, it is **recommended** to perform IO only on buffers that we **know for sure** are not being used by a kernel. Our kernel needs access to wavefields at timesteps $\\textbf{U}_{0}, \\textbf{U}_{1}, \\textbf{U}_{2}$, therefore they are active and **not safe** to copy, but wavefields at earlier timesteps e.g $\\textbf{U}_{-2}, \\textbf{U}_{-1}$ **are** inactive and safe to copy.\n",
    "\n",
    "<figure style=\"margin-bottom 3em; margin-top: 2em; margin-left:auto; margin-right:auto; width:30%\">\n",
    "    <img style=\"vertical-align:middle\" src=\"../images/wavefields.svg\"> <figcaption style= \"text-align:lower; margin:1em; float:bottom; vertical-align:bottom;\">Figure: Active (not okay to copy) and inactive (okay to copy) wavefields.</figcaption>\n",
    "</figure>\n",
    "\n",
    "In this instance, the solution to enable concurrent IO and avoid race conditions is to have an array of at least four memory allocations on the device to represent the wavefield. Then the kernel can operate on the active wavefields, while IO stream/s copy from inactive wavefields. An array of **nscratch=5** allocations to allow extra leeway for copies to finish.\n",
    "\n",
    "During construction of these examples I found it **difficult to maintain synchronisation** when using multiple streams for compute, therefore a **stable solution** used one stream for compute to ensure there were no issues. The concurrent IO solutions use an array of five streams for IO and one stream for compute. Also allocated is an array of five events to demonstrate the use of events in workflow dependencies.\n",
    "\n",
    "#### Output array converted to pinned memory\n",
    "\n",
    "In [wave2d_async_streams.cpp](wave2d_async_streams.cpp) and [wave2d_async_events.cpp](wave2d_async_events.cpp) the 3D output array **out_h** on the host is allocated as pinned memory using **hipHostMalloc** to enable asynchronous copies. I encountered **silent failures** when trying to use addresses **within the pinned memory allocation** as inputs to **hipMemcpyAsync**. Therefore I had to use **hipMemcpy3DAsync** to perform the copies asynchronously and pass in the host pointer returned by **hipHostmalloc**.\n",
    "\n",
    "#### Stream-based synchronisation\n",
    "\n",
    "In [wave2d_async_streams.cpp](wave2d_async_streams.cpp) we use explicit stream-based synchronisation. During each iteration **n** of the time loop then:\n",
    "\n",
    "1. We use **hipStreamSynchronize** to make the host wait for all streams associated with past copies, as well as the compute stream.\n",
    "1. Submit the kernel to compute stream to solve for U[(n+2)%nscratch], then record an event (at index **n**) into the compute stream.\n",
    "1. The wavefield at (**n**-1) (which we term the *copy_index*) is only safe to copy once the compute stream (from the previous iteration) is done with it. Use **hipStreamWaitEvent** to make the IO stream at (**n**-1) wait on event at (**n**-1). Then use the IO stream and **hipMemcpy3DAsync** to asynchronously copy the wavefield at (**n**-1) to the stack of output images. \n",
    "\n",
    "The following diagram shows how the dependencies play out with stream-based synchronisation.\n",
    "\n",
    "<figure style=\"margin-bottom 3em; margin-top: 2em; margin-left:auto; margin-right:auto; width:100%\">\n",
    "    <img style=\"vertical-align:middle\" src=\"../images/wavefields_concurrent_streams.svg\"> <figcaption style= \"text-align:lower; margin:1em; float:bottom; vertical-align:bottom;\">Figure: Workflow of stream-based synchronisation during an iteration.</figcaption>\n",
    "</figure>\n",
    "\n",
    "The time loop for stream-based iteration is replicated below:\n",
    "\n",
    "```C++\n",
    "    for (int n=0; n<NT; n++) {\n",
    "        \n",
    "        // Wait for the event associated with a stream\n",
    "        \n",
    "        // Explicitly wait for all relevant streams to finish\n",
    "        H_ERRCHK(hipStreamSynchronize(streams[(n+2)%nscratch]));\n",
    "        H_ERRCHK(hipStreamSynchronize(streams[(n+1)%nscratch]));\n",
    "        H_ERRCHK(hipStreamSynchronize(streams[n%nscratch]));\n",
    "        H_ERRCHK(hipStreamSynchronize(compute_stream));\n",
    "        \n",
    "        // Get the wavefields\n",
    "        U0_d = U_ds[n%nscratch];\n",
    "        U1_d = U_ds[(n+1)%nscratch];\n",
    "        U2_d = U_ds[(n+2)%nscratch];\n",
    "        \n",
    "        // Shifted time\n",
    "        t = n*dt-2.0*td;\n",
    "        pi2fm2t2 = pi*pi*fm*fm*t*t;\n",
    "        \n",
    "        // Launch the kernel using hipLaunchKernelGGL method\n",
    "        // Use 0 when choosing the default (null) stream\n",
    "        hipLaunchKernelGGL(wave2d_4o, \n",
    "            grid_nblocks, block_size, sharedMemBytes, compute_stream,\n",
    "            U0_d, U1_d, U2_d, V_d,\n",
    "            N0, N1, dt2,\n",
    "            inv_dx02, inv_dx12,\n",
    "            P0, P1, pi2fm2t2\n",
    "        );\n",
    "                           \n",
    "        // Check the status of the kernel launch\n",
    "        H_ERRCHK(hipGetLastError());\n",
    "          \n",
    "        // Insert an event n%nscratch into compute stream\n",
    "        // It will complete afer the kernel does\n",
    "        H_ERRCHK(hipEventRecord(events[n%nscratch], compute_stream));   \n",
    "        \n",
    "        // Read memory from the buffer to the host in an asynchronous manner\n",
    "        if (n>2) {\n",
    "            size_t copy_index=n-1;\n",
    "            \n",
    "            // Insert a wait for the IO stream on the compute event\n",
    "            H_ERRCHK(\n",
    "                hipStreamWaitEvent(\n",
    "                    streams[copy_index%nscratch], \n",
    "                    events[copy_index%nscratch],\n",
    "                    0\n",
    "                )\n",
    "            );\n",
    "            \n",
    "            // Then asynchronously copy a wavefield back\n",
    "            // using the IO stream.\n",
    "            \n",
    "            // Only change what is necessary in copy_parms\n",
    "            copy_parms.srcPtr.ptr = U_ds[copy_index%nscratch];\n",
    "            \n",
    "            // Z positions of 1 don't seem to work on AMD platforms?!?!\n",
    "            copy_parms.dstPos.z = copy_index;\n",
    "            \n",
    "            // Copy memory asynchronously\n",
    "            H_ERRCHK(\n",
    "                hipMemcpy3DAsync(\n",
    "                    &copy_parms,\n",
    "                    streams[copy_index%nscratch]\n",
    "                )\n",
    "            );\n",
    "        }\n",
    "    }\n",
    "```\n",
    "\n",
    "Notice that we used **hipMemcpy3DAsync** with the host pointer **out_h** to peform the asynchronous copy of a wavefield to the output array on the host."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27e24ea-5a7f-4dd0-8f16-63cd834750dc",
   "metadata": {},
   "source": [
    "##### Make and run the solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c98fefa-96b9-44b9-9700-564fd14d7c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device id: 0\n",
      "\tname:                                    NVIDIA GeForce RTX 3060 Laptop GPU\n",
      "\tglobal memory size:                      6219 MB\n",
      "\tavailable registers per block:           65536 \n",
      "\tmax threads per SM or CU:                1536 \n",
      "\tmaximum shared memory size per block:    49 KB\n",
      "\tmaximum shared memory size per SM or CU: 0 KB\n",
      "\tmaximum pitch size for memory copies:    2147 MB\n",
      "\tmax block size:                          (1024,1024,64)\n",
      "\tmax threads in a block:                  1024\n",
      "\tmax Grid size:                           (2147483647,65535,65535)\n",
      "dt=0.001166, Vmax=343.000000\n",
      "dt=0.00116618, fm=34.3, Vmax=343, dt2=1.35998e-06\n",
      "The asynchronous calculation took 14.310000 milliseconds.\n"
     ]
    }
   ],
   "source": [
    "!run wave2d_async_streams.exe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5782d498-6eec-4930-ba60-e6e463899748",
   "metadata": {},
   "source": [
    "If we examine the time elapsed for the asynchronous calculation, it shows a marked improvement over the synchronous one. Plotting the result shows that we get the same answer as with the synchronous solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fd0cf46-d6cd-4795-85b4-d76c7946c591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e53c8c0c3cbf485483344ea0d6cff16f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='n', max=639), Output()), _dom_classes=('widget-interact'â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum residual between results is 0.0\n"
     ]
    }
   ],
   "source": [
    "output_async=np.fromfile(\"array_out.dat\", dtype=float_type)\n",
    "nimages_async=int(output_async.size//(defines[\"N0_U\"]*defines[\"N1_U\"]))\n",
    "images_async=output_async.reshape(nimages_async, defines[\"N0_U\"], defines[\"N1_U\"])\n",
    "\n",
    "py_helper.plot_slices(images_async)\n",
    "\n",
    "print(f\"Maximum residual between results is {np.max(images_async-images_sync)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ff7bcb-41e2-416f-8635-0da72d698f3b",
   "metadata": {},
   "source": [
    "#### Event-based synchronisation\n",
    "\n",
    "In the previous solution the host **explicitly waits** for each IO stream from previous iterations before the kernel works on active wavefields. We can accomplish the same synchronisation by recording an event after each copy and having the compute stream wait for all IO related events before working on the kernel. In [wave2d_async_events.cpp](wave2d_async_events.cpp) the time loop takes the form:\n",
    "\n",
    "\n",
    "1. We use **hipStreamWaitEvent** to make the compute stream wait for copies (from previous iterations) of wavefields that are now at (**n**), (**n**+1) and (**n**+2). \n",
    "1. Submit the kernel to compute stream to solve for U[(n+2)%nscratch], then record an event (at index **n**) into the compute stream.\n",
    "1. The wavefield at (**n**-1) (which we call the *copy_index*) is safe to copy once the compute stream is done with it. We use **hipEventSynchronize** to make the host wait on the IO event at (**n**-1) which is the event from the compute stream at the last iteration. This has the effect of eliminating a backlog of work accumulating on the compute stream.\n",
    "1.Then use the IO stream at (**n**-1) and **hipMemcpy3DAsync** to asynchronously copy the wavefield at (**n**-1) to the stack of output images. \n",
    "1. Finally, an event at (**n**-1) is recorded to the stream at (**n**-1) so it can be waited on at step 1 for the next iteration.\n",
    "\n",
    "The following diagram shows how the dependencies play out with events and streams during a single iteration.\n",
    "\n",
    "<figure style=\"margin-bottom 3em; margin-top: 2em; margin-left:auto; margin-right:auto; width:100%\">\n",
    "    <img style=\"vertical-align:middle\" src=\"../images/wavefields_concurrent_events.svg\"> <figcaption style= \"text-align:lower; margin:1em; float:bottom; vertical-align:bottom;\">Figure: Workflow of event-based synchronisation during an iteration.</figcaption>\n",
    "</figure>\n",
    "\n",
    "The code for the iterations in [wave2d_async_events.cpp](wave2d_async_events.cpp) is produced here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ce10d2-0688-4a30-892b-afff024c8eed",
   "metadata": {},
   "source": [
    "```C++\n",
    "    for (int n=0; n<NT; n++) {\n",
    "        \n",
    "        // Make the compute stream wait on events from previous copies\n",
    "        H_ERRCHK(hipStreamWaitEvent(compute_stream, events[(n+2)%nscratch], 0));\n",
    "        H_ERRCHK(hipStreamWaitEvent(compute_stream, events[(n+1)%nscratch], 0));\n",
    "        H_ERRCHK(hipStreamWaitEvent(compute_stream, events[n%nscratch], 0));\n",
    "        \n",
    "        // Get the wavefields\n",
    "        U0_d = U_ds[n%nscratch];\n",
    "        U1_d = U_ds[(n+1)%nscratch];\n",
    "        U2_d = U_ds[(n+2)%nscratch];\n",
    "        \n",
    "        // Shifted time\n",
    "        t = n*dt-2.0*td;\n",
    "        pi2fm2t2 = pi*pi*fm*fm*t*t;\n",
    "        \n",
    "        // Launch the kernel using hipLaunchKernelGGL method\n",
    "        // Use 0 when choosing the default (null) stream\n",
    "        hipLaunchKernelGGL(wave2d_4o, \n",
    "            grid_nblocks, block_size, sharedMemBytes, compute_stream,\n",
    "            U0_d, U1_d, U2_d, V_d,\n",
    "            N0, N1, dt2,\n",
    "            inv_dx02, inv_dx12,\n",
    "            P0, P1, pi2fm2t2\n",
    "        );\n",
    "                           \n",
    "        // Check the status of the kernel launch\n",
    "        H_ERRCHK(hipGetLastError());\n",
    "          \n",
    "        // Insert an event into stream at n%nscratch\n",
    "        // It will complete afer the kernel does\n",
    "        H_ERRCHK(hipEventRecord(events[n%nscratch], compute_stream));   \n",
    "        \n",
    "        // Read memory from the buffer to the host in an asynchronous manner\n",
    "        if (n>2) {\n",
    "            size_t copy_index=n-1;\n",
    "            \n",
    "            // Explicity wait for compute operation from previous iteration to finish\n",
    "            // before initiating a copy\n",
    "            H_ERRCHK(\n",
    "                hipEventSynchronize(events[copy_index%nscratch])\n",
    "            );\n",
    "            \n",
    "            // Then asynchronously copy a wavefield back\n",
    "            // using an IO stream\n",
    "            \n",
    "            // Only change what is necessary in copy_parms\n",
    "            copy_parms.srcPtr.ptr = U_ds[copy_index%nscratch];\n",
    "            \n",
    "            // Z positions of 1 don't seem to work on AMD platforms?!?!\n",
    "            copy_parms.dstPos.z = copy_index;\n",
    "            \n",
    "            // Copy memory asynchronously\n",
    "            H_ERRCHK(\n",
    "                hipMemcpy3DAsync(\n",
    "                    &copy_parms,\n",
    "                    streams[copy_index%nscratch]\n",
    "                )\n",
    "            );\n",
    "            // Record the event to the IO stream\n",
    "            H_ERRCHK(\n",
    "                hipEventRecord(\n",
    "                    events[copy_index%nscratch],\n",
    "                    streams[copy_index%nscratch]\n",
    "                )\n",
    "            );\n",
    "        }\n",
    "    }\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95733f27-00e3-492b-bea2-8bcbc0fe7881",
   "metadata": {},
   "source": [
    "##### Make and run the solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e741744b-3f79-4d55-8046-b829ce17d635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device id: 0\n",
      "\tname:                                    NVIDIA GeForce RTX 3060 Laptop GPU\n",
      "\tglobal memory size:                      6219 MB\n",
      "\tavailable registers per block:           65536 \n",
      "\tmax threads per SM or CU:                1536 \n",
      "\tmaximum shared memory size per block:    49 KB\n",
      "\tmaximum shared memory size per SM or CU: 0 KB\n",
      "\tmaximum pitch size for memory copies:    2147 MB\n",
      "\tmax block size:                          (1024,1024,64)\n",
      "\tmax threads in a block:                  1024\n",
      "\tmax Grid size:                           (2147483647,65535,65535)\n",
      "dt=0.001166, Vmax=343.000000\n",
      "dt=0.00116618, fm=34.3, Vmax=343, dt2=1.35998e-06\n",
      "The asynchronous calculation took 15.353000 milliseconds.\n"
     ]
    }
   ],
   "source": [
    "!run wave2d_async_events.exe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae90139d-e944-414c-a3ab-f95c10dcf5cd",
   "metadata": {},
   "source": [
    "If we check the time elapsed we find that the concurrent IO solution with event-based synchronsiation takes less time than both squential IO and stream-based synchronisation. A trace of HIP activity in **rocprof_trace/wave2d_async_streams.json** shows that IO is taking place during compute.\n",
    "\n",
    "<figure style=\"margin-bottom 3em; margin-top: 2em; margin-left:auto; margin-right:auto; width:100%\">\n",
    "    <img style=\"vertical-align:middle\" src=\"../images/asynchronous_io.png\"> <figcaption style= \"text-align:lower; margin:1em; float:bottom; vertical-align:bottom;\">Figure: Concurrent IO solution. IO can occur at the same time as compute.</figcaption>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e868c3-7c11-45bc-8b50-b8d5f8c803b1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Plot the wavefield and explore results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "459cbfd2-0fdc-4492-9805-f76db5e9a6c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adef8eddd2504ea8881031c7032648ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='n', max=639), Output()), _dom_classes=('widget-interact'â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum residual between results is 0.0\n"
     ]
    }
   ],
   "source": [
    "# Read the output file back in for display\n",
    "output_async=np.fromfile(\"array_out.dat\", dtype=float_type)\n",
    "nimages_async=int(output_async.size//(defines[\"N0_U\"]*defines[\"N1_U\"]))\n",
    "images_async=output_async.reshape(nimages_async, defines[\"N0_U\"], defines[\"N1_U\"])\n",
    "\n",
    "py_helper.plot_slices(images_async)\n",
    "\n",
    "print(f\"Maximum residual between results is {np.max(images_async-images_sync)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa650819-ac32-4bb0-bc05-d1aef960ac15",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Problems encountered with hipMemcpy3D and asynchronous copies\n",
    "\n",
    "In the concurrent examples you might notice the use of **hipMemcpy3DAsync** to copy wavefields as planes back to the 3D host memory allocation **out_h**. This is because using **hipMemcpyAsync** to copy device memory to an address within a pinned memory allocation **failed silently** and produced faulty copies. Therefore it is **not recommended** to perform asychronous copies with pointers derived from *within* a pinned memory allocation. Only use the pointer returned by **hipHostMalloc** with asynchronous copy functions.\n",
    "\n",
    "Problems were also encountered with **hipMemcpy3D** and **hipMemcpy3DAsync**. For some bizarre reason, during construction of these examples I found that on AMD platforms a value of **copy_parms.dstPos.z=1** resulted in an error for calls to either **hipMemcpy3D** or **hipMemcpy3DAsync**. This strange behaviour was not present with the NVIDIA backend. I have worked around this issue by only copying planes when the plane index z is greater than 2. Hopefully this issue is addressed in a future version of ROCM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8f9bf8-b44f-4527-8ac3-3ea81fe9a8f3",
   "metadata": {},
   "source": [
    "## Summary of learnings\n",
    "\n",
    "In this module we explored how IO can take place at the same time as a kernel using multiple streams. The concurrent IO solution was faster than the sequential IO solution. With concurrent IO it is a safety measure to avoid accessing memory allocations that are being used by a kernel, unless the allocated is managed. Both stream and event-based synchronisation can establish and enforce dependencies between activity that occurs across multiple streams."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb824e5-856c-4b36-ac49-c3801294bc41",
   "metadata": {},
   "source": [
    "<address>\n",
    "Written by Dr. Toby Potter of <a href=\"https://www.pelagos-consulting.com\">Pelagos Consulting and Education</a> for the Pawsey Supercomputing Centre\n",
    "</address>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
