{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "916435d8-e663-4bd0-b3e2-8d8344fd6830",
   "metadata": {},
   "source": [
    "# Porting CUDA programs to HIP\n",
    "\n",
    "HIP API calls are designed to closely match their CUDA equivalents. This enables HIP to function as a thin layer over CUDA and allows for reasonably easy porting of CUDA code to HIP code. Often it is just a matter of replacing **cuda -> hip** in the function calls, but not always. The ROCM suite provides two different tools **hipify-perl** and **hipify-clang** to help with the porting process. The tool **hipify-perl** is robust and uses perl to perform an intelligent (meaning it is aware of the CUDA and HIP API differences) search and replace of cuda calls with hip calls, while the **hipify-clang** tool uses the clang preprocessor and is intended to produce a high quality port. The perl-based method is better for quick ports of small codes, while the clang-based method is intended for ports of large codebases. The hipify-clang tool is more **fragile** though and fails easily unless it has access to all the header files used in the compilation of the CUDA code.\n",
    "\n",
    "## Supported API's\n",
    "\n",
    "The hipify tools will port a majority of CUDA calls as well as calls to CUDA libraries like **cuBLAS**. Tables in [this Github site](https://github.com/ROCm-Developer-Tools/HIPIFY/blob/amd-staging/docs/supported_apis.md) provides some guidance as to what is supported.\n",
    "\n",
    "## Documentation on porting\n",
    "\n",
    "The documentation from AMD at this [site](https://rocm.docs.amd.com/projects/HIP/en/develop/user_guide/hip_porting_guide.html) provides in-depth knowledge about the porting process and preprocessor directives. The guide below is intended to provde complementary information to the official documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca82af5-1228-44b4-b60b-46eacba53713",
   "metadata": {},
   "source": [
    "## Path setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "560fbfdc-c172-4e31-9913-392c5d993953",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PATH'] = f\"{os.environ['PATH']}:../install/bin\"\n",
    "\n",
    "# At a Bash terminal you need to do this instead\n",
    "# source ../env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aef7d88-273f-4ee5-8a56-a7a4fb64cf2f",
   "metadata": {},
   "source": [
    "## Setup and installation\n",
    "\n",
    "From [this source](https://sep5.readthedocs.io/en/latest/Programming_Guides/HIP-porting-guide.html) it is recommended to attempt porting on a machine that has access to both CUDA and HIP libraries. This usually means doing the port on a machine with an NVIDIA GPU and access to CUDA. Then one can try porting portions of the code at a time and compare results. For best results during porting you need to have a version of CUDA that is compatible with your installed version of hipify-clang. The table at [this resource](https://rocm.docs.amd.com/projects/HIPIFY/en/latest/hipify-clang.html) provides information on which version of **hipify-clang** is compatible with which version of CUDA.\n",
    "\n",
    "Once the code is ported and compiles with the HIP compiler, it is important to be aware that HIP functions may try to access ROCm libraries on the backend without prior warning of this dependency. It is then a good idea to make sure you have a complete installation of ROCm, in addition to having a version of ROCm that is API compatible with your code's version of CUDA. \n",
    "\n",
    "It is also important to be aware that some HIP libraries like **hipBLAS** are built to use the corresponding library from ROCm by default. If you need to use these libaries with a CUDA backend you might need to recompile those libraries for use with CUDA.\n",
    "\n",
    "The code below shows what version of hipify-clang that you are using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "679e253f-cc77-40e8-826d-cc8e904394e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AMD LLVM version 17.0.0git\n",
      "  Optimized build.\n"
     ]
    }
   ],
   "source": [
    "!run hipify-clang --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3341127e-3ad1-45b6-bb3f-0624e4d60c33",
   "metadata": {},
   "source": [
    "The [HIPIFY Documentation](https://rocm.docs.amd.com/projects/HIPIFY/en/latest/hipify-clang.html) describes compatibility between CUDA and your version of hipify-clang. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc7abd4-a72e-468b-b7e6-50bb8a4f6db9",
   "metadata": {},
   "source": [
    "## General porting process\n",
    "\n",
    "The general porting process proceeds as follows:\n",
    "\n",
    "1. Compile with CUDA to verify that the program compiles.\n",
    "1. Run a hipify tool to convert sources.\n",
    "    * Use the flag `-hip-kernel-execution-syntax` to convert CUDA kernel launch syntax to HIP kernel launch syntax.\n",
    "1. Adjust the compilation environment to use **hipcc**.\n",
    "    * This has to be manually ported!\n",
    "1. Fix compilation errors.\n",
    "    * It can be the most difficult part of porting!\n",
    "    * Use environment variable `HIP_PLATFORM=nvidia` or `HIP_PLATFORM=amd` to switch between backends.\n",
    "    * Preprocessor directives can separate CUDA code from hip-clang code.\n",
    "        * Use the directive `__HIP_PLATFORM_NVIDIA__` for CUDA-specific code.\n",
    "        * Use the directive `__HIP_PLATFORM_AMD__` for separate AMD-specific code.\n",
    "1. Verify code correctness.\n",
    "    * Test on CUDA and AMD architectures to ensure portability\n",
    "1. Re-tune optimisations for new architecture, but only once you know everything works!\n",
    "1. Document the changes made.\n",
    "\n",
    "The step of running the **hipify** tool is the **easiest part** of the process.  If the code uses simple and well-supported CUDA API calls, then this has the greatest chance of succeeding. If the codebase contains CUDA-specific complexity, or relies on functionality that is no longer supported by recent version of CUDA, then the level of difficulty in porting can increase **dramatically**. Adjusting the compilation environment for **hipcc** often requires knowledge of  build tools, usually this means a working knowledge of **make** or **cmake**.\n",
    "Familiarity with C++ and what the compiler warnings and errors mean is then crucial to massaging the codebase to accept the new compiler.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d427b5-af01-452e-8bb8-b72c57e47ca7",
   "metadata": {},
   "source": [
    "## Example setup\n",
    "\n",
    "In this example we are going to follow the steps above to port a CUDA version of the matrix multiplication code to use HIP. The CUDA version is located in the subdirectory **cuda_mat_mult**, and in the subdirectory **hip_mat_mult** is the corresponding HIP version for reference. On an NVIDIA system (it won't work without CUDA) you can change directory to **cuda_mat_mult** and run `make` to build the software. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b688398-0021-4023-92e2-1db97cd7084b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -r *.exe\n",
      "nvcc -g -O2 -x cu mat_mult.cpp -o mat_mult.exe -lcuda \n",
      "Device id: 0\n",
      "\tname:                                    NVIDIA GeForce RTX 3060 Laptop GPU\n",
      "\tglobal memory size:                      6219 MB\n",
      "\tavailable registers per block:           65536 \n",
      "\tmaximum shared memory size per block:    49 KB\n",
      "\tmaximum pitch size for memory copies:    2147 MB\n",
      "\tmax block size:                          (1024,1024,64)\n",
      "\tmax threads in a block:                  1024\n",
      "\tmax Grid size:                           (2147483647,65535,65535)\n",
      "Maximum error (infinity norm) is: 1.52588e-05\n"
     ]
    }
   ],
   "source": [
    "!cd cuda_mat_mult; make clean; make; \n",
    "!run cuda_mat_mult/mat_mult.exe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbddc62d-858f-4173-8497-f7bc583c9f84",
   "metadata": {},
   "source": [
    "If you list the sources in the directory you can see the C++ file `mat_mult.cpp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87f25e1c-c118-4724-b469-692290b9059a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array_A.dat  array_C.dat      Makefile\t      mat_mult.cpp  mat_size.hpp\n",
      "array_B.dat  cuda_helper.hpp  mat_helper.hpp  mat_mult.exe\n"
     ]
    }
   ],
   "source": [
    "!ls cuda_mat_mult"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dad6a50-e3ee-4c23-9b9a-1c2440c246cd",
   "metadata": {},
   "source": [
    "Ordinarily CUDA source files would need to end in `.cu` otherwise the `nvcc` compiler won't interpret them as CUDA source. However since the `-x cu` flag is in the makefile then `nvcc` treats them as CUDA source. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba935e29-5e67-4abf-a8d0-21f0e14634f3",
   "metadata": {},
   "source": [
    "Let's now make a temporary copy of this directory for conversion purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65ef4517-8bfb-4539-9055-681658dd02a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p temp_mat_mult; cp -r cuda_mat_mult/* temp_mat_mult/ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158aeee9-6c9b-452d-8ab6-510f9d30850f",
   "metadata": {},
   "source": [
    "## Porting techniques with hipify tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fea2199-4187-41fa-8f26-2ef36342b579",
   "metadata": {},
   "source": [
    "### Port a single file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff01866-4aa5-456c-b250-59bb193fb640",
   "metadata": {},
   "source": [
    "The **hipify-perl** command can port a single file to use the HIP API. We use it to port the file **mat_mult.cpp** in the directory **temp_mat_mult**. The flag `-hip-kernel-execution-syntax` changes kernel launch syntax from the CUDA-style triple Chevron `<<< >>>` method to the ANSI C++ compliant method of **hipLaunchKernelGGL**. The following command dumps the output to the command line, but you can use the `-o` flag to specify an output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9a82eaa-6360-4b8c-8c96-7c121b8fb5f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#include \"hip/hip_runtime.h\"\n",
      "/* Code to perform a Matrix multiplication using cuda\n",
      "Written by Dr Toby M. Potter\n",
      "*/\n",
      "\n",
      "// Setup headers\n",
      "#include <cassert>\n",
      "#include <cmath>\n",
      "#include <iostream>\n",
      "\n",
      "// Bring in the size of the matrices\n",
      "#include \"mat_size.hpp\"\n",
      "\n",
      "// Bring in a library to manage matrices on the CPU\n",
      "#include \"mat_helper.hpp\"\n",
      "\n",
      "// Bring in helper header to manage boilerplate code\n",
      "#include \"cuda_helper.hpp\"\n",
      "\n",
      "// standard matrix multiply kernel \n",
      "__global__ void mat_mult (\n",
      "        float* A, \n",
      "        float* B, \n",
      "        float* C, \n",
      "        size_t N1_A, \n",
      "        size_t N0_C,\n",
      "        size_t N1_C) { \n",
      "            \n",
      "    // A is of size (N0_C, N1_A)\n",
      "    // B is of size (N1_A, N1_C)\n",
      "    // C is of size (N0_C, N1_C)   \n",
      "    \n",
      "    // i0 and i1 represent the coordinates in Matrix C \n",
      "    // We use row-major ordering for the matrices\n",
      "    \n",
      "    size_t i0 = blockIdx.y * blockDim.y + threadIdx.y;\n",
      "    size_t i1 = blockIdx.x * blockDim.x + threadIdx.x;\n",
      "    \n",
      "    // Scratch variable\n",
      "    float temp=0.0f; \n",
      "\n",
      "    // Guard mechanism to make sure we do not go\n",
      "    // outside the boundaries of matrix C \n",
      "    if ((i0<N0_C) && (i1<N1_C)) {\n",
      "        // Get the offset within the memory allocation of C\n",
      "        size_t offset = i0*N1_C+i1;\n",
      "        \n",
      "        // Loop over columns of A and rows of B\n",
      "        for (size_t n=0; n<N1_A; n++) {\n",
      "            \n",
      "            // A is of size (N0_C, N1_A)\n",
      "            // B is of size (N1_A, N1_C)\n",
      "            \n",
      "            // Loop across row i0 of A\n",
      "            // and down column i1 of B\n",
      "            temp+=A[i0*N1_A+n]*B[i1+n*N1_C]; \n",
      "        }\n",
      "        \n",
      "        // Set the value in C at offset\n",
      "        C[offset]=temp;\n",
      "        \n",
      "        // Uncomment this to perform elementwise matrix multiplication instead\n",
      "        // C[offset]=A[offset]*B[offset];\n",
      "    }\n",
      "} \n",
      "\n",
      "int main(int argc, char** argv) {\n",
      "    \n",
      "    //// Step 1. Parse program arguments ////\n",
      "\n",
      "    // Parse command line arguments\n",
      "    int dev_index = h_parse_args(argc, argv);\n",
      "    \n",
      "    // Number of devices discovered\n",
      "    int num_devices=0;\n",
      "    \n",
      "    //// Step 2. Discover resources and choose a compute device ////\n",
      "    \n",
      "    // Helper function to acquire devices\n",
      "    // This sets the default device\n",
      "    h_acquire_devices(&num_devices, dev_index);\n",
      "        \n",
      "    // Report on the device in use\n",
      "    h_report_on_device(dev_index);\n",
      "    \n",
      "    // We are going to do a simple array multiplication for this example, \n",
      "    // using raw binary files for input and output\n",
      "    \n",
      "    // A is of size (N0_C, N1_A)\n",
      "    // B is of size (N1_A, N1_C)    \n",
      "    // C is of size (N0_C, N1_C)\n",
      "\n",
      "    size_t N1_A = NCOLS_A, N0_C = NROWS_C, N1_C = NCOLS_C;\n",
      "\n",
      "    //// Step 3. Construct matrices A_h and B_h on the host \n",
      "    //// and fill them with random numbers ////\n",
      "    \n",
      "    // Number of bytes in each array\n",
      "    size_t nbytes_A = N0_C*N1_A*sizeof(float);\n",
      "    size_t nbytes_B = N1_A*N1_C*sizeof(float);\n",
      "    size_t nbytes_C = N0_C*N1_C*sizeof(float);\n",
      "\n",
      "    // Allocate memory for the host arrays\n",
      "    float* A_h = (float*)h_alloc(nbytes_A);\n",
      "    float* B_h = (float*)h_alloc(nbytes_B);\n",
      "    float* C_h = (float*)h_alloc(nbytes_C);\n",
      "\n",
      "    // Fill the host arrays with random numbers \n",
      "    // using the matrix helper library\n",
      "    m_random(A_h, N0_C, N1_A);\n",
      "    m_random(B_h, N1_A, N1_C);\n",
      "    \n",
      "    //// Step 4. Allocate memory for arrays //// \n",
      "    //// A_d, B_d, and C_d on the compute device ////\n",
      "\n",
      "    float *A_d, *B_d, *C_d;\n",
      "    H_ERRCHK(hipMalloc((void**)&A_d, nbytes_A));\n",
      "    H_ERRCHK(hipMalloc((void**)&B_d, nbytes_B));\n",
      "    H_ERRCHK(hipMalloc((void**)&C_d, nbytes_C));\n",
      "\n",
      "    //// Step 5. 1. Upload matrices A_h and B_h from the host //// \n",
      "    //// to A_d and B_d on the device ////\n",
      "    H_ERRCHK(hipMemcpy(A_d, A_h, nbytes_A, hipMemcpyHostToDevice));\n",
      "    H_ERRCHK(hipMemcpy(B_d, B_h, nbytes_B, hipMemcpyHostToDevice));\n",
      " \n",
      "    //// Step 6. Run the kernel to compute C_d ///\n",
      "    //// from A_d and B_d on the device ////\n",
      "        \n",
      "    // Desired block size\n",
      "    dim3 block_size = { 8, 8, 1 };\n",
      "    dim3 global_size = { (uint32_t)N1_C, (uint32_t)N0_C, 1 };\n",
      "    dim3 grid_nblocks;\n",
      "    \n",
      "    // Choose the number of blocks so that Grid fits within it.\n",
      "    h_fit_blocks(&grid_nblocks, global_size, block_size);\n",
      "\n",
      "    // Amount of shared memory to use in the kernel\n",
      "    size_t sharedMemBytes=0;\n",
      "    \n",
      "    // Launch the kernel using CUDA triple Chevron syntax\n",
      "    // Use 0 when choosing the default (null) stream\n",
      "    hipLaunchKernelGGL(mat_mult, grid_nblocks, block_size, sharedMemBytes, 0, A_d, B_d, C_d, N1_A, N0_C, N1_C);\n",
      "    \n",
      "    // Check the status of the kernel launch\n",
      "    H_ERRCHK(hipGetLastError());\n",
      "    \n",
      "    // Wait for any commands to complete on the compute device\n",
      "    H_ERRCHK(hipDeviceSynchronize());\n",
      "\n",
      "    //// Step 7. Copy the buffer for matrix C_d //// \n",
      "    //// on the device back to C_h on the host ////\n",
      "    H_ERRCHK(hipMemcpy((void*)C_h, (const void*)C_d, nbytes_C, hipMemcpyDeviceToHost));\n",
      "    \n",
      "    //// Step 8. Test the computed matrix **C_h** against a known answer\n",
      "    \n",
      "    // Compute the serial solution using the matrix helper library\n",
      "    float* C_answer_h = (float*)calloc(nbytes_C, 1);\n",
      "    m_mat_mult(A_h, B_h, C_answer_h, N1_A, N0_C, N1_C);\n",
      "    \n",
      "    // Uncomment this to check against elementwise matrix multiplication\n",
      "    // m_hadamard(A_h, B_h, C_answer_h, N0_C, N1_C);\n",
      "\n",
      "    // Print the maximum error between matrices\n",
      "    float max_err = m_max_error(C_h, C_answer_h, N0_C, N1_C);\n",
      "    \n",
      "    //// Step 9. Write the contents of matrices A_h, B_h, and C_h to disk ////\n",
      "\n",
      "    // Write out the host arrays to file\n",
      "    h_write_binary(A_h, \"array_A.dat\", nbytes_A);\n",
      "    h_write_binary(B_h, \"array_B.dat\", nbytes_B);\n",
      "    h_write_binary(C_h, \"array_C.dat\", nbytes_C);\n",
      "    \n",
      "    //// Step 10. Clean up memory alllocations and release resources\n",
      "    \n",
      "    // Free the cuda buffers\n",
      "    H_ERRCHK(hipFree(A_d));\n",
      "    H_ERRCHK(hipFree(B_d));\n",
      "    H_ERRCHK(hipFree(C_d));\n",
      "\n",
      "    // Clean up host memory\n",
      "    free(A_h);\n",
      "    free(B_h);\n",
      "    free(C_h);\n",
      "\n",
      "    // Free the answer matrix\n",
      "    free(C_answer_h);\n",
      "    \n",
      "    // Reset compute devices\n",
      "    h_reset_devices(num_devices);\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!cd temp_mat_mult; hipify-perl -hip-kernel-execution-syntax ./mat_mult.cpp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747905a4-2e43-4269-900f-de3bd4205f5f",
   "metadata": {},
   "source": [
    "If we use the `-inplace` flag, **hipify-perl** copies the file [mat_mult.cpp](temp_mat_mult/mat_mult.cpp) first to [mat_mult.cpp.prehip](temp_mat_mult/mat_mult.cpp.prehip) **if that file doesn't already exist**. Then it performs the conversion from [mat_mult.cpp.prehip](temp_mat_mult/mat_mult.cpp.prehip) to [mat_mult.cpp](temp_mat_mult/mat_mult.cpp). You can make changes to `*.prehip` files and run the conversion as many times as you wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4704d730-6ab7-4216-89d9-6b13eeb0024e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[HIPIFY] info: file 'mat_mult.cpp' statistics:\n",
      "  CONVERTED refs count: 15\n",
      "  TOTAL lines of code: 190\n",
      "  WARNINGS: 0\n",
      "[HIPIFY] info: CONVERTED refs by names:\n",
      "  cudaDeviceSynchronize => hipDeviceSynchronize: 1\n",
      "  cudaFree => hipFree: 3\n",
      "  cudaGetLastError => hipGetLastError: 1\n",
      "  cudaMalloc => hipMalloc: 3\n",
      "  cudaMemcpy => hipMemcpy: 3\n",
      "  cudaMemcpyDeviceToHost => hipMemcpyDeviceToHost: 1\n",
      "  cudaMemcpyHostToDevice => hipMemcpyHostToDevice: 2\n"
     ]
    }
   ],
   "source": [
    "!cd temp_mat_mult; hipify-perl -inplace -print-stats -hip-kernel-execution-syntax mat_mult.cpp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498f4116-e38e-4113-a148-844a92d8b83a",
   "metadata": {},
   "source": [
    "Subsequent edits to [mat_mult.cpp.prehip](temp_mat_mult/mat_mult.cpp.prehip) will be propagated across to [mat_mult.cpp](temp_mat_mult/mat_mult.cpp). This allows for an iterative porting process. Use the `--help` flag on hipify commands for more porting options."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f4a7d3-8c4d-431b-901a-870916d81a58",
   "metadata": {},
   "source": [
    "### Examine a directory structure for porting potential\n",
    "\n",
    "We use the scripts **hipexamine-perl.sh** or **hipexamine.sh** to recursively search through a directory and examine the potential for porting a code. Note there is a summary produced for each file, showing what API calls were converted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12620b08-509c-4d8b-ae80-8534892a134b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[HIPIFY] info: file 'cuda_mat_mult/.ipynb_checkpoints/mat_mult-checkpoint.cpp' statistics:\n",
      "  CONVERTED refs count: 14\n",
      "  TOTAL lines of code: 190\n",
      "  WARNINGS: 0\n",
      "[HIPIFY] info: CONVERTED refs by names:\n",
      "  cudaDeviceSynchronize => hipDeviceSynchronize: 1\n",
      "  cudaFree => hipFree: 3\n",
      "  cudaGetLastError => hipGetLastError: 1\n",
      "  cudaMalloc => hipMalloc: 3\n",
      "  cudaMemcpy => hipMemcpy: 3\n",
      "  cudaMemcpyDeviceToHost => hipMemcpyDeviceToHost: 1\n",
      "  cudaMemcpyHostToDevice => hipMemcpyHostToDevice: 2\n",
      "\n",
      "[HIPIFY] info: file 'cuda_mat_mult/mat_mult.cpp' statistics:\n",
      "  CONVERTED refs count: 14\n",
      "  TOTAL lines of code: 190\n",
      "  WARNINGS: 0\n",
      "[HIPIFY] info: CONVERTED refs by names:\n",
      "  cudaDeviceSynchronize => hipDeviceSynchronize: 1\n",
      "  cudaFree => hipFree: 3\n",
      "  cudaGetLastError => hipGetLastError: 1\n",
      "  cudaMalloc => hipMalloc: 3\n",
      "  cudaMemcpy => hipMemcpy: 3\n",
      "  cudaMemcpyDeviceToHost => hipMemcpyDeviceToHost: 1\n",
      "  cudaMemcpyHostToDevice => hipMemcpyHostToDevice: 2\n",
      "  warning: cuda_mat_mult/.ipynb_checkpoints/cuda_helper-checkpoint.hpp:480: removed identifier \"cudaLaunch\" since CUDA 10.1\n",
      "\n",
      "[HIPIFY] info: file 'cuda_mat_mult/.ipynb_checkpoints/cuda_helper-checkpoint.hpp' statistics:\n",
      "  CONVERTED refs count: 55\n",
      "  TOTAL lines of code: 789\n",
      "  WARNINGS: 1\n",
      "[HIPIFY] info: CONVERTED refs by names:\n",
      "  CUDA_SUCCESS => hipSuccess: 4\n",
      "  CUresult => hipError_t: 4\n",
      "  cuGetErrorString => hipDrvGetErrorString: 1\n",
      "  cuInit => hipInit: 1\n",
      "  cuda.h => hip/hip_runtime.h: 2\n",
      "  cudaDevAttrManagedMemory => hipDeviceAttributeManagedMemory: 1\n",
      "  cudaDeviceGetAttribute => hipDeviceGetAttribute: 1\n",
      "  cudaDeviceProp => hipDeviceProp_t: 2\n",
      "  cudaDeviceReset => hipDeviceReset: 1\n",
      "  cudaDeviceSynchronize => hipDeviceSynchronize: 1\n",
      "  cudaError_t => hipError_t: 4\n",
      "  cudaEventCreate => hipEventCreate: 2\n",
      "  cudaEventDestroy => hipEventDestroy: 2\n",
      "  cudaEventElapsedTime => hipEventElapsedTime: 1\n",
      "  cudaEventRecord => hipEventRecord: 3\n",
      "  cudaEventSynchronize => hipEventSynchronize: 2\n",
      "  cudaEvent_t => hipEvent_t: 3\n",
      "  cudaGetDevice => hipGetDevice: 1\n",
      "  cudaGetDeviceCount => hipGetDeviceCount: 2\n",
      "  cudaGetDeviceProperties => hipGetDeviceProperties: 2\n",
      "  cudaGetErrorString => hipGetErrorString: 2\n",
      "  cudaLaunchKernel => hipLaunchKernel: 1\n",
      "  cudaSetDevice => hipSetDevice: 3\n",
      "  cudaStreamCreateWithFlags => hipStreamCreateWithFlags: 1\n",
      "  cudaStreamDefault => hipStreamDefault: 1\n",
      "  cudaStreamDestroy => hipStreamDestroy: 1\n",
      "  cudaStreamNonBlocking => hipStreamNonBlocking: 1\n",
      "  cudaStream_t => hipStream_t: 7\n",
      "  cudaSuccess => hipSuccess: 4\n",
      "  cuda_runtime.h => hip/hip_runtime.h: 2\n",
      "  warning: cuda_mat_mult/cuda_helper.hpp:480: removed identifier \"cudaLaunch\" since CUDA 10.1\n",
      "\n",
      "[HIPIFY] info: file 'cuda_mat_mult/cuda_helper.hpp' statistics:\n",
      "  CONVERTED refs count: 55\n",
      "  TOTAL lines of code: 789\n",
      "  WARNINGS: 1\n",
      "[HIPIFY] info: CONVERTED refs by names:\n",
      "  CUDA_SUCCESS => hipSuccess: 4\n",
      "  CUresult => hipError_t: 4\n",
      "  cuGetErrorString => hipDrvGetErrorString: 1\n",
      "  cuInit => hipInit: 1\n",
      "  cuda.h => hip/hip_runtime.h: 2\n",
      "  cudaDevAttrManagedMemory => hipDeviceAttributeManagedMemory: 1\n",
      "  cudaDeviceGetAttribute => hipDeviceGetAttribute: 1\n",
      "  cudaDeviceProp => hipDeviceProp_t: 2\n",
      "  cudaDeviceReset => hipDeviceReset: 1\n",
      "  cudaDeviceSynchronize => hipDeviceSynchronize: 1\n",
      "  cudaError_t => hipError_t: 4\n",
      "  cudaEventCreate => hipEventCreate: 2\n",
      "  cudaEventDestroy => hipEventDestroy: 2\n",
      "  cudaEventElapsedTime => hipEventElapsedTime: 1\n",
      "  cudaEventRecord => hipEventRecord: 3\n",
      "  cudaEventSynchronize => hipEventSynchronize: 2\n",
      "  cudaEvent_t => hipEvent_t: 3\n",
      "  cudaGetDevice => hipGetDevice: 1\n",
      "  cudaGetDeviceCount => hipGetDeviceCount: 2\n",
      "  cudaGetDeviceProperties => hipGetDeviceProperties: 2\n",
      "  cudaGetErrorString => hipGetErrorString: 2\n",
      "  cudaLaunchKernel => hipLaunchKernel: 1\n",
      "  cudaSetDevice => hipSetDevice: 3\n",
      "  cudaStreamCreateWithFlags => hipStreamCreateWithFlags: 1\n",
      "  cudaStreamDefault => hipStreamDefault: 1\n",
      "  cudaStreamDestroy => hipStreamDestroy: 1\n",
      "  cudaStreamNonBlocking => hipStreamNonBlocking: 1\n",
      "  cudaStream_t => hipStream_t: 7\n",
      "  cudaSuccess => hipSuccess: 4\n",
      "  cuda_runtime.h => hip/hip_runtime.h: 2\n",
      "\n",
      "[HIPIFY] info: file 'GLOBAL' statistics:\n",
      "  CONVERTED refs count: 138\n",
      "  TOTAL lines of code: 2141\n",
      "  WARNINGS: 2\n",
      "[HIPIFY] info: CONVERTED refs by names:\n",
      "  CUDA_SUCCESS => hipSuccess: 8\n",
      "  CUresult => hipError_t: 8\n",
      "  cuGetErrorString => hipDrvGetErrorString: 2\n",
      "  cuInit => hipInit: 2\n",
      "  cuda.h => hip/hip_runtime.h: 4\n",
      "  cudaDevAttrManagedMemory => hipDeviceAttributeManagedMemory: 2\n",
      "  cudaDeviceGetAttribute => hipDeviceGetAttribute: 2\n",
      "  cudaDeviceProp => hipDeviceProp_t: 4\n",
      "  cudaDeviceReset => hipDeviceReset: 2\n",
      "  cudaDeviceSynchronize => hipDeviceSynchronize: 4\n",
      "  cudaError_t => hipError_t: 8\n",
      "  cudaEventCreate => hipEventCreate: 4\n",
      "  cudaEventDestroy => hipEventDestroy: 4\n",
      "  cudaEventElapsedTime => hipEventElapsedTime: 2\n",
      "  cudaEventRecord => hipEventRecord: 6\n",
      "  cudaEventSynchronize => hipEventSynchronize: 4\n",
      "  cudaEvent_t => hipEvent_t: 6\n",
      "  cudaFree => hipFree: 6\n",
      "  cudaGetDevice => hipGetDevice: 2\n",
      "  cudaGetDeviceCount => hipGetDeviceCount: 4\n",
      "  cudaGetDeviceProperties => hipGetDeviceProperties: 4\n",
      "  cudaGetErrorString => hipGetErrorString: 4\n",
      "  cudaGetLastError => hipGetLastError: 2\n",
      "  cudaLaunchKernel => hipLaunchKernel: 2\n",
      "  cudaMalloc => hipMalloc: 6\n",
      "  cudaMemcpy => hipMemcpy: 6\n",
      "  cudaMemcpyDeviceToHost => hipMemcpyDeviceToHost: 2\n",
      "  cudaMemcpyHostToDevice => hipMemcpyHostToDevice: 4\n",
      "  cudaSetDevice => hipSetDevice: 6\n",
      "  cudaStreamCreateWithFlags => hipStreamCreateWithFlags: 2\n",
      "  cudaStreamDefault => hipStreamDefault: 2\n",
      "  cudaStreamDestroy => hipStreamDestroy: 2\n",
      "  cudaStreamNonBlocking => hipStreamNonBlocking: 2\n",
      "  cudaStream_t => hipStream_t: 14\n",
      "  cudaSuccess => hipSuccess: 8\n",
      "  cuda_runtime.h => hip/hip_runtime.h: 4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!hipexamine-perl.sh cuda_mat_mult -exclude-dirs=\".ipynb_checkpoints\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3117e3-f3bf-4c7f-9351-085154143a0e",
   "metadata": {},
   "source": [
    "If we try the hip-clang version we see that it doesn't handle preprocessor directives very well. The following errors with `_aligned_malloc` are due to it not picking up the windows-specific `#define` clauses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a5ac422-ce12-4499-9aed-93f4db9559bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: CUDA version is newer than the latest partially supported version 11.8\n",
      "\u001b[0;1;35mwarning: \u001b[0m\u001b[1mCUDA version is newer than the latest partially supported version 11.8 [-Wunknown-cuda-version]\u001b[0m\n",
      "\n",
      "[HIPIFY] info: file './.ipynb_checkpoints/mat_mult-checkpoint.cpp' statistics:\n",
      "  CONVERTED refs count: 14\n",
      "  UNCONVERTED refs count: 0\n",
      "  CONVERSION %: 100.0\n",
      "  REPLACED bytes: 187\n",
      "  TOTAL bytes: 5944\n",
      "  CHANGED lines of code: 12\n",
      "  TOTAL lines of code: 190\n",
      "  CODE CHANGED (in bytes) %: 3.1\n",
      "  CODE CHANGED (in lines) %: 6.3\n",
      "  TIME ELAPSED s: 0.60\n",
      "[HIPIFY] info: CONVERTED refs by type:\n",
      "  error: 1\n",
      "  device: 1\n",
      "  memory: 9\n",
      "  numeric_literal: 3\n",
      "[HIPIFY] info: CONVERTED refs by API:\n",
      "  CUDA RT API: 14\n",
      "[HIPIFY] info: CONVERTED refs by names:\n",
      "  cudaDeviceSynchronize: 1\n",
      "  cudaFree: 3\n",
      "  cudaGetLastError: 1\n",
      "  cudaMalloc: 3\n",
      "  cudaMemcpy: 3\n",
      "  cudaMemcpyDeviceToHost: 1\n",
      "  cudaMemcpyHostToDevice: 2\n",
      "\u001b[0;1;35mwarning: \u001b[0m\u001b[1mCUDA version is newer than the latest partially supported version 11.8 [-Wunknown-cuda-version]\u001b[0m\n",
      "\n",
      "[HIPIFY] info: file './mat_mult.cpp' statistics:\n",
      "  CONVERTED refs count: 14\n",
      "  UNCONVERTED refs count: 0\n",
      "  CONVERSION %: 100.0\n",
      "  REPLACED bytes: 187\n",
      "  TOTAL bytes: 5944\n",
      "  CHANGED lines of code: 12\n",
      "  TOTAL lines of code: 190\n",
      "  CODE CHANGED (in bytes) %: 3.1\n",
      "  CODE CHANGED (in lines) %: 6.3\n",
      "  TIME ELAPSED s: 0.55\n",
      "[HIPIFY] info: CONVERTED refs by type:\n",
      "  error: 1\n",
      "  device: 1\n",
      "  memory: 9\n",
      "  numeric_literal: 3\n",
      "[HIPIFY] info: CONVERTED refs by API:\n",
      "  CUDA RT API: 14\n",
      "[HIPIFY] info: CONVERTED refs by names:\n",
      "  cudaDeviceSynchronize: 1\n",
      "  cudaFree: 3\n",
      "  cudaGetLastError: 1\n",
      "  cudaMalloc: 3\n",
      "  cudaMemcpy: 3\n",
      "  cudaMemcpyDeviceToHost: 1\n",
      "  cudaMemcpyHostToDevice: 2\n",
      "\u001b[0;1;35mwarning: \u001b[0m\u001b[1mCUDA version is newer than the latest partially supported version 11.8 [-Wunknown-cuda-version]\u001b[0m\n",
      "\u001b[1m/tmp/cuda_helper-checkpoint.hpp-150580.hip:95:5: \u001b[0m\u001b[0;1;31merror: \u001b[0m\u001b[1munknown type name 'SYSTEM_INFO'\u001b[0m\n",
      "    SYSTEM_INFO sys_info;\n",
      "\u001b[0;1;32m    ^\n",
      "\u001b[0m\u001b[1m/tmp/cuda_helper-checkpoint.hpp-150580.hip:381:20: \u001b[0m\u001b[0;1;31merror: \u001b[0m\u001b[1muse of undeclared identifier '_aligned_malloc'; did you mean 'aligned_alloc'?\u001b[0m\n",
      "    void* buffer = _aligned_malloc(nbytes, alignment);\n",
      "\u001b[0;1;32m                   ^~~~~~~~~~~~~~~\n",
      "\u001b[0m\u001b[0;32m                   aligned_alloc\n",
      "\u001b[0m\u001b[1m/usr/include/stdlib.h:592:14: \u001b[0m\u001b[0;1;30mnote: \u001b[0m'aligned_alloc' declared here\u001b[0m\n",
      "extern void *aligned_alloc (size_t __alignment, size_t __size)\n",
      "\u001b[0;1;32m             ^\n",
      "\u001b[0m\u001b[1m/tmp/cuda_helper-checkpoint.hpp-150580.hip:383:11: \u001b[0m\u001b[0;1;31merror: \u001b[0m\u001b[1mredefinition of 'buffer'\u001b[0m\n",
      "    void* buffer = aligned_alloc(alignment, nbytes);\n",
      "\u001b[0;1;32m          ^\n",
      "\u001b[0m\u001b[1m/tmp/cuda_helper-checkpoint.hpp-150580.hip:381:11: \u001b[0m\u001b[0;1;30mnote: \u001b[0mprevious definition is here\u001b[0m\n",
      "    void* buffer = _aligned_malloc(nbytes, alignment);\n",
      "\u001b[0;1;32m          ^\n",
      "\u001b[0m3 errors generated when compiling for host.\n",
      "Error while processing /tmp/cuda_helper-checkpoint.hpp-150580.hip.\n",
      "\n",
      "[HIPIFY] info: file './.ipynb_checkpoints/cuda_helper-checkpoint.hpp' statistics:\n",
      "\n",
      "  ERROR: Statistics is invalid due to failed hipification.\n",
      "\n",
      "  CONVERTED refs count: 52\n",
      "  UNCONVERTED refs count: 0\n",
      "  CONVERSION %: 100.0\n",
      "  REPLACED bytes: 814\n",
      "  TOTAL bytes: 24629\n",
      "  CHANGED lines of code: 48\n",
      "  TOTAL lines of code: 789\n",
      "  CODE CHANGED (in bytes) %: 3.3\n",
      "  CODE CHANGED (in lines) %: 6.1\n",
      "  TIME ELAPSED s: 0.48\n",
      "[HIPIFY] info: CONVERTED refs by type:\n",
      "  error: 3\n",
      "  init: 1\n",
      "  device: 11\n",
      "  stream: 2\n",
      "  event: 9\n",
      "  execution: 1\n",
      "  include_cuda_main_header: 2\n",
      "  type: 16\n",
      "  numeric_literal: 5\n",
      "  define: 2\n",
      "[HIPIFY] info: CONVERTED refs by API:\n",
      "  CUDA Driver API: 7\n",
      "  CUDA RT API: 45\n",
      "[HIPIFY] info: CONVERTED refs by names:\n",
      "  CUDA_SUCCESS: 2\n",
      "  CUresult: 2\n",
      "  cuGetErrorString: 1\n",
      "  cuInit: 1\n",
      "  cuda.h: 1\n",
      "  cudaDevAttrManagedMemory: 1\n",
      "  cudaDeviceGetAttribute: 1\n",
      "  cudaDeviceProp: 2\n",
      "  cudaDeviceReset: 1\n",
      "  cudaDeviceSynchronize: 1\n",
      "  cudaError_t: 2\n",
      "  cudaEventCreate: 2\n",
      "  cudaEventDestroy: 2\n",
      "  cudaEventElapsedTime: 1\n",
      "  cudaEventRecord: 2\n",
      "  cudaEventSynchronize: 2\n",
      "  cudaEvent_t: 3\n",
      "  cudaGetDevice: 1\n",
      "  cudaGetDeviceCount: 2\n",
      "  cudaGetDeviceProperties: 2\n",
      "  cudaGetErrorString: 2\n",
      "  cudaLaunchKernel: 1\n",
      "  cudaSetDevice: 3\n",
      "  cudaStreamCreateWithFlags: 1\n",
      "  cudaStreamDefault: 1\n",
      "  cudaStreamDestroy: 1\n",
      "  cudaStreamNonBlocking: 1\n",
      "  cudaStream_t: 7\n",
      "  cudaSuccess: 2\n",
      "  cuda_runtime.h: 1\n",
      "\u001b[0;1;35mwarning: \u001b[0m\u001b[1mCUDA version is newer than the latest partially supported version 11.8 [-Wunknown-cuda-version]\u001b[0m\n",
      "\u001b[1m/tmp/cuda_helper.hpp-cfba75.hip:95:5: \u001b[0m\u001b[0;1;31merror: \u001b[0m\u001b[1munknown type name 'SYSTEM_INFO'\u001b[0m\n",
      "    SYSTEM_INFO sys_info;\n",
      "\u001b[0;1;32m    ^\n",
      "\u001b[0m\u001b[1m/tmp/cuda_helper.hpp-cfba75.hip:381:20: \u001b[0m\u001b[0;1;31merror: \u001b[0m\u001b[1muse of undeclared identifier '_aligned_malloc'; did you mean 'aligned_alloc'?\u001b[0m\n",
      "    void* buffer = _aligned_malloc(nbytes, alignment);\n",
      "\u001b[0;1;32m                   ^~~~~~~~~~~~~~~\n",
      "\u001b[0m\u001b[0;32m                   aligned_alloc\n",
      "\u001b[0m\u001b[1m/usr/include/stdlib.h:592:14: \u001b[0m\u001b[0;1;30mnote: \u001b[0m'aligned_alloc' declared here\u001b[0m\n",
      "extern void *aligned_alloc (size_t __alignment, size_t __size)\n",
      "\u001b[0;1;32m             ^\n",
      "\u001b[0m\u001b[1m/tmp/cuda_helper.hpp-cfba75.hip:383:11: \u001b[0m\u001b[0;1;31merror: \u001b[0m\u001b[1mredefinition of 'buffer'\u001b[0m\n",
      "    void* buffer = aligned_alloc(alignment, nbytes);\n",
      "\u001b[0;1;32m          ^\n",
      "\u001b[0m\u001b[1m/tmp/cuda_helper.hpp-cfba75.hip:381:11: \u001b[0m\u001b[0;1;30mnote: \u001b[0mprevious definition is here\u001b[0m\n",
      "    void* buffer = _aligned_malloc(nbytes, alignment);\n",
      "\u001b[0;1;32m          ^\n",
      "\u001b[0m3 errors generated when compiling for host.\n",
      "Error while processing /tmp/cuda_helper.hpp-cfba75.hip.\n",
      "\n",
      "[HIPIFY] info: file './cuda_helper.hpp' statistics:\n",
      "\n",
      "  ERROR: Statistics is invalid due to failed hipification.\n",
      "\n",
      "  CONVERTED refs count: 52\n",
      "  UNCONVERTED refs count: 0\n",
      "  CONVERSION %: 100.0\n",
      "  REPLACED bytes: 814\n",
      "  TOTAL bytes: 24629\n",
      "  CHANGED lines of code: 48\n",
      "  TOTAL lines of code: 789\n",
      "  CODE CHANGED (in bytes) %: 3.3\n",
      "  CODE CHANGED (in lines) %: 6.1\n",
      "  TIME ELAPSED s: 0.46\n",
      "[HIPIFY] info: CONVERTED refs by type:\n",
      "  error: 3\n",
      "  init: 1\n",
      "  device: 11\n",
      "  stream: 2\n",
      "  event: 9\n",
      "  execution: 1\n",
      "  include_cuda_main_header: 2\n",
      "  type: 16\n",
      "  numeric_literal: 5\n",
      "  define: 2\n",
      "[HIPIFY] info: CONVERTED refs by API:\n",
      "  CUDA Driver API: 7\n",
      "  CUDA RT API: 45\n",
      "[HIPIFY] info: CONVERTED refs by names:\n",
      "  CUDA_SUCCESS: 2\n",
      "  CUresult: 2\n",
      "  cuGetErrorString: 1\n",
      "  cuInit: 1\n",
      "  cuda.h: 1\n",
      "  cudaDevAttrManagedMemory: 1\n",
      "  cudaDeviceGetAttribute: 1\n",
      "  cudaDeviceProp: 2\n",
      "  cudaDeviceReset: 1\n",
      "  cudaDeviceSynchronize: 1\n",
      "  cudaError_t: 2\n",
      "  cudaEventCreate: 2\n",
      "  cudaEventDestroy: 2\n",
      "  cudaEventElapsedTime: 1\n",
      "  cudaEventRecord: 2\n",
      "  cudaEventSynchronize: 2\n",
      "  cudaEvent_t: 3\n",
      "  cudaGetDevice: 1\n",
      "  cudaGetDeviceCount: 2\n",
      "  cudaGetDeviceProperties: 2\n",
      "  cudaGetErrorString: 2\n",
      "  cudaLaunchKernel: 1\n",
      "  cudaSetDevice: 3\n",
      "  cudaStreamCreateWithFlags: 1\n",
      "  cudaStreamDefault: 1\n",
      "  cudaStreamDestroy: 1\n",
      "  cudaStreamNonBlocking: 1\n",
      "  cudaStream_t: 7\n",
      "  cudaSuccess: 2\n",
      "  cuda_runtime.h: 1\n",
      "\u001b[0;1;35mwarning: \u001b[0m\u001b[1mCUDA version is newer than the latest partially supported version 11.8 [-Wunknown-cuda-version]\u001b[0m\n",
      "\u001b[1m/tmp/mat_helper.hpp-22bc3d.hip:162:5: \u001b[0m\u001b[0;1;31merror: \u001b[0m\u001b[1muse of undeclared identifier 'assert'\u001b[0m\n",
      "    assert(len0_src>=K0);\n",
      "\u001b[0;1;32m    ^\n",
      "\u001b[0m\u001b[1m/tmp/mat_helper.hpp-22bc3d.hip:163:5: \u001b[0m\u001b[0;1;31merror: \u001b[0m\u001b[1muse of undeclared identifier 'assert'\u001b[0m\n",
      "    assert(len1_src>=K1);    \n",
      "\u001b[0;1;32m    ^\n",
      "\u001b[0m2 errors generated when compiling for host.\n",
      "Error while processing /tmp/mat_helper.hpp-22bc3d.hip.\n",
      "\n",
      "[HIPIFY] info: file './mat_helper.hpp' statistics:\n",
      "\n",
      "  ERROR: Statistics is invalid due to failed hipification.\n",
      "\n",
      "  CONVERTED refs count: 0\n",
      "  UNCONVERTED refs count: 0\n",
      "  CONVERSION %: 0.0\n",
      "  REPLACED bytes: 0\n",
      "  TOTAL bytes: 4497\n",
      "  CHANGED lines of code: 1\n",
      "  TOTAL lines of code: 180\n",
      "  CODE CHANGED (in bytes) %: 0.0\n",
      "  CODE CHANGED (in lines) %: 0.6\n",
      "  TIME ELAPSED s: 0.50\n",
      "\u001b[0;1;35mwarning: \u001b[0m\u001b[1mCUDA version is newer than the latest partially supported version 11.8 [-Wunknown-cuda-version]\u001b[0m\n",
      "\n",
      "[HIPIFY] info: file './mat_size.hpp' statistics:\n",
      "  CONVERTED refs count: 0\n",
      "  UNCONVERTED refs count: 0\n",
      "  CONVERSION %: 0.0\n",
      "  REPLACED bytes: 0\n",
      "  TOTAL bytes: 107\n",
      "  CHANGED lines of code: 1\n",
      "  TOTAL lines of code: 3\n",
      "  CODE CHANGED (in bytes) %: 0.0\n",
      "  CODE CHANGED (in lines) %: 33.3\n",
      "  TIME ELAPSED s: 0.19\n",
      "\n",
      "[HIPIFY] info: file 'GLOBAL' statistics:\n",
      "\n",
      "  ERROR: Statistics is invalid due to failed hipification.\n",
      "\n",
      "  CONVERTED refs count: 132\n",
      "  UNCONVERTED refs count: 0\n",
      "  CONVERSION %: 100.0\n",
      "  REPLACED bytes: 2002\n",
      "  TOTAL bytes: 65750\n",
      "  CHANGED lines of code: 122\n",
      "  TOTAL lines of code: 2141\n",
      "  CODE CHANGED (in bytes) %: 3.0\n",
      "  CODE CHANGED (in lines) %: 5.7\n",
      "  TIME ELAPSED s: 2.79\n",
      "[HIPIFY] info: CONVERTED refs by type:\n",
      "  error: 8\n",
      "  init: 2\n",
      "  device: 24\n",
      "  memory: 18\n",
      "  stream: 4\n",
      "  event: 18\n",
      "  execution: 2\n",
      "  include_cuda_main_header: 4\n",
      "  type: 32\n",
      "  numeric_literal: 16\n",
      "  define: 4\n",
      "[HIPIFY] info: CONVERTED refs by API:\n",
      "  CUDA Driver API: 14\n",
      "  CUDA RT API: 118\n",
      "[HIPIFY] info: CONVERTED refs by names:\n",
      "  CUDA_SUCCESS: 4\n",
      "  CUresult: 4\n",
      "  cuGetErrorString: 2\n",
      "  cuInit: 2\n",
      "  cuda.h: 2\n",
      "  cudaDevAttrManagedMemory: 2\n",
      "  cudaDeviceGetAttribute: 2\n",
      "  cudaDeviceProp: 4\n",
      "  cudaDeviceReset: 2\n",
      "  cudaDeviceSynchronize: 4\n",
      "  cudaError_t: 4\n",
      "  cudaEventCreate: 4\n",
      "  cudaEventDestroy: 4\n",
      "  cudaEventElapsedTime: 2\n",
      "  cudaEventRecord: 4\n",
      "  cudaEventSynchronize: 4\n",
      "  cudaEvent_t: 6\n",
      "  cudaFree: 6\n",
      "  cudaGetDevice: 2\n",
      "  cudaGetDeviceCount: 4\n",
      "  cudaGetDeviceProperties: 4\n",
      "  cudaGetErrorString: 4\n",
      "  cudaGetLastError: 2\n",
      "  cudaLaunchKernel: 2\n",
      "  cudaMalloc: 6\n",
      "  cudaMemcpy: 6\n",
      "  cudaMemcpyDeviceToHost: 2\n",
      "  cudaMemcpyHostToDevice: 4\n",
      "  cudaSetDevice: 6\n",
      "  cudaStreamCreateWithFlags: 2\n",
      "  cudaStreamDefault: 2\n",
      "  cudaStreamDestroy: 2\n",
      "  cudaStreamNonBlocking: 2\n",
      "  cudaStream_t: 14\n",
      "  cudaSuccess: 4\n",
      "  cuda_runtime.h: 2\n",
      "\n",
      "[HIPIFY] info: TOTAL statistics:\n",
      "  CONVERTED files: 3\n",
      "  PROCESSED files: 6\n"
     ]
    }
   ],
   "source": [
    "!cd cuda_mat_mult; hipexamine.sh . -I."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec657b6f-2a15-4ad1-b8ef-650aedeb25fb",
   "metadata": {},
   "source": [
    "### Porting a directory structure inplace\n",
    "\n",
    "Both the **hipconvertinplace-perl.sh** and **hipconvertinplace.sh** scripts have the ability to convert a code tree inplace. The additional option **-hip-kernel-execution-syntax** replaces CUDA triple Chevron kernel calls with the equivalent call to **hipLaunchKernelGGL** macro.\n",
    "\n",
    "#### Porting inplace with hipify-perl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23846221-b541-45a8-950b-9278b477b207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[HIPIFY] info: file 'temp_mat_mult/mat_mult.cpp' statistics:\n",
      "  CONVERTED refs count: 15\n",
      "  TOTAL lines of code: 190\n",
      "  WARNINGS: 0\n",
      "[HIPIFY] info: CONVERTED refs by names:\n",
      "  cudaDeviceSynchronize => hipDeviceSynchronize: 1\n",
      "  cudaFree => hipFree: 3\n",
      "  cudaGetLastError => hipGetLastError: 1\n",
      "  cudaMalloc => hipMalloc: 3\n",
      "  cudaMemcpy => hipMemcpy: 3\n",
      "  cudaMemcpyDeviceToHost => hipMemcpyDeviceToHost: 1\n",
      "  cudaMemcpyHostToDevice => hipMemcpyHostToDevice: 2\n",
      "  warning: temp_mat_mult/cuda_helper.hpp:480: removed identifier \"cudaLaunch\" since CUDA 10.1\n",
      "\n",
      "[HIPIFY] info: file 'temp_mat_mult/cuda_helper.hpp' statistics:\n",
      "  CONVERTED refs count: 56\n",
      "  TOTAL lines of code: 789\n",
      "  WARNINGS: 1\n",
      "[HIPIFY] info: CONVERTED refs by names:\n",
      "  CUDA_SUCCESS => hipSuccess: 4\n",
      "  CUresult => hipError_t: 4\n",
      "  cuGetErrorString => hipDrvGetErrorString: 1\n",
      "  cuInit => hipInit: 1\n",
      "  cuda.h => hip/hip_runtime.h: 2\n",
      "  cudaDevAttrManagedMemory => hipDeviceAttributeManagedMemory: 1\n",
      "  cudaDeviceGetAttribute => hipDeviceGetAttribute: 1\n",
      "  cudaDeviceProp => hipDeviceProp_t: 2\n",
      "  cudaDeviceReset => hipDeviceReset: 1\n",
      "  cudaDeviceSynchronize => hipDeviceSynchronize: 1\n",
      "  cudaError_t => hipError_t: 4\n",
      "  cudaEventCreate => hipEventCreate: 2\n",
      "  cudaEventDestroy => hipEventDestroy: 2\n",
      "  cudaEventElapsedTime => hipEventElapsedTime: 1\n",
      "  cudaEventRecord => hipEventRecord: 3\n",
      "  cudaEventSynchronize => hipEventSynchronize: 2\n",
      "  cudaEvent_t => hipEvent_t: 3\n",
      "  cudaGetDevice => hipGetDevice: 1\n",
      "  cudaGetDeviceCount => hipGetDeviceCount: 2\n",
      "  cudaGetDeviceProperties => hipGetDeviceProperties: 2\n",
      "  cudaGetErrorString => hipGetErrorString: 2\n",
      "  cudaLaunchKernel => hipLaunchKernel: 1\n",
      "  cudaSetDevice => hipSetDevice: 3\n",
      "  cudaStreamCreateWithFlags => hipStreamCreateWithFlags: 1\n",
      "  cudaStreamDefault => hipStreamDefault: 1\n",
      "  cudaStreamDestroy => hipStreamDestroy: 1\n",
      "  cudaStreamNonBlocking => hipStreamNonBlocking: 1\n",
      "  cudaStream_t => hipStream_t: 7\n",
      "  cudaSuccess => hipSuccess: 4\n",
      "  cuda_runtime.h => hip/hip_runtime.h: 2\n",
      "\n",
      "[HIPIFY] info: file 'GLOBAL' statistics:\n",
      "  CONVERTED refs count: 71\n",
      "  TOTAL lines of code: 1162\n",
      "  WARNINGS: 1\n",
      "[HIPIFY] info: CONVERTED refs by names:\n",
      "  CUDA_SUCCESS => hipSuccess: 4\n",
      "  CUresult => hipError_t: 4\n",
      "  cuGetErrorString => hipDrvGetErrorString: 1\n",
      "  cuInit => hipInit: 1\n",
      "  cuda.h => hip/hip_runtime.h: 2\n",
      "  cudaDevAttrManagedMemory => hipDeviceAttributeManagedMemory: 1\n",
      "  cudaDeviceGetAttribute => hipDeviceGetAttribute: 1\n",
      "  cudaDeviceProp => hipDeviceProp_t: 2\n",
      "  cudaDeviceReset => hipDeviceReset: 1\n",
      "  cudaDeviceSynchronize => hipDeviceSynchronize: 2\n",
      "  cudaError_t => hipError_t: 4\n",
      "  cudaEventCreate => hipEventCreate: 2\n",
      "  cudaEventDestroy => hipEventDestroy: 2\n",
      "  cudaEventElapsedTime => hipEventElapsedTime: 1\n",
      "  cudaEventRecord => hipEventRecord: 3\n",
      "  cudaEventSynchronize => hipEventSynchronize: 2\n",
      "  cudaEvent_t => hipEvent_t: 3\n",
      "  cudaFree => hipFree: 3\n",
      "  cudaGetDevice => hipGetDevice: 1\n",
      "  cudaGetDeviceCount => hipGetDeviceCount: 2\n",
      "  cudaGetDeviceProperties => hipGetDeviceProperties: 2\n",
      "  cudaGetErrorString => hipGetErrorString: 2\n",
      "  cudaGetLastError => hipGetLastError: 1\n",
      "  cudaLaunchKernel => hipLaunchKernel: 1\n",
      "  cudaMalloc => hipMalloc: 3\n",
      "  cudaMemcpy => hipMemcpy: 3\n",
      "  cudaMemcpyDeviceToHost => hipMemcpyDeviceToHost: 1\n",
      "  cudaMemcpyHostToDevice => hipMemcpyHostToDevice: 2\n",
      "  cudaSetDevice => hipSetDevice: 3\n",
      "  cudaStreamCreateWithFlags => hipStreamCreateWithFlags: 1\n",
      "  cudaStreamDefault => hipStreamDefault: 1\n",
      "  cudaStreamDestroy => hipStreamDestroy: 1\n",
      "  cudaStreamNonBlocking => hipStreamNonBlocking: 1\n",
      "  cudaStream_t => hipStream_t: 7\n",
      "  cudaSuccess => hipSuccess: 4\n",
      "  cuda_runtime.h => hip/hip_runtime.h: 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!hipconvertinplace-perl.sh temp_mat_mult -hip-kernel-execution-syntax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2326375-a616-4704-81c3-9c3c671151bb",
   "metadata": {},
   "source": [
    "#### Porting inplace with hipify-clang\n",
    "\n",
    "Here is the same port with **hipify-clang**. I have commented it out because it doesn't produce a port without access to all the necessary include files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3be9bcbc-4135-41ab-903d-c048cabbf961",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!hipconvertinplace.sh temp_mat_mult -hip-kernel-execution-syntax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc505b5e-1291-4007-86e8-ae027dfca4cd",
   "metadata": {},
   "source": [
    "### Building the ported code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f3bf73-e76a-49cc-b006-4b90e1eed6b2",
   "metadata": {},
   "source": [
    "If we examine the source tree we see that every source file that has been hipified has been first copied to a file with suffix `*.prehip`. Then the converted code is overwritten in place of the old file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "93c79b1c-e721-4421-9183-a44ed01ccedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 2464\n",
      "-rw-r--r-- 1 toby toby  262144 Jun 10 16:24 array_A.dat\n",
      "-rw-r--r-- 1 toby toby  262144 Jun 10 16:24 array_B.dat\n",
      "-rw-r--r-- 1 toby toby  262144 Jun 10 16:24 array_C.dat\n",
      "-rw-r--r-- 1 toby toby   24660 Jun 10 16:26 cuda_helper.hpp\n",
      "-rw-r--r-- 1 toby toby   24629 Jun 10 16:25 cuda_helper.hpp.prehip\n",
      "-rw-r--r-- 1 toby toby     703 Jun 10 16:24 Makefile\n",
      "-rw-r--r-- 1 toby toby    4497 Jun 10 16:26 mat_helper.hpp\n",
      "-rw-r--r-- 1 toby toby    4497 Jun 10 16:25 mat_helper.hpp.prehip\n",
      "-rw-r--r-- 1 toby toby    5975 Jun 10 16:25 mat_mult.cpp\n",
      "-rw-r--r-- 1 toby toby    5944 Jun 10 16:24 mat_mult.cpp.prehip\n",
      "-rwxr-xr-x 1 toby toby 1632488 Jun 10 16:24 mat_mult.exe\n",
      "-rw-r--r-- 1 toby toby     107 Jun 10 16:26 mat_size.hpp\n",
      "-rw-r--r-- 1 toby toby     107 Jun 10 16:25 mat_size.hpp.prehip\n"
     ]
    }
   ],
   "source": [
    "!ls -l temp_mat_mult"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2964cedf-53df-49d3-a98a-812a59e34aeb",
   "metadata": {},
   "source": [
    "Try making the ported code with hipcc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b1e07138-163e-4615-93fe-62b653908f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -r *.exe\n",
      "hipcc -g -O2 -x cu mat_mult.cpp -o mat_mult.exe -lcuda \n",
      "\u001b[01m\u001b[0m\u001b[01m/opt/rocm-6.0.2/include/hip/nvidia_detail/nvidia_hip_runtime_api.h(506)\u001b[0m: \u001b[01;35mwarning\u001b[0m #1056-D: returning pointer to local variable\n",
      "      return &a;\n",
      "             ^\n",
      "\n",
      "\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
      "\n",
      "\u001b[01m\u001b[0m\u001b[01mcuda_helper.hpp(54)\u001b[0m: \u001b[01;31merror\u001b[0m: function \u001b[01m\"h_errchk\"\u001b[0m has already been defined\n",
      "  void h_errchk(hipError_t errcode, const char* message) {\n",
      "       ^\n",
      "\n",
      "1 error detected in the compilation of \"mat_mult.cpp\".\n",
      "make: *** [Makefile:29: mat_mult.exe] Error 2\n"
     ]
    }
   ],
   "source": [
    "!cd temp_mat_mult; make clean; make CXX=\"hipcc\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39501460-ca62-4103-be05-aa00069beaaf",
   "metadata": {},
   "source": [
    "In the original file **cuda_mat_mult/cuda_helper.hpp** we had overloaded the **h_errchk** function to accept errorcodes of both type **CUResult** and **cudaError_t**. Following conversion to HIP the errorcode has been replaced with just **hipError_t**. Therefore we need to manually delete the duplicate **h_errchk** function in **[temp_mat_mult/cuda_helper.hpp.prehip](temp_mat_mult/cuda_helper.hpp.prehip)**. Then rerun the conversion and the make. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9142234b-e1d6-4a1c-b387-c679b25df42b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  warning: cuda_helper.hpp:465: removed identifier \"cudaLaunch\" since CUDA 10.1\n",
      "hipcc -g -O2 -x cu mat_mult.cpp -o mat_mult.exe -lcuda \n",
      "\u001b[01m\u001b[0m\u001b[01m/opt/rocm-6.0.2/include/hip/nvidia_detail/nvidia_hip_runtime_api.h(506)\u001b[0m: \u001b[01;35mwarning\u001b[0m #1056-D: returning pointer to local variable\n",
      "      return &a;\n",
      "             ^\n",
      "\n",
      "\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
      "\n",
      "\u001b[01m\u001b[0m\u001b[01m/opt/rocm-6.0.2/include/hip/nvidia_detail/nvidia_hip_runtime_api.h(506)\u001b[0m: \u001b[01;35mwarning\u001b[0m #1056-D: returning pointer to local variable\n",
      "      return &a;\n",
      "             ^\n",
      "\n",
      "\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
      "\n",
      "\u001b[01m\u001b[K/opt/rocm-6.0.2/include/hip/nvidia_detail/nvidia_hip_runtime_api.h:\u001b[m\u001b[K In function \u001b[01m\u001b[KCUDA_RESOURCE_DESC*\u001b[01;32m\u001b[K hipResourceDesTocudaResourceDes\u001b[m\u001b[K(const HIP_RESOURCE_DESC*)\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K/opt/rocm-6.0.2/include/hip/nvidia_detail/nvidia_hip_runtime_api.h:506:8:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kaddress of local variable \u001b[01m\u001b[Ka\u001b[m\u001b[K returned [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wreturn-local-addr\u0007-Wreturn-local-addr\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      "  506 |     ret\u001b[01;35m\u001b[Kur\u001b[m\u001b[Kn &a;\n",
      "      |        \u001b[01;35m\u001b[K^~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/opt/rocm-6.0.2/include/hip/nvidia_detail/nvidia_hip_runtime_api.h:480:20:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
      "  480 |     CUDA_RESOURCE_D\u001b[01;36m\u001b[KE\u001b[m\u001b[KSC a;\n",
      "      |                    \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/opt/rocm-6.0.2/include/hip/nvidia_detail/nvidia_hip_runtime_api.h:\u001b[m\u001b[K In function \u001b[01m\u001b[KhipError_t\u001b[01;32m\u001b[K hipMemcpyToArray\u001b[m\u001b[K(hipArray_t, size_t, size_t, const void*, size_t, hipMemcpyKind)\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K/opt/rocm-6.0.2/include/hip/nvidia_detail/nvidia_hip_runtime_api.h:2070:48:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K\u001b[01m\u001b[KcudaError_t\u001b[01;32m\u001b[K cudaMemcpyToArray\u001b[m\u001b[K(cudaArray_t, size_t, size_t, const void*, size_t, cudaMemcpyKind)\u001b[m\u001b[K is deprecated [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 2070 |     return hipCUDAErrorTohipEr\u001b[01;35m\u001b[Kror(\u001b[m\u001b[K\n",
      "      |                               \u001b[01;35m\u001b[K~~~~\u001b[m\u001b[K             \u001b[01;35m\u001b[K^\u001b[m\u001b[K                                        \n",
      "\u001b[01m\u001b[K/usr/local/cuda/bin/../targets/x86_64-linux/include/cuda_runtime_api.h:8181:46:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
      " 8181 | extern __CUDA_DEPRECATED __host__ cudaError_t\u001b[01;36m\u001b[K CUDARTAPI cudaMe\u001b[m\u001b[KmcpyToArray(cudaArray_t dst, size_t wOffset, size_t hOffset, const void *src, size_t count, enum cudaMemcpyKind kind);\n",
      "      |                                              \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/opt/rocm-6.0.2/include/hip/nvidia_detail/nvidia_hip_runtime_api.h:\u001b[m\u001b[K In function \u001b[01m\u001b[KhipError_t\u001b[01;32m\u001b[K hipMemcpyFromArray\u001b[m\u001b[K(void*, hipArray_const_t, size_t, size_t, size_t, hipMemcpyKind)\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K/opt/rocm-6.0.2/include/hip/nvidia_detail/nvidia_hip_runtime_api.h:2077:50:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K\u001b[01m\u001b[KcudaError_t\u001b[01;32m\u001b[K cudaMemcpyFromArray\u001b[m\u001b[K(void*, cudaArray_const_t, size_t, size_t, size_t, cudaMemcpyKind)\u001b[m\u001b[K is deprecated [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 2077 |     return hipCUDAErrorTohipEr\u001b[01;35m\u001b[Kror(cudaMemcpyFromArray(dst, srcArray, wOffset, hOffset, count,\u001b[m\u001b[K\n",
      "      |                               \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K  \n",
      "\u001b[01m\u001b[K/usr/local/cuda/bin/../targets/x86_64-linux/include/cuda_runtime_api.h:8223:46:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
      " 8223 | extern __CUDA_DEPRECATED __host__ cudaError_t\u001b[01;36m\u001b[K CUDARTAPI cudaMemc\u001b[m\u001b[KpyFromArray(void *dst, cudaArray_const_t src, size_t wOffset, size_t hOffset, size_t count, enum cudaMemcpyKind kind);\n",
      "      |                                              \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/opt/rocm-6.0.2/include/hip/nvidia_detail/nvidia_hip_runtime_api.h:\u001b[m\u001b[K In function \u001b[01m\u001b[KhipError_t\u001b[01;32m\u001b[K hipFuncSetSharedMemConfig\u001b[m\u001b[K(const void*, hipSharedMemConfig)\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K/opt/rocm-6.0.2/include/hip/nvidia_detail/nvidia_hip_runtime_api.h:2108:57:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K\u001b[01m\u001b[KcudaError_t\u001b[01;32m\u001b[K cudaFuncSetSharedMemConfig\u001b[m\u001b[K(const void*, cudaSharedMemConfig)\u001b[m\u001b[K is deprecated [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 2108 |     return hipCUDAErrorTohipEr\u001b[01;35m\u001b[Kror(cudaFuncSetSharedMemConfig(func, con\u001b[m\u001b[Kfig));\n",
      "      |                               \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/usr/local/cuda/bin/../targets/x86_64-linux/include/cuda_runtime_api.h:4971:46:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
      " 4971 | extern __CUDA_DEPRECATED __host__ cudaError_t\u001b[01;36m\u001b[K CUDARTAPI cudaFuncSetShar\u001b[m\u001b[KedMemConfig(const void *func, enum cudaSharedMemConfig config);\n",
      "      |                                              \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/opt/rocm-6.0.2/include/hip/nvidia_detail/nvidia_hip_runtime_api.h:\u001b[m\u001b[K In function \u001b[01m\u001b[KhipError_t\u001b[01;32m\u001b[K hipCtxSetSharedMemConfig\u001b[m\u001b[K(hipSharedMemConfig)\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K/opt/rocm-6.0.2/include/hip/nvidia_detail/nvidia_hip_runtime_api.h:2981:53:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K\u001b[01m\u001b[KCUresult\u001b[01;32m\u001b[K cuCtxSetSharedMemConfig\u001b[m\u001b[K(CUsharedconfig)\u001b[m\u001b[K is deprecated [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 2981 |     return hipCUResultTohipEr\u001b[01;35m\u001b[Kror(cuCtxSetSharedMemConfig((CUsharedconfig)con\u001b[m\u001b[Kfig));\n",
      "      |                              \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/usr/local/cuda/bin/../targets/x86_64-linux/include/cuda.h:6855:36:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
      " 6855 | __CUDA_DEPRECATED CUresult CUDAAPI \u001b[01;36m\u001b[KcuCtxSetSharedMemConfig\u001b[m\u001b[K(CUsharedconfig config);\n",
      "      |                                    \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/opt/rocm-6.0.2/include/hip/nvidia_detail/nvidia_hip_runtime_api.h:\u001b[m\u001b[K In function \u001b[01m\u001b[KhipError_t\u001b[01;32m\u001b[K hipCtxGetSharedMemConfig\u001b[m\u001b[K(hipSharedMemConfig*)\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K/opt/rocm-6.0.2/include/hip/nvidia_detail/nvidia_hip_runtime_api.h:2985:53:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K\u001b[01m\u001b[KCUresult\u001b[01;32m\u001b[K cuCtxGetSharedMemConfig\u001b[m\u001b[K(CUsharedconfig*)\u001b[m\u001b[K is deprecated [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 2985 |     return hipCUResultTohipEr\u001b[01;35m\u001b[Kror(cuCtxGetSharedMemConfig((CUsharedconfig*)pConf\u001b[m\u001b[Kig));\n",
      "      |                              \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/usr/local/cuda/bin/../targets/x86_64-linux/include/cuda.h:6800:36:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
      " 6800 | __CUDA_DEPRECATED CUresult CUDAAPI \u001b[01;36m\u001b[KcuCtxGetSharedMemConfig\u001b[m\u001b[K(CUsharedconfig *pConfig);\n",
      "      |                                    \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/opt/rocm-6.0.2/include/hip/nvidia_detail/nvidia_hip_runtime_api.h:\u001b[m\u001b[K In function \u001b[01m\u001b[KhipError_t\u001b[01;32m\u001b[K hipCtxDetach\u001b[m\u001b[K(hipCtx_t)\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K/opt/rocm-6.0.2/include/hip/nvidia_detail/nvidia_hip_runtime_api.h:2997:41:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K\u001b[01m\u001b[KCUresult\u001b[01;32m\u001b[K cuCtxDetach\u001b[m\u001b[K(CUcontext)\u001b[m\u001b[K is deprecated [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 2997 |     return hipCUResultTohipEr\u001b[01;35m\u001b[Kror(cuCtxDetach(\u001b[m\u001b[Kctx));\n",
      "      |                              \u001b[01;35m\u001b[K~~~~~~~~~~~^~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/usr/local/cuda/bin/../targets/x86_64-linux/include/cuda.h:6755:36:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
      " 6755 | __CUDA_DEPRECATED CUresult CUDAAPI \u001b[01;36m\u001b[KcuCtxDetach\u001b[m\u001b[K(CUcontext ctx);\n",
      "      |                                    \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/opt/rocm-6.0.2/include/hip/nvidia_detail/nvidia_hip_runtime_api.h:\u001b[m\u001b[K In function \u001b[01m\u001b[KhipError_t\u001b[01;32m\u001b[K hipDeviceComputeCapability\u001b[m\u001b[K(int*, int*, hipDevice_t)\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K/opt/rocm-6.0.2/include/hip/nvidia_detail/nvidia_hip_runtime_api.h:3005:55:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K\u001b[01m\u001b[KCUresult\u001b[01;32m\u001b[K cuDeviceComputeCapability\u001b[m\u001b[K(int*, int*, CUdevice)\u001b[m\u001b[K is deprecated [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 3005 |     return hipCUResultTohipEr\u001b[01;35m\u001b[Kror(cuDeviceComputeCapability(major, minor, dev\u001b[m\u001b[Kice));\n",
      "      |                              \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/usr/local/cuda/bin/../targets/x86_64-linux/include/cuda.h:5440:36:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
      " 5440 | __CUDA_DEPRECATED CUresult CUDAAPI \u001b[01;36m\u001b[KcuDeviceComputeCapability\u001b[m\u001b[K(int *major, int *minor, CUdevice dev);\n",
      "      |                                    \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/opt/rocm-6.0.2/include/hip/nvidia_detail/nvidia_hip_runtime_api.h:\u001b[m\u001b[K In function \u001b[01m\u001b[KhipError_t\u001b[01;32m\u001b[K hipDeviceGetSharedMemConfig\u001b[m\u001b[K(hipSharedMemConfig*)\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K/opt/rocm-6.0.2/include/hip/nvidia_detail/nvidia_hip_runtime_api.h:3038:59:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K\u001b[01m\u001b[KcudaError_t\u001b[01;32m\u001b[K cudaDeviceGetSharedMemConfig\u001b[m\u001b[K(cudaSharedMemConfig*)\u001b[m\u001b[K is deprecated [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 3038 |     return hipCUDAErrorTohipEr\u001b[01;35m\u001b[Kror(cudaDeviceGetSharedMemConfig(con\u001b[m\u001b[Kfig));\n",
      "      |                               \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/usr/local/cuda/bin/../targets/x86_64-linux/include/cuda_runtime_api.h:1043:46:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
      " 1043 | extern __CUDA_DEPRECATED __host__ __cudart_bu\u001b[01;36m\u001b[Kiltin__ cudaError_t CUDARTAP\u001b[m\u001b[KI cudaDeviceGetSharedMemConfig(enum cudaSharedMemConfig *pConfig);\n",
      "      |                                              \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/opt/rocm-6.0.2/include/hip/nvidia_detail/nvidia_hip_runtime_api.h:\u001b[m\u001b[K In function \u001b[01m\u001b[KhipError_t\u001b[01;32m\u001b[K hipDeviceSetSharedMemConfig\u001b[m\u001b[K(hipSharedMemConfig)\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K/opt/rocm-6.0.2/include/hip/nvidia_detail/nvidia_hip_runtime_api.h:3042:59:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K\u001b[01m\u001b[KcudaError_t\u001b[01;32m\u001b[K cudaDeviceSetSharedMemConfig\u001b[m\u001b[K(cudaSharedMemConfig)\u001b[m\u001b[K is deprecated [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 3042 |     return hipCUDAErrorTohipEr\u001b[01;35m\u001b[Kror(cudaDeviceSetSharedMemConfig(con\u001b[m\u001b[Kfig));\n",
      "      |                               \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/usr/local/cuda/bin/../targets/x86_64-linux/include/cuda_runtime_api.h:1089:46:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
      " 1089 | extern __CUDA_DEPRECATED __host__ cudaError_t\u001b[01;36m\u001b[K CUDARTAPI cudaDeviceSetShar\u001b[m\u001b[KedMemConfig(enum cudaSharedMemConfig config);\n",
      "      |                                              \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/opt/rocm-6.0.2/include/hip/nvidia_detail/nvidia_hip_runtime_api.h:\u001b[m\u001b[K In function \u001b[01m\u001b[KhipError_t\u001b[01;32m\u001b[K hipModuleGetTexRef\u001b[m\u001b[K(CUtexref_st**, hipModule_t, const char*)\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K/opt/rocm-6.0.2/include/hip/nvidia_detail/nvidia_hip_runtime_api.h:3071:47:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K\u001b[01m\u001b[KCUresult\u001b[01;32m\u001b[K cuModuleGetTexRef\u001b[m\u001b[K(CUtexref_st**, CUmodule, const char*)\u001b[m\u001b[K is deprecated [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 3071 |     return hipCUResultTohipEr\u001b[01;35m\u001b[Kror(cuModuleGetTexRef(pTexRef, hmod, n\u001b[m\u001b[Kame));\n",
      "      |                              \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/usr/local/cuda/bin/../targets/x86_64-linux/include/cuda.h:7407:36:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
      " 7407 | __CUDA_DEPRECATED CUresult CUDAAPI \u001b[01;36m\u001b[KcuModuleGetTexRef\u001b[m\u001b[K(CUtexref *pTexRef, CUmodule hmod, const char *name);\n",
      "      |                                    \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/opt/rocm-6.0.2/include/hip/nvidia_detail/nvidia_hip_runtime_api.h:\u001b[m\u001b[K In function \u001b[01m\u001b[KhipError_t\u001b[01;32m\u001b[K hipLaunchCooperativeKernelMultiDevice\u001b[m\u001b[K(hipLaunchParams*, int, unsigned int)\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K/opt/rocm-6.0.2/include/hip/nvidia_detail/nvidia_hip_runtime_api.h:3219:69:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K\u001b[01m\u001b[KcudaError_t\u001b[01;32m\u001b[K cudaLaunchCooperativeKernelMultiDevice\u001b[m\u001b[K(cudaLaunchParams*, unsigned int, unsigned int)\u001b[m\u001b[K is deprecated [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 3219 | rn hipCUDAErrorTohipEr\u001b[01;35m\u001b[Kror(cudaLaunchCooperativeKernelMultiDevice(launchParamsList, numDevices, fl\u001b[m\u001b[Kags));\n",
      "      |                       \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "\n",
      "\u001b[01m\u001b[K/usr/local/cuda/bin/../targets/x86_64-linux/include/cuda_runtime_api.h:4622:46:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
      " 4622 | extern __CUDA_DEPRECATED __host__ cudaError_t\u001b[01;36m\u001b[K CUDARTAPI cudaLaunchCooperativeKernel\u001b[m\u001b[KMultiDevice(struct cudaLaunchParams *launchParamsList, unsigned int numDevices, unsigned int flags  __dv(0));\n",
      "      |                                              \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/opt/rocm-6.0.2/include/hip/nvidia_detail/nvidia_hip_runtime_api.h:\u001b[m\u001b[K In function \u001b[01m\u001b[KhipError_t\u001b[01;32m\u001b[K hipModuleLaunchCooperativeKernelMultiDevice\u001b[m\u001b[K(hipFunctionLaunchParams*, unsigned int, unsigned int)\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K/opt/rocm-6.0.2/include/hip/nvidia_detail/nvidia_hip_runtime_api.h:3226:66:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K\u001b[01m\u001b[KCUresult\u001b[01;32m\u001b[K cuLaunchCooperativeKernelMultiDevice\u001b[m\u001b[K(CUDA_LAUNCH_PARAMS*, unsigned int, unsigned int)\u001b[m\u001b[K is deprecated [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 3226 | eturn hipCUResultTohipEr\u001b[01;35m\u001b[Kror(cuLaunchCooperativeKernelMultiDevice(launchParamsList,\u001b[m\u001b[K\n",
      "      |                         \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K               \n",
      "\n",
      "\u001b[01m\u001b[K/usr/local/cuda/bin/../targets/x86_64-linux/include/cuda.h:17082:36:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
      "17082 | __CUDA_DEPRECATED CUresult CUDAAPI \u001b[01;36m\u001b[KcuLaunchCooperativeKernelMultiDevice\u001b[m\u001b[K(CUDA_LAUNCH_PARAMS *launchParamsList, unsigned int numDevices, unsigned int flags);\n",
      "      |                                    \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/opt/rocm-6.0.2/include/hip/nvidia_detail/nvidia_hip_runtime_api.h:\u001b[m\u001b[K In function \u001b[01m\u001b[KhipError_t\u001b[01;32m\u001b[K hipTexRefSetAddressMode\u001b[m\u001b[K(CUtexref, int, CUaddress_mode)\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K/opt/rocm-6.0.2/include/hip/nvidia_detail/nvidia_hip_runtime_api.h:3515:52:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K\u001b[01m\u001b[KCUresult\u001b[01;32m\u001b[K cuTexRefSetAddressMode\u001b[m\u001b[K(CUtexref, int, CUaddress_mode)\u001b[m\u001b[K is deprecated [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 3515 |     return hipCUResultTohipEr\u001b[01;35m\u001b[Kror(cuTexRefSetAddressMode(hTexRef,dim,a\u001b[m\u001b[Km));\n",
      "      |                              \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/usr/local/cuda/bin/../targets/x86_64-linux/include/cuda.h:21313:36:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
      "21313 | __CUDA_DEPRECATED CUresult CUDAAPI \u001b[01;36m\u001b[KcuTexRefSetAddressMode\u001b[m\u001b[K(CUtexref hTexRef, int dim, CUaddress_mode am);\n",
      "      |                                    \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/opt/rocm-6.0.2/include/hip/nvidia_detail/nvidia_hip_runtime_api.h:\u001b[m\u001b[K In function \u001b[01m\u001b[KhipError_t\u001b[01;32m\u001b[K hipTexRefSetFilterMode\u001b[m\u001b[K(CUtexref, CUfilter_mode)\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K/opt/rocm-6.0.2/include/hip/nvidia_detail/nvidia_hip_runtime_api.h:3519:51:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K\u001b[01m\u001b[KCUresult\u001b[01;32m\u001b[K cuTexRefSetFilterMode\u001b[m\u001b[K(CUtexref, CUfilter_mode)\u001b[m\u001b[K is deprecated [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 3519 |     return hipCUResultTohipEr\u001b[01;35m\u001b[Kror(cuTexRefSetFilterMode(hTexRef,\u001b[m\u001b[Kfm));\n",
      "      |                              \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/usr/local/cuda/bin/../targets/x86_64-linux/include/cuda.h:21349:36:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
      "21349 | __CUDA_DEPRECATED CUresult CUDAAPI \u001b[01;36m\u001b[KcuTexRefSetFilterMode\u001b[m\u001b[K(CUtexref hTexRef, CUfilter_mode fm);\n",
      "      |                                    \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/opt/rocm-6.0.2/include/hip/nvidia_detail/nvidia_hip_runtime_api.h:\u001b[m\u001b[K In function \u001b[01m\u001b[KhipError_t\u001b[01;32m\u001b[K hipTexRefSetAddress\u001b[m\u001b[K(size_t*, CUtexref, hipDeviceptr_t, size_t)\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K/opt/rocm-6.0.2/include/hip/nvidia_detail/nvidia_hip_runtime_api.h:3523:51:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K\u001b[01m\u001b[KCUresult\u001b[01;32m\u001b[K cuTexRefSetAddress_v2\u001b[m\u001b[K(size_t*, CUtexref, CUdeviceptr, size_t)\u001b[m\u001b[K is deprecated [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 3523 |     return hipCUResultTohipEr\u001b[01;35m\u001b[Kror(cuTexRefSetAddress(ByteOffset,hTexRef,dptr,bytes));\u001b[m\u001b[K\n",
      "      |                              \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/usr/local/cuda/bin/../targets/x86_64-linux/include/cuda.h:21183:36:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
      "21183 | __CUDA_DEPRECATED CUresult CUDAAPI \u001b[01;36m\u001b[KcuTexRefSetAddress(si\u001b[m\u001b[Kze_t *ByteOffset, CUtexref hTexRef, CUdeviceptr dptr, size_t bytes);\n",
      "      |                                    \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/opt/rocm-6.0.2/include/hip/nvidia_detail/nvidia_hip_runtime_api.h:\u001b[m\u001b[K In function \u001b[01m\u001b[KhipError_t\u001b[01;32m\u001b[K hipTexRefSetAddress2D\u001b[m\u001b[K(CUtexref, const CUDA_ARRAY_DESCRIPTOR*, hipDeviceptr_t, size_t)\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K/opt/rocm-6.0.2/include/hip/nvidia_detail/nvidia_hip_runtime_api.h:3527:53:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K\u001b[01m\u001b[KCUresult\u001b[01;32m\u001b[K cuTexRefSetAddress2D_v3\u001b[m\u001b[K(CUtexref, const CUDA_ARRAY_DESCRIPTOR*, CUdeviceptr, size_t)\u001b[m\u001b[K is deprecated [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 3527 |     return hipCUResultTohipEr\u001b[01;35m\u001b[Kror(cuTexRefSetAddress2D(hTexRef,desc,dptr,Pitch));\u001b[m\u001b[K\n",
      "      |                              \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/usr/local/cuda/bin/../targets/x86_64-linux/include/cuda.h:21238:36:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
      "21238 | __CUDA_DEPRECATED CUresult CUDAAPI \u001b[01;36m\u001b[KcuTexRefSetAddress2D(CU\u001b[m\u001b[Ktexref hTexRef, const CUDA_ARRAY_DESCRIPTOR *desc, CUdeviceptr dptr, size_t Pitch);\n",
      "      |                                    \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/opt/rocm-6.0.2/include/hip/nvidia_detail/nvidia_hip_runtime_api.h:\u001b[m\u001b[K In function \u001b[01m\u001b[KhipError_t\u001b[01;32m\u001b[K hipTexRefSetFormat\u001b[m\u001b[K(CUtexref, CUarray_format, int)\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K/opt/rocm-6.0.2/include/hip/nvidia_detail/nvidia_hip_runtime_api.h:3531:47:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K\u001b[01m\u001b[KCUresult\u001b[01;32m\u001b[K cuTexRefSetFormat\u001b[m\u001b[K(CUtexref, CUarray_format, int)\u001b[m\u001b[K is deprecated [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 3531 |     return hipCUResultTohipEr\u001b[01;35m\u001b[Kror(cuTexRefSetFormat(hTexRef,fmt,NumPackedComponent\u001b[m\u001b[Ks));\n",
      "      |                              \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/usr/local/cuda/bin/../targets/x86_64-linux/include/cuda.h:21270:36:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
      "21270 | __CUDA_DEPRECATED CUresult CUDAAPI \u001b[01;36m\u001b[KcuTexRefSetFormat\u001b[m\u001b[K(CUtexref hTexRef, CUarray_format fmt, int NumPackedComponents);\n",
      "      |                                    \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/opt/rocm-6.0.2/include/hip/nvidia_detail/nvidia_hip_runtime_api.h:\u001b[m\u001b[K In function \u001b[01m\u001b[KhipError_t\u001b[01;32m\u001b[K hipTexRefSetFlags\u001b[m\u001b[K(CUtexref, unsigned int)\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K/opt/rocm-6.0.2/include/hip/nvidia_detail/nvidia_hip_runtime_api.h:3535:46:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K\u001b[01m\u001b[KCUresult\u001b[01;32m\u001b[K cuTexRefSetFlags\u001b[m\u001b[K(CUtexref, unsigned int)\u001b[m\u001b[K is deprecated [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 3535 |     return hipCUResultTohipEr\u001b[01;35m\u001b[Kror(cuTexRefSetFlags(hTexRef,Fla\u001b[m\u001b[Kgs));\n",
      "      |                              \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/usr/local/cuda/bin/../targets/x86_64-linux/include/cuda.h:21549:36:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
      "21549 | __CUDA_DEPRECATED CUresult CUDAAPI \u001b[01;36m\u001b[KcuTexRefSetFlags\u001b[m\u001b[K(CUtexref hTexRef, unsigned int Flags);\n",
      "      |                                    \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/opt/rocm-6.0.2/include/hip/nvidia_detail/nvidia_hip_runtime_api.h:\u001b[m\u001b[K In function \u001b[01m\u001b[KhipError_t\u001b[01;32m\u001b[K hipTexRefSetArray\u001b[m\u001b[K(CUtexref, hipArray_t, unsigned int)\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K/opt/rocm-6.0.2/include/hip/nvidia_detail/nvidia_hip_runtime_api.h:3539:46:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K\u001b[01m\u001b[KCUresult\u001b[01;32m\u001b[K cuTexRefSetArray\u001b[m\u001b[K(CUtexref, CUarray, unsigned int)\u001b[m\u001b[K is deprecated [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 3539 |     return hipCUResultTohipEr\u001b[01;35m\u001b[Kror(cuTexRefSetArray(hTexRef,(CUarray)hArray,Flag\u001b[m\u001b[Ks));\n",
      "      |                              \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/usr/local/cuda/bin/../targets/x86_64-linux/include/cuda.h:21107:36:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
      "21107 | __CUDA_DEPRECATED CUresult CUDAAPI \u001b[01;36m\u001b[KcuTexRefSetArray\u001b[m\u001b[K(CUtexref hTexRef, CUarray hArray, unsigned int Flags);\n",
      "      |                                    \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "Device id: 0\n",
      "\tname:                                    NVIDIA GeForce RTX 3060 Laptop GPU\n",
      "\tglobal memory size:                      6219 MB\n",
      "\tavailable registers per block:           65536 \n",
      "\tmaximum shared memory size per block:    49 KB\n",
      "\tmaximum pitch size for memory copies:    2147 MB\n",
      "\tmax block size:                          (1024,1024,64)\n",
      "\tmax threads in a block:                  1024\n",
      "\tmax Grid size:                           (2147483647,65535,65535)\n",
      "Maximum error (infinity norm) is: 1.52588e-05\n"
     ]
    }
   ],
   "source": [
    "# Run this after fixing code\n",
    "!cd temp_mat_mult; hipify-perl -inplace -hip-kernel-execution-syntax cuda_helper.hpp\n",
    "!cd temp_mat_mult; make CXX=\"hipcc\"\n",
    "!run temp_mat_mult/mat_mult.exe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218d61e3-fdc4-4dd1-8ac1-e537872577fb",
   "metadata": {},
   "source": [
    "Now we should have a successful port of the CUDA code to HIP!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7773754-7ae2-47e9-b51a-bf220e64241f",
   "metadata": {},
   "source": [
    "## Case studies in porting codes\n",
    "\n",
    "Apart from the toy example above, it is informative to try porting some real-world applications and see what learnings can be derived from the process. Here are a few examples.\n",
    "\n",
    "### Visualising shipwrecks\n",
    "\n",
    "From [this resource](https://blogs.nvidia.com/blog/2022/11/18/3d-shipwrecks-perth/) researchers at Curtin University are using open source visualisation software to render 3D environments of shipwrecks. Some feedback from their work was:\n",
    "\n",
    "* Fragility between CUDA and HIP API's. They had to have a version of ROCm that closely matches the CUDA library.\n",
    "* Some CUDA functions were just wrappers that call other CUDA functions and thus were unable to be ported.\n",
    "* HIP functions would call ROCm functions on an AMD backend. It was a trial and error process to work out what ROCm libraries were requried.\n",
    "\n",
    "### FiCoS\n",
    "\n",
    "From [this github site](https://gitlab.com/andrea-tango/ficos) FiCoS is a simulator for biochemical networks. It uses CUDA to solve Ordinary Differential Equations (ODE's) in parallel over a GPU. As the project is quite small the **hipconvertinplace-perl.sh** script was used to easily port the code. During compilation I encountered numerous errors of the following type:\n",
    "\n",
    "* Arrays of the data type **hipDoubleComplex** were incompatible with required input arguments of type **hipBlasDoubleComplex** for hibBlas routines when trying to use the NVIDIA backend. This was not an issue with the AMD backend.\n",
    "* Dynamic parallelism (kernels launching kernels) is a design feature that is employed in this code. Dynamic parallelism is not supported on the AMD backend, which means that the codebase must be refactored in order to use AMD.\n",
    "\n",
    "### CURC\n",
    "\n",
    "[CURC](https://github.com/BioinfoSZU/CURC) is a CUDA-based bioinformatics tool to compress and decompress genome information from FASTQ files. \n",
    "\n",
    "* The port itself proceeded smoothly, however there were a lot of warnings about texture reference API calls, which are no longer supported in CUDA 12 in favour of texture object API calls.\n",
    "* Cmake build system was CUDA-specific and required modification to use HIP.\n",
    "\n",
    "### Miluphcuda\n",
    "\n",
    "From their [Github page](https://github.com/christophmschaefer/miluphcuda) Miluphcuda is a CUDA-based Smoothed Particle Hydrodynamics code for modelling astrophysical impacts. Porting this source tree was a **complete success** as it did not use any deprecated or CUDA-specific features, and required only minor syntactical changes. One flag that was required during compilation was `-fgpu-rdc`, and that is because some source files relied on constant memory whose symbols were uploaded in another source file. The relocatable device flag enabled compiliation to be okay with the absent symbols. The ported source tree is available [here](https://github.com/drtpotter/miluph-hip)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2d5ccc-66a6-4dff-a76e-2b017591f613",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Learnings from the porting process\n",
    "\n",
    "### Code complexity can be an enemy of progress!\n",
    "\n",
    "The quest for greater performance often comes at the price of greater complexity. Porting efforts in the case studies were often thwarted because I would encounter assembly code or esoteric CUDA features like texture reference calls that are no longer supported by either CUDA or HIP. When developing codes it is important to weigh in tradeoffs between small increases in efficiency versus the developer cost of maintaining this complexity.\n",
    "\n",
    "### Naming conventions\n",
    "\n",
    "Vendor diversity is good for business. This means that vendors will change, as will the  functions used to access the accelerator hardware. Therefore incorporating the words `cuda` or `hip`, or even `gpu` in function or class names isn't going to port well to new platforms, and may confuse future porting tools. \n",
    "\n",
    "### Hardware differences between GPU's\n",
    "\n",
    "#### Thread team size\n",
    "\n",
    "From [Yuhsiang et al (2020)](https://www.researchgate.net/publication/342464640_Preparing_Ginkgo_for_AMD_GPUs_--_A_Testimonial_on_Porting_CUDA_Code_to_HIP) the primary architectural difference between AMD and NVIDIA is AMD's use of 64 threads in a thread team versus 32 threads for NVIDIA devices.\n",
    "\n",
    "#### Available registers at peak occupancy\n",
    "\n",
    "We saw from Lesson 7 that the MI250X GPU may have fewer available registers per kernel thread in order to maintain peak occupancy. Combined with difference in compiler and runtime maturity, this may mean that a kernel running on AMD hardware may not achieve the same occupancy as it did on NVIDIA hardware. The tool Omniperf is good at showing the occupancy of your kernels. If you see reduced occupancy in ported code then see some of the tips in Lesson 7 on <a href=\"../L7_Kernel_Optimisation/Optimisation.ipynb\">optimising kernels</a> to try and reduce register pressure.\n",
    "\n",
    "### Software differences between CUDA and hip-clang\n",
    "\n",
    "CUDA has the notion of a driver API and a runtime API. HIP fuses the two into one API and then supports a subset of the fused API. If your CUDA code provides functionality for using both API's then you might encounter some redundant code in the overlap. This [table](https://rocm.docs.amd.com/projects/HIP/en/latest/user_guide/faq.html#what-apis-and-features-does-hip-support) has the most up-to-date listing of features that are supported and not supported in HIP. In particular, here are some notable points of difference between CUDA and HIP.\n",
    "\n",
    "* Launching kernels from kernels (dynamic parallelism) is not supported in HIP. Avoid using this design pattern when building cross-platform code.\n",
    "* Context management is deprecated in HIP. CUDA code that uses contexts can be migrated to HIP's simpler method of using primary contexts and switching devices from threads.\n",
    "* Graphics interopability with OpenGL is not yet supported.\n",
    "* The CUDA API has undergone some major changes such as the removal (since CUDA 12) of the texture reference API in favour of the texture object API.\n",
    "* Any inline `PTX` assembly instructions for CUDA kernels will need to be ported across to `hsaco` assembly in AMD kernels. Use the preprocessor directives `__HIP_PLATFORM_NVIDIA__` and `__HIP_PLATFORM_AMD__` to enclose vendor specific code.\n",
    "* CUDA doesn't appear to support math operations on vector types. Use preprocessor directives to index into individual vector components when using the CUDA backend.\n",
    "* The memory types `cudaMemoryType*` evaluates to different enum values when porting from CUDA to HIP. Hopefully these are not hardcoded into your CUDA code, but it is something to be aware of. \n",
    "\n",
    "## Miscellaneous porting tips\n",
    "\n",
    "### Relocatable device code\n",
    "\n",
    "From [this source](https://docs.amd.com/projects/HIP/en/latest/user_guide/hip_porting_driver_api.html) The linker option `-fgpu-rdc` allows for kernels to call functions that are compiled for different translation units. This is useful for instances where a kernel might not be aware of things like allocations in constant memory where symbols are uploaded in another file. At the [Pawsey P'Con 23 Hackathon](https://pawsey.org.au/event/pacer-conference-2023-pcon23-registration/) a team found that the use of this flag generated excessively long link times though. \n",
    "\n",
    "### Preprocessor directives\n",
    "\n",
    "During compilation the preprocessor directive `__HIP_PLATFORM_NVIDIA__` is defined when using an NVIDIA backend, and the preprocessor directive `__HIP_PLATFORM_AMD__` is defined when using an AMD backend.\n",
    "\n",
    "### Switching between backends\n",
    "\n",
    "The environment variable `HIP_PLATFORM` controls what backend to use when compiling HIP source. Set `HIP_PLATFORM=nvidia` to use the CUDA backend and set `HIP_PLATFORM=amd` or leave it unset to use the AMD backend."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3526c094-8cec-4a2d-a01e-3e7070ee8624",
   "metadata": {},
   "source": [
    "## Alternative approaches to porting\n",
    "\n",
    "Maintaining a codebase to work across both AMD and NVIDIA backends is a difficult task, especially if you are trying to ensure the code is both performant and portable between platforms. An easier task might be to use abstraction to separate the science from the hardware access. Then you can construct vendor-specific libraries to access the hardware and do the math transformations, and choose between them at compiler time. For an example of this see [this code](https://github.com/pelahi/profile_util) from Pawsey staff member Pascal Elahi. Alternatively, you can use libraries like [kokkos](https://github.com/kokkos/kokkos) or [raja](https://computing.llnl.gov/projects/raja-managing-application-portability-next-generation-platforms) to manage hardware access at the cost of transparency.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c7e50b-2e68-4e85-b7f6-f3418de89d67",
   "metadata": {},
   "source": [
    "<address>\n",
    "Written by Dr. Toby Potter of <a href=\"https://www.pelagos-consulting.com\">Pelagos Consulting and Education</a> for the <a href=\"https://pawsey.org.au\">Pawsey Supercomputing Research Centre</a> with significant input from the Pawsey team. All trademarks mentioned are the property of their respective owners.\n",
    "</address>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
